16:24:28.950 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:24:30.990 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:24:31.414 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:24:31.422 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:24:31.427 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:24:31.427 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:24:31.439 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:24:33.427 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3092.
16:24:33.526 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:24:33.658 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:24:33.677 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:24:33.678 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:24:33.692 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-8ac3abad-be6d-4180-b709-df411e332192
16:24:33.819 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:24:34.036 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:24:34.352 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @11156ms
16:24:34.604 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:24:34.656 INFO  [main] org.spark_project.jetty.server.Server - Started @11461ms
16:24:34.743 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:24:34.744 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:24:34.792 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
16:24:34.793 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
16:24:34.793 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
16:24:34.794 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
16:24:34.794 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
16:24:34.795 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
16:24:34.795 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
16:24:34.797 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
16:24:34.797 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
16:24:34.798 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
16:24:34.798 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
16:24:34.799 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
16:24:34.800 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
16:24:34.800 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
16:24:34.801 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
16:24:34.801 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
16:24:34.802 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
16:24:34.803 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
16:24:34.803 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
16:24:34.804 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:24:34.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
16:24:34.814 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
16:24:34.816 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
16:24:34.816 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
16:24:34.817 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
16:24:34.819 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:24:35.173 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:24:35.257 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3133.
16:24:35.283 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3133
16:24:35.294 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:24:35.388 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.411 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3133 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.417 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.417 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.711 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
16:24:38.685 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:24:38.706 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:24:38.707 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:24:38.725 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL,null,AVAILABLE,@Spark}
16:24:38.726 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL/json,null,AVAILABLE,@Spark}
16:24:38.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution,null,AVAILABLE,@Spark}
16:24:38.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23468512{/SQL/execution/json,null,AVAILABLE,@Spark}
16:24:38.731 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53747c4a{/static/sql,null,AVAILABLE,@Spark}
16:24:40.021 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:24:41.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:24:41.257 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:24:43.479 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:24:44.776 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:24:44.779 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:24:45.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:24:45.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:24:45.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:24:45.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:24:45.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:24:45.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:24:45.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:24:45.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:24:45.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:24:45.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:24:45.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:24:45.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:24:45.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:24:51.288 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507
16:24:51.316 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507
16:24:51.318 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/607e9f8d-cbe9-4841-8a67-3df0165efd8b_resources
16:24:51.321 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/607e9f8d-cbe9-4841-8a67-3df0165efd8b
16:24:51.325 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/607e9f8d-cbe9-4841-8a67-3df0165efd8b
16:24:51.329 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/607e9f8d-cbe9-4841-8a67-3df0165efd8b/_tmp_space.db
16:24:51.345 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:24:51.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:24:51.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:24:51.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:24:51.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:24:51.504 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:24:51.981 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/3f18d854-f1d1-45e1-b03b-80e1b7586e92_resources
16:24:51.986 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3f18d854-f1d1-45e1-b03b-80e1b7586e92
16:24:51.990 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/3f18d854-f1d1-45e1-b03b-80e1b7586e92
16:24:51.995 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3f18d854-f1d1-45e1-b03b-80e1b7586e92/_tmp_space.db
16:24:51.999 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:24:52.169 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:24:52.186 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:24:52.597 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:24:52.598 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:24:52.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:52.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:53.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.312 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.314 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.314 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.314 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:53.365 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:24:53.907 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table ods_release.ods_01_release_session (inference mode: INFER_AND_SAVE)
16:24:53.918 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:24:53.919 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:24:53.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:53.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.943 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:53.943 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.964 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.964 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.966 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.966 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.966 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:54.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
16:24:54.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
16:24:55.839 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
16:24:55.908 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
16:24:55.909 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
16:24:55.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:24:55.911 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:24:55.966 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
16:24:56.234 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
16:24:56.781 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
16:24:56.814 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:3133 (size: 26.8 KB, free: 1988.7 MB)
16:24:56.853 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
16:24:56.993 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
16:24:56.995 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
16:24:57.157 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5019 bytes)
16:24:57.237 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:24:58.889 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1781 bytes result sent to driver
16:24:58.905 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1772 ms on localhost (executor driver) (1/1)
16:24:58.908 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:24:58.926 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 1.815 s
16:24:58.930 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 3.090782 s
16:24:59.019 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table ods_release.ods_01_release_session
16:24:59.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:24:59.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:24:59.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:59.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:59.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session
16:24:59.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session	
16:25:00.214 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:25:00.226 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:25:00.227 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:25:00.227 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:25:00.228 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:25:00.228 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:25:00.229 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:25:00.251 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:25:00.279 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:25:00.282 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:25:00.283 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:25:00.284 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:25:00.284 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:25:00.285 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:25:00.286 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:25:00.287 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:25:00.288 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:25:00.288 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:25:00.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.392 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.594 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:25:00.625 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:25:00.628 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:25:00.695 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:25:00.768 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:25:00.769 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:25:00.785 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:25:00.792 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:25:00.805 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:25:00.809 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:25:00.810 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-d9b9e097-5052-446d-bde7-fba37b69038e
16:29:04.166 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:29:04.448 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:29:04.467 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:29:04.467 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:29:04.468 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:29:04.468 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:29:04.469 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:29:05.292 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3456.
16:29:05.312 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:29:05.329 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:29:05.332 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:29:05.332 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:29:05.342 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-199be220-6cc4-436a-ad2a-8ffe69acdae5
16:29:05.359 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:29:05.404 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:29:05.487 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3790ms
16:29:05.556 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:29:05.571 INFO  [main] org.spark_project.jetty.server.Server - Started @3875ms
16:29:05.596 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6de0a959{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:29:05.596 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:29:05.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6650813a{/jobs,null,AVAILABLE,@Spark}
16:29:05.619 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30272916{/jobs/json,null,AVAILABLE,@Spark}
16:29:05.619 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job,null,AVAILABLE,@Spark}
16:29:05.620 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job/json,null,AVAILABLE,@Spark}
16:29:05.621 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/stages,null,AVAILABLE,@Spark}
16:29:05.622 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages/json,null,AVAILABLE,@Spark}
16:29:05.623 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/stage,null,AVAILABLE,@Spark}
16:29:05.624 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e8f862{/stages/stage/json,null,AVAILABLE,@Spark}
16:29:05.625 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/pool,null,AVAILABLE,@Spark}
16:29:05.626 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool/json,null,AVAILABLE,@Spark}
16:29:05.626 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/storage,null,AVAILABLE,@Spark}
16:29:05.627 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage/json,null,AVAILABLE,@Spark}
16:29:05.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/rdd,null,AVAILABLE,@Spark}
16:29:05.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd/json,null,AVAILABLE,@Spark}
16:29:05.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/environment,null,AVAILABLE,@Spark}
16:29:05.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment/json,null,AVAILABLE,@Spark}
16:29:05.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/executors,null,AVAILABLE,@Spark}
16:29:05.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors/json,null,AVAILABLE,@Spark}
16:29:05.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/threadDump,null,AVAILABLE,@Spark}
16:29:05.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:29:05.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/static,null,AVAILABLE,@Spark}
16:29:05.640 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/,null,AVAILABLE,@Spark}
16:29:05.641 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/api,null,AVAILABLE,@Spark}
16:29:05.642 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/jobs/job/kill,null,AVAILABLE,@Spark}
16:29:05.643 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/stages/stage/kill,null,AVAILABLE,@Spark}
16:29:05.645 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:29:05.717 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:29:05.746 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3497.
16:29:05.746 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3497
16:29:05.747 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:29:05.781 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.784 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3497 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.787 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.788 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.961 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@529cfee5{/metrics/json,null,AVAILABLE,@Spark}
16:29:07.335 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:29:07.362 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:29:07.363 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:29:07.370 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19489b27{/SQL,null,AVAILABLE,@Spark}
16:29:07.370 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL/json,null,AVAILABLE,@Spark}
16:29:07.371 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL/execution,null,AVAILABLE,@Spark}
16:29:07.371 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution/json,null,AVAILABLE,@Spark}
16:29:07.372 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69cd7630{/static/sql,null,AVAILABLE,@Spark}
16:29:07.855 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:29:08.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:29:08.550 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:29:09.926 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:29:11.511 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:29:11.514 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:29:11.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:29:11.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:29:11.894 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:29:11.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:29:11.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:29:12.002 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:29:12.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:29:12.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:29:12.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:29:12.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:29:12.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:29:12.379 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:29:12.379 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:29:12.882 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/f4365138-42ea-4731-be33-e2ada62905aa_resources
16:29:12.927 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f4365138-42ea-4731-be33-e2ada62905aa
16:29:12.932 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/f4365138-42ea-4731-be33-e2ada62905aa
16:29:12.937 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f4365138-42ea-4731-be33-e2ada62905aa/_tmp_space.db
16:29:12.942 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:29:12.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:12.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:12.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:29:12.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:29:12.979 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:29:13.178 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/ec3b6d36-39eb-4119-aa5f-f0ebd102590f_resources
16:29:13.204 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/ec3b6d36-39eb-4119-aa5f-f0ebd102590f
16:29:13.208 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/ec3b6d36-39eb-4119-aa5f-f0ebd102590f
16:29:13.213 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/ec3b6d36-39eb-4119-aa5f-f0ebd102590f/_tmp_space.db
16:29:13.215 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:29:13.238 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:29:13.243 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:29:13.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:29:13.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:29:13.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:29:13.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:29:13.553 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:29:13.554 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:29:13.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.603 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.603 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.604 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.604 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.604 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:29:13.622 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:29:14.010 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:29:14.022 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:29:14.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:29:14.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:29:14.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:29:14.024 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:29:14.024 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:29:14.039 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:29:14.064 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:29:14.067 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:29:14.067 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:29:14.068 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:29:14.069 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:29:14.070 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:29:14.071 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:29:14.072 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:29:14.073 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:29:14.074 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:29:14.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.142 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.285 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:29:14.293 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6de0a959{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:29:14.296 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:29:14.307 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:29:14.320 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:29:14.321 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:29:14.330 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:29:14.334 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:29:14.339 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:29:14.340 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:29:14.341 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-8178cb57-9c23-42b2-af3a-8a267a597b7e
16:34:43.542 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:34:43.851 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:34:43.871 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:34:43.872 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:34:43.872 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:34:43.873 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:34:43.873 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:34:44.741 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3593.
16:34:44.761 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:34:44.778 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:34:44.782 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:34:44.783 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:34:44.792 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-a267a7a4-5f70-4fa6-9d46-dbe4aacffe49
16:34:44.811 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:34:44.861 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:34:44.956 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4212ms
16:34:45.027 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:34:45.041 INFO  [main] org.spark_project.jetty.server.Server - Started @4300ms
16:34:45.070 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:34:45.071 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:34:45.097 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
16:34:45.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
16:34:45.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
16:34:45.099 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
16:34:45.100 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
16:34:45.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
16:34:45.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
16:34:45.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
16:34:45.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
16:34:45.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
16:34:45.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
16:34:45.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
16:34:45.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
16:34:45.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
16:34:45.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
16:34:45.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
16:34:45.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
16:34:45.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
16:34:45.109 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
16:34:45.109 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:34:45.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
16:34:45.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
16:34:45.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
16:34:45.120 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
16:34:45.121 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
16:34:45.123 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:34:45.205 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:34:45.235 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3634.
16:34:45.236 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3634
16:34:45.237 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:34:45.273 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.275 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3634 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.278 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.278 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.451 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
16:34:46.984 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:34:47.011 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:34:47.011 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:34:47.019 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
16:34:47.019 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
16:34:47.020 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
16:34:47.021 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
16:34:47.023 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
16:34:47.505 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:34:48.197 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:34:48.225 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:34:49.483 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:34:50.798 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:34:50.801 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:34:51.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:34:51.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:34:51.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:34:51.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:34:51.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:34:51.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:34:51.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:34:51.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:34:51.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:34:51.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:34:51.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:34:51.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:34:51.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:34:51.879 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/09ba03b2-8a78-4f6a-8bee-a983da4e3493_resources
16:34:51.888 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/09ba03b2-8a78-4f6a-8bee-a983da4e3493
16:34:51.893 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/09ba03b2-8a78-4f6a-8bee-a983da4e3493
16:34:51.901 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/09ba03b2-8a78-4f6a-8bee-a983da4e3493/_tmp_space.db
16:34:51.906 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:34:51.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:51.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:51.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:34:51.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:34:51.943 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:34:52.124 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/b880db71-db9d-4954-9200-35833409f5f8_resources
16:34:52.201 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b880db71-db9d-4954-9200-35833409f5f8
16:34:52.205 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/b880db71-db9d-4954-9200-35833409f5f8
16:34:52.215 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b880db71-db9d-4954-9200-35833409f5f8/_tmp_space.db
16:34:52.219 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:34:52.243 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:34:52.249 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:34:52.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:34:52.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:34:52.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:34:52.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:34:52.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:34:52.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:34:52.580 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.592 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:34:52.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:34:52.952 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:34:52.967 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:34:52.968 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:34:52.983 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:34:53.012 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:34:53.015 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:34:53.017 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:34:53.018 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:34:53.019 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:34:53.020 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:34:53.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:34:53.022 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:34:53.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:34:53.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:34:53.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.093 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.116 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.242 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:34:53.249 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:34:53.252 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:34:53.264 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:34:53.278 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:34:53.278 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:34:53.286 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:34:53.290 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:34:53.296 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:34:53.296 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:34:53.297 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-d1d0e69e-4e01-4166-959b-32ffc76309b7
16:35:12.880 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:35:13.235 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:35:13.254 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:35:13.255 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:35:13.255 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:35:13.255 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:35:13.256 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:35:14.163 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3697.
16:35:14.181 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:35:14.199 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:35:14.204 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:35:14.204 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:35:14.214 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-5754cc95-f6f9-473d-adb0-6f78b45f0069
16:35:14.233 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:35:14.283 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:35:14.370 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4203ms
16:35:14.436 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:35:14.457 INFO  [main] org.spark_project.jetty.server.Server - Started @4291ms
16:35:14.480 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:35:14.480 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:35:14.506 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
16:35:14.507 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
16:35:14.508 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
16:35:14.509 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
16:35:14.509 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
16:35:14.510 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
16:35:14.510 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
16:35:14.511 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
16:35:14.512 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
16:35:14.513 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
16:35:14.514 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
16:35:14.514 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
16:35:14.515 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
16:35:14.515 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
16:35:14.516 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
16:35:14.517 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
16:35:14.518 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
16:35:14.519 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
16:35:14.519 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
16:35:14.520 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:35:14.525 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
16:35:14.526 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
16:35:14.527 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
16:35:14.527 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
16:35:14.528 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
16:35:14.531 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:35:14.612 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:35:14.654 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3738.
16:35:14.655 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3738
16:35:14.657 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:35:14.691 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.694 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3738 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.697 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.697 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.870 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
16:35:16.397 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:35:16.420 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:35:16.421 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:35:16.426 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
16:35:16.427 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
16:35:16.429 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
16:35:16.429 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
16:35:16.432 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
16:35:16.927 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:35:17.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:35:17.668 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:35:18.836 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:35:20.258 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:35:20.261 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:35:20.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:35:20.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:35:20.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:35:20.638 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:35:20.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:35:20.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:35:20.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:35:20.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:35:20.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:35:20.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:35:20.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:35:20.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:35:20.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:35:21.315 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/74626c68-d377-4dc1-b79b-84710f6fa6f5_resources
16:35:21.349 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/74626c68-d377-4dc1-b79b-84710f6fa6f5
16:35:21.352 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/74626c68-d377-4dc1-b79b-84710f6fa6f5
16:35:21.356 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/74626c68-d377-4dc1-b79b-84710f6fa6f5/_tmp_space.db
16:35:21.360 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:35:21.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:21.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:21.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:35:21.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:35:21.393 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:35:21.580 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/776b2ee1-649e-4362-b2c9-1c85d68eecae_resources
16:35:21.584 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/776b2ee1-649e-4362-b2c9-1c85d68eecae
16:35:21.587 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/776b2ee1-649e-4362-b2c9-1c85d68eecae
16:35:21.595 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/776b2ee1-649e-4362-b2c9-1c85d68eecae/_tmp_space.db
16:35:21.599 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:35:21.642 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:35:21.651 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:35:21.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:35:21.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:35:21.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:21.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:21.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:21.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:22.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:22.049 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:35:22.386 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:35:22.399 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:35:22.399 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:35:22.400 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:35:22.400 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:35:22.401 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:35:22.402 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:35:22.417 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:35:22.445 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:35:22.448 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:35:22.449 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:35:22.449 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:35:22.451 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:35:22.453 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:35:22.454 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:35:22.455 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:35:22.455 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:35:22.456 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:35:22.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.708 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:35:22.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:22.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:22.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:23.566 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1
16:35:24.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:35:24.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:35:24.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:24.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:24.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:24.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:24.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:35:24.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:35:24.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:35:24.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:35:24.607 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
16:35:24.608 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:35:24.611 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:35:24.688 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:35:24.705 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:35:25.095 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:25.096 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:26.009 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 590.912862 ms
16:35:26.087 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 64.994816 ms
16:35:26.285 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
16:35:26.414 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
16:35:26.418 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:3738 (size: 26.3 KB, free: 1988.7 MB)
16:35:26.421 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from insertInto at SparkHelper.scala:35
16:35:26.532 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
16:35:26.823 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
16:35:26.837 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (insertInto at SparkHelper.scala:35)
16:35:26.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (insertInto at SparkHelper.scala:35) with 4 output partitions
16:35:26.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (insertInto at SparkHelper.scala:35)
16:35:26.840 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:35:26.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
16:35:26.877 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35), which has no missing parents
16:35:27.050 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1988.3 MB)
16:35:27.054 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.3 MB)
16:35:27.055 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:3738 (size: 10.4 KB, free: 1988.7 MB)
16:35:27.055 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:35:27.067 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
16:35:27.068 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
16:35:27.108 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
16:35:27.111 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
16:35:27.112 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
16:35:27.112 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
16:35:27.114 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
16:35:27.115 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
16:35:27.116 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
16:35:27.116 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
16:35:27.126 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
16:35:27.126 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
16:35:27.126 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:35:27.126 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
16:35:27.126 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
16:35:27.126 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
16:35:27.126 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
16:35:27.126 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
16:35:27.346 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:35:27.404 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.42008 ms
16:35:27.458 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
16:35:27.459 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
16:35:28.814 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.857 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:29.243 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1610 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1610 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1610 bytes result sent to driver
16:35:29.251 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 2134 ms on localhost (executor driver) (1/8)
16:35:29.253 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 2139 ms on localhost (executor driver) (2/8)
16:35:29.254 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 2139 ms on localhost (executor driver) (3/8)
16:35:29.254 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 2141 ms on localhost (executor driver) (4/8)
16:35:29.254 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 2143 ms on localhost (executor driver) (5/8)
16:35:29.254 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2155 ms on localhost (executor driver) (6/8)
16:35:29.254 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 2143 ms on localhost (executor driver) (7/8)
16:35:33.127 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1739 bytes result sent to driver
16:35:33.128 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 6016 ms on localhost (executor driver) (8/8)
16:35:33.130 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:35:33.131 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (insertInto at SparkHelper.scala:35) finished in 6.042 s
16:35:33.131 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:35:33.132 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:35:33.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
16:35:33.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:35:33.148 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35), which has no missing parents
16:35:33.215 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
16:35:33.217 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 66.1 KB, free 1988.1 MB)
16:35:33.218 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:3738 (size: 66.1 KB, free: 1988.6 MB)
16:35:33.219 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:35:33.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
16:35:33.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 4 tasks
16:35:33.224 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:35:33.224 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:35:33.224 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 10, localhost, executor driver, partition 2, ANY, 4726 bytes)
16:35:33.225 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 11, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:35:33.225 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 3.0 in stage 1.0 (TID 11)
16:35:33.225 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 9)
16:35:33.225 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
16:35:33.225 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 10)
16:35:33.320 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.320 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.320 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.320 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.323 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.323 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.323 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.323 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.381 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.908554 ms
16:35:33.419 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.436556 ms
16:35:33.678 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.678 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.679 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.679 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.680 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.680 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.681 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.681 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.705 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.9126 ms
16:35:33.763 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.693157 ms
16:35:33.783 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.539766 ms
16:35:34.093 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@79d6f404
16:35:34.094 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bfae730
16:35:34.094 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3763a9fd
16:35:34.094 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a365b31
16:35:34.126 INFO  [Executor task launch worker for task 9] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16:35:34.156 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000003_0/bdp_day=20190924/part-00003-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.156 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000001_0/bdp_day=20190924/part-00001-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.156 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000000_0/bdp_day=20190924/part-00000-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.156 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000002_0/bdp_day=20190924/part-00002-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.179 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.179 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.179 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.179 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.180 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.180 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.180 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.180 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.183 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.183 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.183 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.184 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.184 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.184 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.185 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.185 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.186 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.186 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.626 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5d2b30af
16:35:34.626 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@294c5f99
16:35:34.627 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@363379ee
16:35:34.627 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@27127132
16:35:35.021 INFO  [Executor task launch worker for task 8] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,597
16:35:35.024 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,547
16:35:35.026 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,984
16:35:35.031 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,815
16:35:35.328 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.328 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.328 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.328 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.330 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.330 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.330 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.330 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.331 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.331 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.331 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.331 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.332 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,447 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.332 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.332 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.332 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.334 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,784B for [gender] BINARY: 21,446 values, 2,753B raw, 2,753B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,446 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,786B for [gender] BINARY: 21,447 values, 2,755B raw, 2,755B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,789B for [gender] BINARY: 21,446 values, 2,758B raw, 2,758B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,247B for [area_code] BINARY: 21,446 values, 3,206B raw, 3,206B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 3,213B for [area_code] BINARY: 21,446 values, 3,172B raw, 3,172B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,222B for [area_code] BINARY: 21,447 values, 3,181B raw, 3,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,227B for [area_code] BINARY: 21,446 values, 3,186B raw, 3,186B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.336 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.336 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.337 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.337 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.337 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.341 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.342 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.343 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.344 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.344 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.344 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:35.345 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:35.345 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:35.345 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:36.564 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000001
16:35:36.564 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000002
16:35:36.565 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000001_0: Committed
16:35:36.565 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000002_0: Committed
16:35:36.594 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 9). 2615 bytes result sent to driver
16:35:36.594 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 10). 2615 bytes result sent to driver
16:35:36.595 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 9) in 3371 ms on localhost (executor driver) (1/4)
16:35:36.596 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 10) in 3372 ms on localhost (executor driver) (2/4)
16:35:38.470 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000003
16:35:38.470 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000003_0: Committed
16:35:38.471 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 3.0 in stage 1.0 (TID 11). 2572 bytes result sent to driver
16:35:38.472 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000000
16:35:38.472 INFO  [Executor task launch worker for task 8] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000000_0: Committed
16:35:38.472 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 11) in 5247 ms on localhost (executor driver) (3/4)
16:35:38.474 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 2572 bytes result sent to driver
16:35:38.475 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 5253 ms on localhost (executor driver) (4/4)
16:35:38.475 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:35:38.475 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (insertInto at SparkHelper.scala:35) finished in 5.254 s
16:35:38.479 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: insertInto at SparkHelper.scala:35, took 11.655995 s
16:35:38.555 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
16:35:38.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:38.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:35:38.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:35:38.807 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
16:35:38.940 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00000-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00000-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.031 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00001-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00001-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.044 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00002-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00002-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.056 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00003-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00003-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:35:39.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:35:39.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
16:35:39.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
16:35:39.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
16:35:39.149 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
16:35:39.153 WARN  [main] hive.log - Updated size to 5782457
16:35:39.478 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
16:35:39.504 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
16:35:39.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:35:39.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:35:39.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.513 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:39.634 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_customer (inference mode: INFER_AND_SAVE)
16:35:39.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:35:39.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:35:39.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:39.705 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_customer
16:35:39.705 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_customer	
16:35:39.821 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 1 output partitions
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (insertInto at SparkHelper.scala:35)
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35), which has no missing parents
16:35:39.836 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 74.4 KB, free 1988.0 MB)
16:35:39.838 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.0 MB)
16:35:39.838 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:3738 (size: 26.8 KB, free: 1988.6 MB)
16:35:39.839 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
16:35:39.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
16:35:39.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
16:35:39.842 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5050 bytes)
16:35:39.844 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 12)
16:35:40.469 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 12). 1978 bytes result sent to driver
16:35:40.469 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 12) in 629 ms on localhost (executor driver) (1/1)
16:35:40.470 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
16:35:40.470 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (insertInto at SparkHelper.scala:35) finished in 0.630 s
16:35:40.470 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 0.649859 s
16:35:40.478 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_customer
16:35:40.479 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:35:40.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:35:40.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:40.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.596 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.597 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:40.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.695 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer
16:35:40.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer	
16:35:40.921 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:35:40.929 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:35:40.931 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:35:40.965 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:35:41.049 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:35:41.050 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:35:41.050 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:35:41.054 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:35:41.059 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:35:41.060 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:35:41.061 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-053b050f-f2c3-41ac-9dfb-4c8a2fb0885a
16:38:52.196 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:38:52.483 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:38:52.501 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:38:52.501 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:38:52.503 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:38:52.503 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:38:52.503 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:38:53.300 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3868.
16:38:53.324 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:38:53.341 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:38:53.344 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:38:53.344 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:38:53.354 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-67ef129d-28ad-4fb1-b121-00b5ee312650
16:38:53.372 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:38:53.415 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:38:53.497 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3789ms
16:38:53.574 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:38:53.589 INFO  [main] org.spark_project.jetty.server.Server - Started @3882ms
16:38:53.610 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@27e358a7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:38:53.611 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:38:53.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50cf5a23{/jobs,null,AVAILABLE,@Spark}
16:38:53.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/json,null,AVAILABLE,@Spark}
16:38:53.633 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/jobs/job,null,AVAILABLE,@Spark}
16:38:53.634 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/job/json,null,AVAILABLE,@Spark}
16:38:53.634 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages,null,AVAILABLE,@Spark}
16:38:53.635 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/json,null,AVAILABLE,@Spark}
16:38:53.636 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/stage,null,AVAILABLE,@Spark}
16:38:53.637 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/stage/json,null,AVAILABLE,@Spark}
16:38:53.637 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool,null,AVAILABLE,@Spark}
16:38:53.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/pool/json,null,AVAILABLE,@Spark}
16:38:53.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage,null,AVAILABLE,@Spark}
16:38:53.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/json,null,AVAILABLE,@Spark}
16:38:53.639 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd,null,AVAILABLE,@Spark}
16:38:53.639 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd/json,null,AVAILABLE,@Spark}
16:38:53.640 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment,null,AVAILABLE,@Spark}
16:38:53.640 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/environment/json,null,AVAILABLE,@Spark}
16:38:53.641 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors,null,AVAILABLE,@Spark}
16:38:53.642 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/json,null,AVAILABLE,@Spark}
16:38:53.643 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump,null,AVAILABLE,@Spark}
16:38:53.643 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:38:53.651 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/static,null,AVAILABLE,@Spark}
16:38:53.652 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/,null,AVAILABLE,@Spark}
16:38:53.654 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/api,null,AVAILABLE,@Spark}
16:38:53.655 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/jobs/job/kill,null,AVAILABLE,@Spark}
16:38:53.656 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/stages/stage/kill,null,AVAILABLE,@Spark}
16:38:53.658 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:38:53.730 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:38:53.757 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3909.
16:38:53.757 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3909
16:38:53.758 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:38:53.791 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.794 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3909 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.796 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.797 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.965 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319854f0{/metrics/json,null,AVAILABLE,@Spark}
16:38:55.362 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:38:55.386 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:38:55.386 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:38:55.392 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL,null,AVAILABLE,@Spark}
16:38:55.392 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@125d47c4{/SQL/json,null,AVAILABLE,@Spark}
16:38:55.393 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution,null,AVAILABLE,@Spark}
16:38:55.393 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f2466c{/SQL/execution/json,null,AVAILABLE,@Spark}
16:38:55.395 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b53840a{/static/sql,null,AVAILABLE,@Spark}
16:38:55.852 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:38:56.502 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:38:56.533 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:38:57.630 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:38:58.867 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:38:58.869 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:38:59.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:38:59.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:38:59.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:38:59.246 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:38:59.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:38:59.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:38:59.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:38:59.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:38:59.301 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:38:59.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:38:59.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:38:59.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:38:59.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:38:59.809 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/bff0e9f9-04d5-4c09-a664-2295d277892e_resources
16:38:59.820 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/bff0e9f9-04d5-4c09-a664-2295d277892e
16:38:59.823 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/bff0e9f9-04d5-4c09-a664-2295d277892e
16:38:59.828 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/bff0e9f9-04d5-4c09-a664-2295d277892e/_tmp_space.db
16:38:59.832 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:38:59.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:38:59.856 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:38:59.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:38:59.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:38:59.867 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:39:00.025 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/8a95594f-8c29-4d1b-903a-273c1c0d79d8_resources
16:39:00.037 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8a95594f-8c29-4d1b-903a-273c1c0d79d8
16:39:00.040 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/8a95594f-8c29-4d1b-903a-273c1c0d79d8
16:39:00.100 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8a95594f-8c29-4d1b-903a-273c1c0d79d8/_tmp_space.db
16:39:00.103 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:39:00.127 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:39:00.132 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:39:00.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:39:00.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:39:00.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:00.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:00.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:00.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:00.449 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.461 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.462 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.462 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.462 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:00.477 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:39:00.764 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:39:00.775 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:39:00.776 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:39:00.777 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:39:00.777 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:39:00.778 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:39:00.778 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:39:00.792 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:39:00.816 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:39:00.819 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:39:00.820 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:39:00.821 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:39:00.822 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:39:00.823 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:39:00.823 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:39:00.824 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:39:00.825 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:39:00.825 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:39:00.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.843 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.878 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.879 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.904 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:01.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:39:01.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:01.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:01.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:01.408 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1
16:39:01.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:39:01.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:39:01.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:01.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:01.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:01.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:01.656 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:01.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:39:01.675 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:39:01.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:39:01.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:39:01.873 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
16:39:01.876 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:39:01.878 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:39:01.889 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:39:01.893 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:39:01.950 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:01.951 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:02.356 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 248.779258 ms
16:39:02.407 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.86255 ms
16:39:02.498 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
16:39:02.648 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
16:39:02.659 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:3909 (size: 26.3 KB, free: 1988.7 MB)
16:39:02.662 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from insertInto at SparkHelper.scala:35
16:39:02.669 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
16:39:02.841 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
16:39:02.854 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (insertInto at SparkHelper.scala:35)
16:39:02.856 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (insertInto at SparkHelper.scala:35) with 4 output partitions
16:39:02.856 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (insertInto at SparkHelper.scala:35)
16:39:02.857 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:39:02.859 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
16:39:02.863 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35), which has no missing parents
16:39:02.956 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1988.3 MB)
16:39:02.961 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.3 MB)
16:39:02.962 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:3909 (size: 10.4 KB, free: 1988.7 MB)
16:39:02.962 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:39:02.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
16:39:02.973 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
16:39:03.010 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
16:39:03.012 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
16:39:03.013 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
16:39:03.014 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
16:39:03.014 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
16:39:03.015 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
16:39:03.016 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
16:39:03.016 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
16:39:03.025 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
16:39:03.025 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:39:03.025 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
16:39:03.025 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
16:39:03.025 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
16:39:03.025 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
16:39:03.025 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
16:39:03.025 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
16:39:03.057 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:39:03.201 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 38.819191 ms
16:39:03.220 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
16:39:03.221 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
16:39:03.221 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
16:39:04.423 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.424 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.454 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.501 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.540 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1567 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1653 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1610 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1653 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1653 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1610 bytes result sent to driver
16:39:04.544 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1567 bytes result sent to driver
16:39:04.550 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1534 ms on localhost (executor driver) (1/8)
16:39:04.553 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1537 ms on localhost (executor driver) (2/8)
16:39:04.553 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1542 ms on localhost (executor driver) (3/8)
16:39:04.553 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1539 ms on localhost (executor driver) (4/8)
16:39:04.554 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1553 ms on localhost (executor driver) (5/8)
16:39:04.554 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1539 ms on localhost (executor driver) (6/8)
16:39:04.554 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1542 ms on localhost (executor driver) (7/8)
16:39:07.184 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1739 bytes result sent to driver
16:39:07.185 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 4172 ms on localhost (executor driver) (8/8)
16:39:07.187 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:39:07.187 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (insertInto at SparkHelper.scala:35) finished in 4.195 s
16:39:07.188 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:39:07.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:39:07.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
16:39:07.190 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:39:07.192 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35), which has no missing parents
16:39:07.252 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
16:39:07.254 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 66.0 KB, free 1988.1 MB)
16:39:07.254 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:3909 (size: 66.0 KB, free: 1988.6 MB)
16:39:07.255 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:39:07.256 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
16:39:07.256 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 4 tasks
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 10, localhost, executor driver, partition 2, ANY, 4726 bytes)
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 11, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:39:07.260 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 9)
16:39:07.260 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 3.0 in stage 1.0 (TID 11)
16:39:07.260 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 10)
16:39:07.260 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
16:39:07.313 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.313 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.313 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.313 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.316 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.316 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.316 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.316 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.346 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.099467 ms
16:39:07.368 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.961888 ms
16:39:07.553 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.553 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.553 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.553 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.553 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.554 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.554 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.554 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.565 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.128104 ms
16:39:07.610 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.865596 ms
16:39:07.630 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.52505 ms
16:39:07.729 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b9d2c24
16:39:07.730 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d7ea8e5
16:39:07.730 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@523b7748
16:39:07.730 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ca1349a
16:39:07.743 INFO  [Executor task launch worker for task 9] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16:39:07.748 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.748 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.748 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.748 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.749 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000000_0/bdp_day=20190924/part-00000-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.749 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000002_0/bdp_day=20190924/part-00002-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.749 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000001_0/bdp_day=20190924/part-00001-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.749 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000003_0/bdp_day=20190924/part-00003-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.751 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.751 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.751 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.751 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.752 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.752 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.752 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.752 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.753 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.755 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.755 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.755 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.755 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.975 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6605ac4d
16:39:07.976 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@68164763
16:39:07.976 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3d94f215
16:39:07.977 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@383cabdc
16:39:08.393 INFO  [Executor task launch worker for task 8] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,597
16:39:08.393 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,984
16:39:08.396 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,547
16:39:08.399 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,815
16:39:08.535 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.535 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.535 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.535 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.536 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.536 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.536 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.536 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.537 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.537 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.537 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.538 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.538 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,447 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.538 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.541 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.541 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.541 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.542 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.542 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.542 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.543 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.543 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.543 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.543 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.543 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.544 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,446 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.544 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,786B for [gender] BINARY: 21,447 values, 2,755B raw, 2,755B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 3,213B for [area_code] BINARY: 21,446 values, 3,172B raw, 3,172B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,789B for [gender] BINARY: 21,446 values, 2,758B raw, 2,758B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,222B for [area_code] BINARY: 21,447 values, 3,181B raw, 3,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,227B for [area_code] BINARY: 21,446 values, 3,186B raw, 3,186B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.544 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.545 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.545 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.545 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.545 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.545 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.545 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,784B for [gender] BINARY: 21,446 values, 2,753B raw, 2,753B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.545 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.545 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.546 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,247B for [area_code] BINARY: 21,446 values, 3,206B raw, 3,206B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.546 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.546 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.546 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.547 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.547 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.547 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.547 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.547 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.547 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.548 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.548 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.827 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000002
16:39:08.827 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000000
16:39:08.827 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000003
16:39:08.827 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000003_0: Committed
16:39:08.827 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000002_0: Committed
16:39:08.827 INFO  [Executor task launch worker for task 8] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000000_0: Committed
16:39:08.831 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 3.0 in stage 1.0 (TID 11). 2658 bytes result sent to driver
16:39:08.831 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 2658 bytes result sent to driver
16:39:08.831 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 10). 2658 bytes result sent to driver
16:39:08.832 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 11) in 1573 ms on localhost (executor driver) (1/4)
16:39:08.832 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 10) in 1573 ms on localhost (executor driver) (2/4)
16:39:08.832 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 1575 ms on localhost (executor driver) (3/4)
16:39:08.839 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000001
16:39:08.839 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000001_0: Committed
16:39:08.840 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 9). 2615 bytes result sent to driver
16:39:08.841 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 9) in 1582 ms on localhost (executor driver) (4/4)
16:39:08.841 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:39:08.841 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (insertInto at SparkHelper.scala:35) finished in 1.584 s
16:39:08.846 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: insertInto at SparkHelper.scala:35, took 6.005189 s
16:39:08.905 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
16:39:08.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:08.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:08.934 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:08.935 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:08.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:08.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:08.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:09.005 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:09.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:09.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:39:09.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:39:09.114 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
16:39:09.127 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00000-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00000-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.142 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00001-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00001-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.201 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00002-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00002-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.262 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00003-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00003-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:39:09.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:39:09.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
16:39:09.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
16:39:09.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
16:39:09.343 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
16:39:09.346 WARN  [main] hive.log - Updated size to 5782457
16:39:09.637 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
16:39:09.643 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
16:39:09.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:39:09.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:39:09.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:09.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:09.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:09.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:09.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:09.737 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:39:09.743 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@27e358a7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:39:09.745 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:39:09.755 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:39:09.837 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:39:09.838 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:39:09.842 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:39:09.845 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:39:09.851 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:39:09.851 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:39:09.852 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-cd48a837-f951-4d2e-af8b-b229f111fb1b
16:41:32.183 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:41:32.500 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:41:32.521 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:41:32.521 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:41:32.521 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:41:32.522 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:41:32.522 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:41:33.371 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 4013.
16:41:33.393 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:41:33.410 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:41:33.414 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:41:33.414 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:41:33.423 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-7fc9a955-241f-49a7-808e-75d51177f9ea
16:41:33.442 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:41:33.488 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:41:33.575 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3997ms
16:41:33.648 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:41:33.664 INFO  [main] org.spark_project.jetty.server.Server - Started @4087ms
16:41:33.687 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:41:33.688 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:41:33.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
16:41:33.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
16:41:33.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
16:41:33.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
16:41:33.717 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
16:41:33.718 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
16:41:33.718 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
16:41:33.719 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
16:41:33.719 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
16:41:33.721 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
16:41:33.721 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
16:41:33.722 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
16:41:33.723 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
16:41:33.723 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
16:41:33.724 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
16:41:33.725 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
16:41:33.726 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
16:41:33.726 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
16:41:33.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
16:41:33.728 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:41:33.733 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
16:41:33.734 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
16:41:33.735 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
16:41:33.736 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
16:41:33.737 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
16:41:33.738 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:41:33.816 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:41:33.851 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4055.
16:41:33.853 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:4055
16:41:33.854 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:41:33.890 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:33.892 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:4055 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:33.894 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:33.895 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:34.074 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
16:41:35.599 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:41:35.622 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:41:35.623 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:41:35.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
16:41:35.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
16:41:35.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
16:41:35.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
16:41:35.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
16:41:36.127 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:41:36.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:41:36.859 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:41:38.127 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:41:39.527 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:41:39.530 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:41:39.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:41:39.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:41:39.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:41:39.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:41:39.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:41:39.969 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:41:39.969 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:41:40.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:41:40.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:41:40.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:41:40.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:41:40.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:41:40.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:41:40.554 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/a9fe9b95-c855-4f3a-a147-a33332dae926_resources
16:41:40.577 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a9fe9b95-c855-4f3a-a147-a33332dae926
16:41:40.581 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/a9fe9b95-c855-4f3a-a147-a33332dae926
16:41:40.618 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a9fe9b95-c855-4f3a-a147-a33332dae926/_tmp_space.db
16:41:40.625 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:41:40.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:40.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:40.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:41:40.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:41:40.661 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:41:40.815 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/fe2a029c-51ee-4a43-a471-15d1af2c322e_resources
16:41:40.825 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/fe2a029c-51ee-4a43-a471-15d1af2c322e
16:41:40.830 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/fe2a029c-51ee-4a43-a471-15d1af2c322e
16:41:40.837 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/fe2a029c-51ee-4a43-a471-15d1af2c322e/_tmp_space.db
16:41:40.840 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:41:40.868 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:41:40.875 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:41:41.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:41:41.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:41:41.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:41.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:41.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:41.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:41.215 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.229 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.229 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.231 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.231 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.231 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:41:41.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:41:41.540 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:41:41.552 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:41:41.553 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:41:41.553 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:41:41.554 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:41:41.554 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:41:41.555 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:41:41.570 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:41:41.592 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:41:41.593 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:41:41.594 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:41:41.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:41:41.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:41:41.596 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:41:41.596 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:41:41.597 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:41:41.597 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:41:41.597 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:41:41.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.615 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.659 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.670 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:42.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:41:42.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:41:42.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:42.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:42.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:42.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:42.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:41:42.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:41:42.110 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:41:42.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:41:42.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:41:42.321 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
16:41:42.323 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:41:42.326 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:41:42.337 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:41:42.341 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:41:42.761 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 233.066061 ms
16:41:43.083 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 40.619488 ms
16:41:43.170 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
16:41:43.276 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
16:41:43.280 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:4055 (size: 26.3 KB, free: 1988.7 MB)
16:41:43.284 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseCustomer.scala:59
16:41:43.294 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
16:41:43.408 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseCustomer.scala:59
16:41:43.425 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DwReleaseCustomer.scala:59)
16:41:43.427 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseCustomer.scala:59) with 1 output partitions
16:41:43.428 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseCustomer.scala:59)
16:41:43.428 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:41:43.430 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
16:41:43.434 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseCustomer.scala:59), which has no missing parents
16:41:43.549 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1988.3 MB)
16:41:43.552 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.3 MB)
16:41:43.553 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:4055 (size: 10.4 KB, free: 1988.7 MB)
16:41:43.554 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:41:43.565 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
16:41:43.566 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
16:41:43.603 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
16:41:43.605 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
16:41:43.605 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
16:41:43.606 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
16:41:43.606 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
16:41:43.606 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
16:41:43.607 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
16:41:43.607 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
16:41:43.615 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
16:41:43.615 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
16:41:43.615 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
16:41:43.616 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:41:43.615 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
16:41:43.615 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
16:41:43.615 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
16:41:43.615 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
16:41:43.756 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 37.530104 ms
16:41:43.777 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
16:41:44.902 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.928 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.969 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:41:45.256 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1653 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1610 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1653 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1653 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1610 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1653 bytes result sent to driver
16:41:45.259 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1610 bytes result sent to driver
16:41:45.266 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1658 ms on localhost (executor driver) (1/8)
16:41:45.268 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1662 ms on localhost (executor driver) (2/8)
16:41:45.268 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1663 ms on localhost (executor driver) (3/8)
16:41:45.269 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1662 ms on localhost (executor driver) (4/8)
16:41:45.269 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1662 ms on localhost (executor driver) (5/8)
16:41:45.269 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1664 ms on localhost (executor driver) (6/8)
16:41:45.269 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1675 ms on localhost (executor driver) (7/8)
16:41:48.110 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1782 bytes result sent to driver
16:41:48.111 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 4506 ms on localhost (executor driver) (8/8)
16:41:48.111 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:41:48.112 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseCustomer.scala:59) finished in 4.527 s
16:41:48.112 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:41:48.113 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:41:48.113 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
16:41:48.114 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:41:48.116 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseCustomer.scala:59), which has no missing parents
16:41:48.122 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
16:41:48.123 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
16:41:48.125 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:4055 (size: 2.2 KB, free: 1988.7 MB)
16:41:48.125 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:41:48.126 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0))
16:41:48.126 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
16:41:48.128 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:41:48.129 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
16:41:48.140 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:41:48.141 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
16:41:48.168 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 11678 bytes result sent to driver
16:41:48.168 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 41 ms on localhost (executor driver) (1/1)
16:41:48.168 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:41:48.169 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseCustomer.scala:59) finished in 0.042 s
16:41:48.173 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseCustomer.scala:59, took 4.763907 s
16:41:48.268 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:41:48.275 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:41:48.278 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:41:48.288 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:41:48.381 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:41:48.381 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:41:48.382 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:41:48.384 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:41:48.390 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:41:48.391 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:41:48.391 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-afe93cb9-e97f-4492-9138-a6f3a0d639ec
17:35:20.778 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:35:21.073 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:35:21.091 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
17:35:21.092 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
17:35:21.092 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:35:21.093 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:35:21.093 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
17:35:21.925 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5678.
17:35:21.943 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:35:21.963 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:35:21.967 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:35:21.968 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:35:21.981 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-2f7b3835-75df-4f56-890b-d292d6c33d68
17:35:22.001 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
17:35:22.050 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:35:22.131 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3935ms
17:35:22.194 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:35:22.210 INFO  [main] org.spark_project.jetty.server.Server - Started @4016ms
17:35:22.233 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:35:22.233 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:35:22.258 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
17:35:22.259 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
17:35:22.260 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
17:35:22.260 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
17:35:22.261 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
17:35:22.262 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
17:35:22.262 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
17:35:22.263 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
17:35:22.264 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
17:35:22.265 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
17:35:22.265 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
17:35:22.266 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
17:35:22.266 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
17:35:22.267 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
17:35:22.267 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
17:35:22.268 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
17:35:22.268 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
17:35:22.269 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
17:35:22.269 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
17:35:22.270 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:35:22.277 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
17:35:22.278 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
17:35:22.279 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
17:35:22.280 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
17:35:22.280 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
17:35:22.282 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
17:35:22.359 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:35:22.388 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5719.
17:35:22.390 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:5719
17:35:22.392 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:35:22.425 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 5719, None)
17:35:22.428 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:5719 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 5719, None)
17:35:22.430 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 5719, None)
17:35:22.431 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 5719, None)
17:35:22.595 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
17:35:24.014 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
17:35:24.036 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
17:35:24.037 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
17:35:24.043 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
17:35:24.044 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
17:35:24.045 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
17:35:24.045 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
17:35:24.047 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
17:35:24.560 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:35:25.213 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:35:25.241 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:35:26.484 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:35:27.706 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:35:27.708 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:35:27.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:35:27.926 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:35:28.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:35:28.126 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:35:28.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
17:35:28.140 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:35:28.140 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:35:28.183 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:35:28.184 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:35:28.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
17:35:28.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
17:35:28.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:35:28.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:35:28.683 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/0a4ddf7f-fe27-40c5-9095-5acb5988bf7a_resources
17:35:28.692 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/0a4ddf7f-fe27-40c5-9095-5acb5988bf7a
17:35:28.695 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/0a4ddf7f-fe27-40c5-9095-5acb5988bf7a
17:35:28.699 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/0a4ddf7f-fe27-40c5-9095-5acb5988bf7a/_tmp_space.db
17:35:28.704 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:35:28.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:35:28.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:35:28.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:35:28.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:35:28.737 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:35:28.898 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/e7dc252c-32ee-4243-8985-b2978b4891fe_resources
17:35:28.927 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/e7dc252c-32ee-4243-8985-b2978b4891fe
17:35:28.931 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/e7dc252c-32ee-4243-8985-b2978b4891fe
17:35:28.940 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/e7dc252c-32ee-4243-8985-b2978b4891fe/_tmp_space.db
17:35:28.944 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:35:28.981 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:35:28.989 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:35:29.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:35:29.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:35:29.197 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:35:29.198 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:35:29.289 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:35:29.289 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:35:29.316 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:35:29.343 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:35:29.662 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:35:29.672 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:35:29.673 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:35:29.674 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:35:29.675 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:35:29.675 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:35:29.675 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
17:35:29.676 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
17:35:29.878 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:35:29.878 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:35:29.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:35:29.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:35:29.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:35:29.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:35:29.945 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:29.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:35:29.968 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:35:29.968 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:35:29.973 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:35:29.973 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:35:30.180 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:35:30.182 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
17:35:30.183 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
17:35:30.194 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
17:35:30.198 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:35:30.520 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 202.90663 ms
17:35:30.761 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 41.1947 ms
17:35:30.855 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
17:35:30.956 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
17:35:30.960 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:5719 (size: 26.2 KB, free: 1988.7 MB)
17:35:30.964 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseExposure.scala:65
17:35:30.974 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:35:31.082 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseExposure.scala:65
17:35:31.100 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DwReleaseExposure.scala:65)
17:35:31.103 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseExposure.scala:65) with 1 output partitions
17:35:31.103 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseExposure.scala:65)
17:35:31.103 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
17:35:31.105 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
17:35:31.109 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65), which has no missing parents
17:35:31.201 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 17.6 KB, free 1988.4 MB)
17:35:31.205 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.3 MB)
17:35:31.206 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:5719 (size: 7.5 KB, free: 1988.7 MB)
17:35:31.207 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
17:35:31.222 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:35:31.223 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
17:35:31.271 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:35:31.274 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:35:31.275 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:35:31.275 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:35:31.276 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:35:31.277 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:35:31.277 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:35:31.278 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:35:31.287 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
17:35:31.287 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
17:35:31.287 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
17:35:31.287 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
17:35:31.287 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
17:35:31.287 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
17:35:31.287 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
17:35:31.287 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
17:35:31.395 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:35:31.395 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:35:31.395 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:35:31.395 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:35:31.395 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:35:31.395 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:35:31.395 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:35:31.395 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:35:32.590 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.590 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.590 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.590 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.590 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.590 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.590 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.622 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
17:35:32.640 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:35:32.708 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1457 bytes result sent to driver
17:35:32.708 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1414 bytes result sent to driver
17:35:32.708 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1500 bytes result sent to driver
17:35:32.708 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1500 bytes result sent to driver
17:35:32.708 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1500 bytes result sent to driver
17:35:32.708 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1414 bytes result sent to driver
17:35:32.712 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1414 bytes result sent to driver
17:35:32.717 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1443 ms on localhost (executor driver) (1/8)
17:35:32.719 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1444 ms on localhost (executor driver) (2/8)
17:35:32.719 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1443 ms on localhost (executor driver) (3/8)
17:35:32.720 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1443 ms on localhost (executor driver) (4/8)
17:35:32.720 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1446 ms on localhost (executor driver) (5/8)
17:35:32.720 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1443 ms on localhost (executor driver) (6/8)
17:35:32.720 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1460 ms on localhost (executor driver) (7/8)
17:35:33.533 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1586 bytes result sent to driver
17:35:33.534 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2258 ms on localhost (executor driver) (8/8)
17:35:33.535 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
17:35:33.536 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseExposure.scala:65) finished in 2.287 s
17:35:33.538 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:35:33.545 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:35:33.546 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
17:35:33.546 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:35:33.549 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65), which has no missing parents
17:35:33.556 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
17:35:33.558 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
17:35:33.559 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:5719 (size: 2.2 KB, free: 1988.7 MB)
17:35:33.560 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
17:35:33.561 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
17:35:33.561 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
17:35:33.565 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:35:33.565 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
17:35:33.578 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:35:33.579 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
17:35:33.598 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 4596 bytes result sent to driver
17:35:33.599 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 36 ms on localhost (executor driver) (1/1)
17:35:33.599 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
17:35:33.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseExposure.scala:65) finished in 0.037 s
17:35:33.605 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseExposure.scala:65, took 2.522447 s
17:35:33.635 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
17:35:33.642 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:35:33.642 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:35:33.674 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:35:33.677 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.677 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.679 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:35:33.679 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:35:33.768 ERROR [main] release.etl.dw.DwReleaseCustomer$ - `dw_release`.`dw_release_customer` requires that the data to be inserted have the same number of columns as the target table: target table has 18 column(s) but the inserted data has 8 column(s), including 0 partition column(s) having constant value(s).;
org.apache.spark.sql.AnalysisException: `dw_release`.`dw_release_customer` requires that the data to be inserted have the same number of columns as the target table: target table has 18 column(s) but the inserted data has 8 column(s), including 0 partition column(s) having constant value(s).;
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(rules.scala:325)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:381)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:376)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:376)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:312)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)
	at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:72)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:283)
	at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:269)
	at release.util.SparkHelper$.writetableData(SparkHelper.scala:35)
	at release.etl.dw.DwReleaseExposure$.handleReleaseJob(DwReleaseExposure.scala:68)
	at release.etl.dw.DwReleaseExposure$$anonfun$handleJobs$1.apply(DwReleaseExposure.scala:105)
	at release.etl.dw.DwReleaseExposure$$anonfun$handleJobs$1.apply(DwReleaseExposure.scala:103)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseExposure$.handleJobs(DwReleaseExposure.scala:103)
	at release.etl.dw.DwReleaseExposure$.main(DwReleaseExposure.scala:26)
	at release.etl.dw.DwReleaseExposure.main(DwReleaseExposure.scala)
17:35:33.772 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:35:33.779 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:35:33.781 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
17:35:33.793 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:35:33.877 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:35:33.877 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:35:33.882 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:35:33.884 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:35:33.889 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:35:33.889 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:35:33.890 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-e32e2110-a14a-4eaf-aedf-93c6ed5e93bf
17:36:48.369 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:36:48.655 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:36:48.674 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
17:36:48.674 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
17:36:48.675 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:36:48.675 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:36:48.675 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
17:36:49.524 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5805.
17:36:49.543 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:36:49.578 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:36:49.584 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:36:49.584 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:36:49.596 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-42bece0c-5e93-423e-9472-270492d5dc38
17:36:49.616 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
17:36:49.671 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:36:49.764 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4053ms
17:36:49.827 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:36:49.841 INFO  [main] org.spark_project.jetty.server.Server - Started @4132ms
17:36:49.865 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:36:49.865 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:36:49.887 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
17:36:49.888 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
17:36:49.889 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
17:36:49.889 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
17:36:49.890 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
17:36:49.890 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
17:36:49.891 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
17:36:49.892 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
17:36:49.893 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
17:36:49.893 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
17:36:49.894 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
17:36:49.894 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
17:36:49.895 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
17:36:49.896 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
17:36:49.897 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
17:36:49.898 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
17:36:49.899 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
17:36:49.899 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
17:36:49.900 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
17:36:49.901 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:36:49.906 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
17:36:49.907 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
17:36:49.908 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
17:36:49.909 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
17:36:49.909 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
17:36:49.911 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
17:36:49.988 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:36:50.026 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5846.
17:36:50.027 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:5846
17:36:50.029 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:36:50.065 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 5846, None)
17:36:50.068 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:5846 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 5846, None)
17:36:50.071 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 5846, None)
17:36:50.072 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 5846, None)
17:36:50.280 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
17:36:51.686 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
17:36:51.709 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
17:36:51.709 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
17:36:51.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
17:36:51.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
17:36:51.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
17:36:51.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
17:36:51.718 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
17:36:52.188 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:36:52.859 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:36:52.887 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:36:54.204 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:36:55.290 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:36:55.292 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:36:55.600 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:36:55.604 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:36:55.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:36:55.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:36:55.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
17:36:55.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:36:55.758 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:36:55.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:36:55.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:36:55.806 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
17:36:55.807 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
17:36:55.811 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:36:55.811 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:36:56.301 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/aa8678e5-7212-4e41-81b6-6d8a00b19105_resources
17:36:56.318 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/aa8678e5-7212-4e41-81b6-6d8a00b19105
17:36:56.321 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/aa8678e5-7212-4e41-81b6-6d8a00b19105
17:36:56.326 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/aa8678e5-7212-4e41-81b6-6d8a00b19105/_tmp_space.db
17:36:56.330 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:36:56.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:36:56.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:36:56.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:36:56.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:36:56.364 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:36:56.523 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/a5c0b843-6191-403a-96a5-a196afea4836_resources
17:36:56.529 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a5c0b843-6191-403a-96a5-a196afea4836
17:36:56.534 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/a5c0b843-6191-403a-96a5-a196afea4836
17:36:56.545 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a5c0b843-6191-403a-96a5-a196afea4836/_tmp_space.db
17:36:56.549 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:36:56.577 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:36:56.583 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:36:56.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:36:56.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:36:56.785 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:36:56.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:36:56.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:36:56.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:36:56.910 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.922 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:56.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:36:56.938 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:36:57.268 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:36:57.278 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:36:57.278 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:36:57.279 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:36:57.280 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:36:57.280 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:36:57.281 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
17:36:57.281 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
17:36:57.495 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:36:57.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:36:57.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:36:57.502 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:36:57.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:36:57.536 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:36:57.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:36:57.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:36:57.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:36:57.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:36:57.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:36:57.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:36:57.808 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:36:57.810 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
17:36:57.813 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
17:36:57.826 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
17:36:57.830 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:36:58.197 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 226.880969 ms
17:36:58.439 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 46.176431 ms
17:36:58.525 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
17:36:58.619 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
17:36:58.623 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:5846 (size: 26.2 KB, free: 1988.7 MB)
17:36:58.628 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseExposure.scala:65
17:36:58.635 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:36:58.759 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseExposure.scala:65
17:36:58.776 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DwReleaseExposure.scala:65)
17:36:58.779 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseExposure.scala:65) with 1 output partitions
17:36:58.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseExposure.scala:65)
17:36:58.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
17:36:58.782 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
17:36:58.786 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65), which has no missing parents
17:36:58.865 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 17.6 KB, free 1988.4 MB)
17:36:58.869 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.3 MB)
17:36:58.870 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:5846 (size: 7.5 KB, free: 1988.7 MB)
17:36:58.871 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
17:36:58.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:36:58.883 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
17:36:58.919 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:36:58.921 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:36:58.922 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:36:58.922 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:36:58.922 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:36:58.923 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:36:58.923 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:36:58.923 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:36:58.932 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
17:36:58.932 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
17:36:58.932 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
17:36:58.932 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
17:36:58.932 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
17:36:58.932 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
17:36:58.932 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
17:36:58.932 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
17:36:59.037 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:36:59.037 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:36:59.037 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:36:59.037 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:36:59.037 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:36:59.037 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:36:59.037 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:36:59.037 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:37:00.209 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.209 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.209 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.209 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.209 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.209 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.232 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.282 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:37:00.382 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1457 bytes result sent to driver
17:37:00.382 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1457 bytes result sent to driver
17:37:00.382 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1371 bytes result sent to driver
17:37:00.382 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1414 bytes result sent to driver
17:37:00.382 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1414 bytes result sent to driver
17:37:00.383 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1371 bytes result sent to driver
17:37:00.389 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1414 bytes result sent to driver
17:37:00.395 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1470 ms on localhost (executor driver) (1/8)
17:37:00.398 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1489 ms on localhost (executor driver) (2/8)
17:37:00.398 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1475 ms on localhost (executor driver) (3/8)
17:37:00.399 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1476 ms on localhost (executor driver) (4/8)
17:37:00.399 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1478 ms on localhost (executor driver) (5/8)
17:37:00.399 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1477 ms on localhost (executor driver) (6/8)
17:37:00.399 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1478 ms on localhost (executor driver) (7/8)
17:37:00.857 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
17:37:01.257 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1629 bytes result sent to driver
17:37:01.258 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2336 ms on localhost (executor driver) (8/8)
17:37:01.259 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
17:37:01.259 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseExposure.scala:65) finished in 2.358 s
17:37:01.261 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:37:01.266 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:37:01.267 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
17:37:01.267 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:37:01.270 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65), which has no missing parents
17:37:01.277 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
17:37:01.279 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
17:37:01.281 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:5846 (size: 2.2 KB, free: 1988.7 MB)
17:37:01.281 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
17:37:01.283 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
17:37:01.283 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
17:37:01.285 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:37:01.285 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
17:37:01.296 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:37:01.298 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
17:37:01.316 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 4596 bytes result sent to driver
17:37:01.317 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 33 ms on localhost (executor driver) (1/1)
17:37:01.317 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
17:37:01.318 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseExposure.scala:65) finished in 0.035 s
17:37:01.321 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseExposure.scala:65, took 2.561226 s
17:37:01.346 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
17:37:01.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:37:01.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:37:01.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:37:01.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.380 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.381 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:37:01.381 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:37:01.392 ERROR [main] release.etl.dw.DwReleaseCustomer$ - `dw_release`.`dw_release_customer` requires that the data to be inserted have the same number of columns as the target table: target table has 18 column(s) but the inserted data has 8 column(s), including 0 partition column(s) having constant value(s).;
org.apache.spark.sql.AnalysisException: `dw_release`.`dw_release_customer` requires that the data to be inserted have the same number of columns as the target table: target table has 18 column(s) but the inserted data has 8 column(s), including 0 partition column(s) having constant value(s).;
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(rules.scala:325)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:381)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:376)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:376)
	at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion.apply(rules.scala:312)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)
	at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:72)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)
	at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:283)
	at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:269)
	at release.util.SparkHelper$.writetableData(SparkHelper.scala:35)
	at release.etl.dw.DwReleaseExposure$.handleReleaseJob(DwReleaseExposure.scala:68)
	at release.etl.dw.DwReleaseExposure$$anonfun$handleJobs$1.apply(DwReleaseExposure.scala:105)
	at release.etl.dw.DwReleaseExposure$$anonfun$handleJobs$1.apply(DwReleaseExposure.scala:103)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseExposure$.handleJobs(DwReleaseExposure.scala:103)
	at release.etl.dw.DwReleaseExposure$.main(DwReleaseExposure.scala:26)
	at release.etl.dw.DwReleaseExposure.main(DwReleaseExposure.scala)
17:37:01.399 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:37:01.405 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:37:01.407 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
17:37:01.416 INFO  [dispatcher-event-loop-6] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:37:01.493 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:37:01.615 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:37:01.620 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:37:01.622 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:37:01.627 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:37:01.628 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:37:01.628 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-86f39952-167f-4837-9f6b-6f8c53bc7e24
17:39:49.631 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:39:49.915 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:39:49.934 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
17:39:49.934 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
17:39:49.935 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:39:49.935 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:39:49.935 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
17:39:50.800 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5942.
17:39:50.819 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:39:50.836 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:39:50.839 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:39:50.840 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:39:50.850 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-b100d370-e00f-4c82-8348-291e735df98e
17:39:50.866 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
17:39:50.913 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:39:50.989 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3965ms
17:39:51.059 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:39:51.076 INFO  [main] org.spark_project.jetty.server.Server - Started @4054ms
17:39:51.097 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:39:51.097 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:39:51.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
17:39:51.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
17:39:51.120 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
17:39:51.121 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
17:39:51.122 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
17:39:51.123 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
17:39:51.123 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
17:39:51.124 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
17:39:51.125 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
17:39:51.125 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
17:39:51.126 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
17:39:51.126 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
17:39:51.127 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
17:39:51.127 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
17:39:51.128 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
17:39:51.128 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
17:39:51.129 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
17:39:51.129 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
17:39:51.130 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
17:39:51.130 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:39:51.135 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
17:39:51.137 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
17:39:51.138 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
17:39:51.138 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
17:39:51.139 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
17:39:51.141 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
17:39:51.214 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:39:51.241 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5983.
17:39:51.242 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:5983
17:39:51.243 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:39:51.274 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 5983, None)
17:39:51.278 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:5983 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 5983, None)
17:39:51.281 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 5983, None)
17:39:51.281 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 5983, None)
17:39:51.451 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
17:39:52.872 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
17:39:52.897 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
17:39:52.898 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
17:39:52.904 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
17:39:52.904 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
17:39:52.905 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
17:39:52.905 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
17:39:52.907 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
17:39:53.373 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:39:54.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:39:54.066 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:39:55.143 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:39:56.318 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:39:56.320 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:39:56.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:39:56.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:39:56.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:39:56.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:39:56.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
17:39:56.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:39:56.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:39:56.815 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:39:56.816 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:39:56.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
17:39:56.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
17:39:56.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:39:56.826 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:39:57.321 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/a69a6cea-ed92-40a5-8f06-e0226a181ecd_resources
17:39:57.330 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a69a6cea-ed92-40a5-8f06-e0226a181ecd
17:39:57.334 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/a69a6cea-ed92-40a5-8f06-e0226a181ecd
17:39:57.338 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a69a6cea-ed92-40a5-8f06-e0226a181ecd/_tmp_space.db
17:39:57.342 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:39:57.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:39:57.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:39:57.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:39:57.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:39:57.377 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:39:57.541 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/711d82d7-96c3-4177-b0e0-9582e4265223_resources
17:39:57.546 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/711d82d7-96c3-4177-b0e0-9582e4265223
17:39:57.552 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/711d82d7-96c3-4177-b0e0-9582e4265223
17:39:57.558 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/711d82d7-96c3-4177-b0e0-9582e4265223/_tmp_space.db
17:39:57.561 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:39:57.592 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:39:57.598 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:39:57.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:39:57.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:39:57.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:39:57.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:39:57.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:39:57.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:39:57.931 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.945 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.945 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.946 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:57.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:39:57.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:39:58.276 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:39:58.290 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:39:58.291 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:39:58.292 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:39:58.292 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:39:58.293 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:39:58.293 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
17:39:58.294 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
17:39:58.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:39:58.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:39:58.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:39:58.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:39:58.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:39:58.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:39:58.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.562 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:39:58.562 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:39:58.584 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:39:58.585 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:39:58.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:39:58.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:39:58.787 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:39:58.788 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
17:39:58.790 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
17:39:58.801 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
17:39:58.806 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:39:59.143 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 206.66683 ms
17:39:59.382 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 42.507886 ms
17:39:59.479 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
17:40:00.078 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
17:40:00.081 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:5983 (size: 26.2 KB, free: 1988.7 MB)
17:40:00.085 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseExposure.scala:65
17:40:00.093 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:40:00.190 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseExposure.scala:65
17:40:00.204 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DwReleaseExposure.scala:65)
17:40:00.206 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseExposure.scala:65) with 1 output partitions
17:40:00.207 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseExposure.scala:65)
17:40:00.207 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
17:40:00.209 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
17:40:00.212 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65), which has no missing parents
17:40:00.291 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 17.6 KB, free 1988.4 MB)
17:40:00.294 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.3 MB)
17:40:00.295 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:5983 (size: 7.5 KB, free: 1988.7 MB)
17:40:00.295 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
17:40:00.309 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:40:00.310 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
17:40:00.350 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:40:00.353 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:40:00.354 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:40:00.354 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:40:00.354 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:40:00.355 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:40:00.356 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:40:00.356 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:40:00.365 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
17:40:00.365 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
17:40:00.365 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
17:40:00.365 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
17:40:00.365 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
17:40:00.365 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
17:40:00.365 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
17:40:00.365 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
17:40:00.459 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:40:00.459 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:40:00.459 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:40:00.459 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:40:00.459 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:40:00.459 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:40:00.459 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:40:00.459 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:40:01.696 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.696 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.696 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.696 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.696 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.696 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.696 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.723 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:01.827 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
17:40:01.834 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1457 bytes result sent to driver
17:40:01.834 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1457 bytes result sent to driver
17:40:01.834 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1457 bytes result sent to driver
17:40:01.834 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1457 bytes result sent to driver
17:40:01.834 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1457 bytes result sent to driver
17:40:01.834 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1457 bytes result sent to driver
17:40:01.837 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1457 bytes result sent to driver
17:40:01.845 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1490 ms on localhost (executor driver) (1/8)
17:40:01.847 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1492 ms on localhost (executor driver) (2/8)
17:40:01.848 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1496 ms on localhost (executor driver) (3/8)
17:40:01.848 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1492 ms on localhost (executor driver) (4/8)
17:40:01.849 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1509 ms on localhost (executor driver) (5/8)
17:40:01.849 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1494 ms on localhost (executor driver) (6/8)
17:40:01.849 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1495 ms on localhost (executor driver) (7/8)
17:40:02.495 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1586 bytes result sent to driver
17:40:02.496 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2142 ms on localhost (executor driver) (8/8)
17:40:02.497 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
17:40:02.498 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseExposure.scala:65) finished in 2.166 s
17:40:02.500 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:40:02.505 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:40:02.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
17:40:02.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:40:02.509 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65), which has no missing parents
17:40:02.516 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
17:40:02.517 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
17:40:02.519 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:5983 (size: 2.2 KB, free: 1988.7 MB)
17:40:02.520 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
17:40:02.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
17:40:02.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
17:40:02.523 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:40:02.524 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
17:40:02.535 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:40:02.537 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
17:40:02.555 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 4596 bytes result sent to driver
17:40:02.556 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 34 ms on localhost (executor driver) (1/1)
17:40:02.556 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
17:40:02.557 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseExposure.scala:65) finished in 0.034 s
17:40:02.560 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseExposure.scala:65, took 2.370420 s
17:40:02.586 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
17:40:02.592 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:02.592 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:02.613 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.613 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.613 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.613 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.613 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.614 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.614 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.614 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:40:02.685 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1
17:40:02.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:40:02.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:40:02.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:40:02.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:40:02.860 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:40:02.860 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:40:02.880 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.880 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:02.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:40:02.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:40:02.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:40:02.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:40:02.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:40:02.919 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:40:02.919 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
17:40:02.920 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
17:40:02.920 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
17:40:02.920 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:40:02.929 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:40:02.931 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:40:02.958 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 311.6 KB, free 1988.0 MB)
17:40:02.974 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
17:40:02.975 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:5983 (size: 26.2 KB, free: 1988.6 MB)
17:40:02.976 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
17:40:02.976 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:40:03.040 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
17:40:03.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:35)
17:40:03.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 4 output partitions
17:40:03.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:35)
17:40:03.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
17:40:03.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
17:40:03.042 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35), which has no missing parents
17:40:03.045 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.6 KB, free 1988.0 MB)
17:40:03.047 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.0 MB)
17:40:03.048 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:5983 (size: 7.5 KB, free: 1988.6 MB)
17:40:03.048 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
17:40:03.049 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:40:03.050 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 8 tasks
17:40:03.050 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 9, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:40:03.051 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 10, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:40:03.051 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 11, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:40:03.052 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 12, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:40:03.053 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 13, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:40:03.053 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 14, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:40:03.053 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 15, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:40:03.054 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 16, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:40:03.054 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 9)
17:40:03.054 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 10)
17:40:03.054 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 11)
17:40:03.054 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 14)
17:40:03.054 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 12)
17:40:03.054 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 15)
17:40:03.054 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 13)
17:40:03.054 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 16)
17:40:03.059 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:40:03.059 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:40:03.061 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:40:03.062 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:40:03.064 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:40:03.065 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:40:03.067 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:40:03.068 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:40:03.084 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.084 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.087 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.103 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 10). 1328 bytes result sent to driver
17:40:03.104 INFO  [Executor task launch worker for task 14] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.104 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.108 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 10) in 57 ms on localhost (executor driver) (1/8)
17:40:03.111 INFO  [Executor task launch worker for task 13] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.120 INFO  [Executor task launch worker for task 15] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.124 INFO  [Executor task launch worker for task 12] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:40:03.149 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 14). 1371 bytes result sent to driver
17:40:03.151 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 14) in 98 ms on localhost (executor driver) (2/8)
17:40:03.154 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 9). 1328 bytes result sent to driver
17:40:03.155 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 9) in 105 ms on localhost (executor driver) (3/8)
17:40:03.160 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 11). 1371 bytes result sent to driver
17:40:03.161 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 11) in 110 ms on localhost (executor driver) (4/8)
17:40:03.178 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 16). 1328 bytes result sent to driver
17:40:03.180 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 16) in 127 ms on localhost (executor driver) (5/8)
17:40:03.182 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 15). 1328 bytes result sent to driver
17:40:03.183 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 15) in 130 ms on localhost (executor driver) (6/8)
17:40:03.187 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 13). 1371 bytes result sent to driver
17:40:03.189 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 13) in 136 ms on localhost (executor driver) (7/8)
17:40:03.442 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 12). 1586 bytes result sent to driver
17:40:03.443 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 12) in 392 ms on localhost (executor driver) (8/8)
17:40:03.443 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
17:40:03.443 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:35) finished in 0.393 s
17:40:03.443 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:40:03.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:40:03.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
17:40:03.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:40:03.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
17:40:03.503 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 167.4 KB, free 1987.8 MB)
17:40:03.506 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.8 MB)
17:40:03.506 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:5983 (size: 62.1 KB, free: 1988.6 MB)
17:40:03.507 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
17:40:03.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
17:40:03.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
17:40:03.508 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 17, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:40:03.508 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 18, localhost, executor driver, partition 1, ANY, 4726 bytes)
17:40:03.509 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 19, localhost, executor driver, partition 2, ANY, 4726 bytes)
17:40:03.509 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 20, localhost, executor driver, partition 3, ANY, 4726 bytes)
17:40:03.509 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 19)
17:40:03.509 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 17)
17:40:03.509 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 18)
17:40:03.509 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 20)
17:40:03.542 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:40:03.542 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:40:03.542 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:40:03.543 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
17:40:03.544 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:40:03.544 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:40:03.544 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:40:03.544 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:40:03.561 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.046231 ms
17:40:03.586 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.833887 ms
17:40:03.652 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:40:03.652 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:40:03.652 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:40:03.653 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:40:03.654 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:40:03.654 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:40:03.654 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:40:03.654 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:40:03.665 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.7844 ms
17:40:03.723 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.06906 ms
17:40:03.740 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.776998 ms
17:40:03.824 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@792a51af
17:40:03.826 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2125daff
17:40:03.826 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@574a6a1c
17:40:03.826 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d3b21b7
17:40:03.836 INFO  [Executor task launch worker for task 18] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17:40:03.841 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:40:03.841 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:40:03.841 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:40:03.841 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:40:03.841 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174003_0003_m_000003_0/bdp_day=20190924/part-00003-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:40:03.841 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174003_0003_m_000001_0/bdp_day=20190924/part-00001-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:40:03.841 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174003_0003_m_000002_0/bdp_day=20190924/part-00002-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:40:03.841 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174003_0003_m_000000_0/bdp_day=20190924/part-00000-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:40:03.844 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression set to false
17:40:03.844 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression set to false
17:40:03.844 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression set to false
17:40:03.844 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression set to false
17:40:03.845 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:40:03.845 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:40:03.845 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:40:03.845 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:40:03.846 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:40:03.846 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:40:03.846 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:40:03.846 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:40:03.846 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:40:03.846 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:40:03.846 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:40:03.846 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:40:03.846 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:40:03.847 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Validation is off
17:40:03.847 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Validation is off
17:40:03.847 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Validation is off
17:40:03.847 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Validation is off
17:40:03.847 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:40:03.847 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:40:03.847 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:40:03.847 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:40:04.139 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1e3d61b0
17:40:04.139 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@52d610b6
17:40:04.140 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@57777f67
17:40:04.140 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@44d73399
17:40:04.157 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:5983 in memory (size: 7.5 KB, free: 1988.6 MB)
17:40:04.191 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:5983 in memory (size: 2.2 KB, free: 1988.6 MB)
17:40:04.370 INFO  [Executor task launch worker for task 20] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,106
17:40:04.370 INFO  [Executor task launch worker for task 17] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,496
17:40:04.371 INFO  [Executor task launch worker for task 18] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,186
17:40:04.372 INFO  [Executor task launch worker for task 19] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 937,961
17:40:04.474 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.475 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.475 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.475 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.477 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.477 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.477 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.477 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.478 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.478 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.478 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.478 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
17:40:04.478 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
17:40:04.478 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,345 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
17:40:04.478 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
17:40:04.479 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
17:40:04.479 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
17:40:04.479 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.479 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:40:04.479 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.479 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.479 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:40:04.479 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
17:40:04.480 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:40:04.480 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:40:04.480 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
17:40:04.480 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
17:40:04.481 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:40:04.706 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174003_0003_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/task_20190925174003_0003_m_000002
17:40:04.706 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174003_0003_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/task_20190925174003_0003_m_000003
17:40:04.706 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174003_0003_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/task_20190925174003_0003_m_000000
17:40:04.706 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174003_0003_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/_temporary/0/task_20190925174003_0003_m_000001
17:40:04.706 INFO  [Executor task launch worker for task 20] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174003_0003_m_000003_0: Committed
17:40:04.706 INFO  [Executor task launch worker for task 18] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174003_0003_m_000001_0: Committed
17:40:04.706 INFO  [Executor task launch worker for task 17] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174003_0003_m_000000_0: Committed
17:40:04.706 INFO  [Executor task launch worker for task 19] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174003_0003_m_000002_0: Committed
17:40:04.710 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 17). 2525 bytes result sent to driver
17:40:04.710 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 18). 2525 bytes result sent to driver
17:40:04.710 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 20). 2568 bytes result sent to driver
17:40:04.710 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 19). 2525 bytes result sent to driver
17:40:04.712 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 17) in 1204 ms on localhost (executor driver) (1/4)
17:40:04.712 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 20) in 1203 ms on localhost (executor driver) (2/4)
17:40:04.712 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 19) in 1204 ms on localhost (executor driver) (3/4)
17:40:04.712 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 18) in 1204 ms on localhost (executor driver) (4/4)
17:40:04.712 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
17:40:04.713 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:35) finished in 1.205 s
17:40:04.714 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 1.674305 s
17:40:04.765 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
17:40:04.766 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:04.766 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:04.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:04.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:04.810 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:04.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:04.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:04.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:04.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:04.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:04.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:04.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:40:04.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:04.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:04.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:40:04.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:40:04.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:40:04.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:40:04.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:40:04.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:40:04.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:40:04.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:40:04.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:04.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:04.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
17:40:04.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
17:40:04.949 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
17:40:04.965 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/bdp_day=20190924/part-00000-183a6b06-f46e-42ba-804f-419e4c886108.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00000-183a6b06-f46e-42ba-804f-419e4c886108.c000, Status:true
17:40:04.981 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/bdp_day=20190924/part-00001-183a6b06-f46e-42ba-804f-419e4c886108.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00001-183a6b06-f46e-42ba-804f-419e4c886108.c000, Status:true
17:40:04.996 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/bdp_day=20190924/part-00002-183a6b06-f46e-42ba-804f-419e4c886108.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00002-183a6b06-f46e-42ba-804f-419e4c886108.c000, Status:true
17:40:05.011 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/bdp_day=20190924/part-00003-183a6b06-f46e-42ba-804f-419e4c886108.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00003-183a6b06-f46e-42ba-804f-419e4c886108.c000, Status:true
17:40:05.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
17:40:05.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
17:40:05.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_exposure
17:40:05.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_exposure	
17:40:05.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
17:40:05.096 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
17:40:05.098 WARN  [main] hive.log - Updated size to 2286733
17:40:05.300 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-40-02_683_6047621716177257192-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
17:40:05.306 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
17:40:05.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
17:40:05.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
17:40:05.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:05.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:05.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:05.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:05.362 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.362 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.362 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.362 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.363 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.363 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.363 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.363 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:40:05.368 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_exposure (inference mode: INFER_AND_SAVE)
17:40:05.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
17:40:05.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
17:40:05.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:05.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:05.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:05.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:05.418 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.418 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.418 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.418 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.419 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.419 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.419 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:05.419 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:40:05.420 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_exposure
17:40:05.420 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_exposure	
17:40:05.511 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
17:40:05.512 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 1 output partitions
17:40:05.512 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (insertInto at SparkHelper.scala:35)
17:40:05.512 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
17:40:05.512 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
17:40:05.512 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:35), which has no missing parents
17:40:05.528 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 74.4 KB, free 1987.7 MB)
17:40:05.531 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1987.7 MB)
17:40:05.531 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:5983 (size: 26.8 KB, free: 1988.6 MB)
17:40:05.532 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
17:40:05.532 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
17:40:05.533 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
17:40:05.536 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 5050 bytes)
17:40:05.537 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 21)
17:40:06.097 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 21). 1691 bytes result sent to driver
17:40:06.098 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 21) in 565 ms on localhost (executor driver) (1/1)
17:40:06.098 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
17:40:06.099 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (insertInto at SparkHelper.scala:35) finished in 0.566 s
17:40:06.100 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.587606 s
17:40:06.105 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_exposure
17:40:06.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
17:40:06.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
17:40:06.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:06.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:06.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:06.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:06.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:40:06.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:06.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:06.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:06.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:06.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:40:06.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:40:06.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:40:06.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:40:06.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_exposure newtbl=dw_release_exposure
17:40:06.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_exposure newtbl=dw_release_exposure	
17:40:06.381 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:40:06.390 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:40:06.392 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
17:40:06.404 INFO  [dispatcher-event-loop-4] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:40:06.515 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:40:06.516 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:40:06.517 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:40:06.520 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:40:06.525 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:40:06.525 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:40:06.526 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-77dec5b5-46fe-41db-8db8-375086b2f747
17:41:32.299 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:41:32.592 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:41:32.610 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
17:41:32.611 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
17:41:32.611 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:41:32.612 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:41:32.612 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
17:41:33.441 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6074.
17:41:33.458 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:41:33.476 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:41:33.479 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:41:33.479 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:41:33.489 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-fec1f173-22db-48c7-8aea-4c0ed2dc6197
17:41:33.507 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
17:41:33.554 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:41:33.632 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3863ms
17:41:33.694 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:41:33.710 INFO  [main] org.spark_project.jetty.server.Server - Started @3942ms
17:41:33.732 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:41:33.732 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:41:33.755 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
17:41:33.756 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
17:41:33.756 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
17:41:33.757 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
17:41:33.758 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
17:41:33.758 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
17:41:33.759 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
17:41:33.760 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
17:41:33.761 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
17:41:33.761 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
17:41:33.762 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
17:41:33.762 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
17:41:33.763 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
17:41:33.763 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
17:41:33.764 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
17:41:33.764 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
17:41:33.765 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
17:41:33.765 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
17:41:33.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
17:41:33.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:41:33.773 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
17:41:33.773 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
17:41:33.774 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
17:41:33.775 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
17:41:33.775 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
17:41:33.778 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
17:41:33.848 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:41:33.876 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6115.
17:41:33.876 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:6115
17:41:33.877 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:41:33.910 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 6115, None)
17:41:33.912 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:6115 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 6115, None)
17:41:33.915 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 6115, None)
17:41:33.916 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 6115, None)
17:41:34.089 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
17:41:35.540 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
17:41:35.565 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
17:41:35.566 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
17:41:35.572 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
17:41:35.573 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
17:41:35.574 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
17:41:35.575 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
17:41:35.577 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
17:41:36.044 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:41:36.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:41:36.761 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:41:37.934 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:41:39.265 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:41:39.267 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:41:39.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:41:39.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:41:39.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:41:39.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:41:39.630 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
17:41:39.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:41:39.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:41:39.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:41:39.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:41:39.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
17:41:39.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
17:41:39.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:41:39.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:41:40.245 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/1dbd862a-6886-4d9c-a099-232bfa453e04_resources
17:41:40.285 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/1dbd862a-6886-4d9c-a099-232bfa453e04
17:41:40.288 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/1dbd862a-6886-4d9c-a099-232bfa453e04
17:41:40.293 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/1dbd862a-6886-4d9c-a099-232bfa453e04/_tmp_space.db
17:41:40.297 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:41:40.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:40.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:40.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:41:40.327 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:41:40.331 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:41:40.525 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/71377428-ba42-49e9-8b79-55699f79d1bd_resources
17:41:40.543 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/71377428-ba42-49e9-8b79-55699f79d1bd
17:41:40.548 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/71377428-ba42-49e9-8b79-55699f79d1bd
17:41:40.558 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/71377428-ba42-49e9-8b79-55699f79d1bd/_tmp_space.db
17:41:40.562 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:41:40.590 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:41:40.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:41:40.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:41:40.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:41:40.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:41:40.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:41:40.889 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:41:40.889 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:41:40.920 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.931 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.932 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.932 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.933 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.933 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.933 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.934 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.934 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:40.934 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:41:40.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:41:41.257 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:41:41.267 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:41:41.268 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:41:41.268 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:41:41.269 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:41:41.269 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:41:41.270 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
17:41:41.285 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
17:41:41.309 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
17:41:41.312 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
17:41:41.313 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
17:41:41.314 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
17:41:41.315 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
17:41:41.316 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
17:41:41.317 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
17:41:41.318 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
17:41:41.319 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
17:41:41.319 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
17:41:41.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.344 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.356 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.390 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.403 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:41:41.403 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:41:41.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:41:41.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:41:41.704 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:41:41.704 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:41:41.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:41:41.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:41:41.755 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.756 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.756 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.756 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.756 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.756 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.757 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.757 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.757 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:41.757 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:41:41.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:41:41.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:41:41.779 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:41:41.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:41:41.953 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:41:41.956 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
17:41:41.958 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
17:41:41.969 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
17:41:41.974 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:41:42.368 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 244.321773 ms
17:41:42.612 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 45.952036 ms
17:41:42.705 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
17:41:43.291 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
17:41:43.295 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:6115 (size: 26.3 KB, free: 1988.7 MB)
17:41:43.300 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseCustomer.scala:59
17:41:43.308 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:41:43.414 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseCustomer.scala:59
17:41:43.430 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DwReleaseCustomer.scala:59)
17:41:43.432 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseCustomer.scala:59) with 1 output partitions
17:41:43.432 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseCustomer.scala:59)
17:41:43.433 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
17:41:43.434 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
17:41:43.440 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseCustomer.scala:59), which has no missing parents
17:41:43.574 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1988.3 MB)
17:41:43.580 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.3 MB)
17:41:43.604 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:6115 (size: 10.4 KB, free: 1988.7 MB)
17:41:43.605 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
17:41:43.613 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
17:41:43.622 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:41:43.623 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
17:41:43.659 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:41:43.661 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:41:43.662 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:41:43.662 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:41:43.663 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:41:43.664 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:41:43.664 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:41:43.665 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:41:43.674 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
17:41:43.674 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
17:41:43.674 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
17:41:43.674 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
17:41:43.674 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
17:41:43.674 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
17:41:43.674 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
17:41:43.674 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
17:41:43.823 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.989665 ms
17:41:43.852 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:41:43.852 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:41:43.852 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:41:43.852 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:41:43.852 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:41:43.852 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:41:43.852 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:41:43.852 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:41:45.013 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.013 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.013 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.013 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.013 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.013 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.013 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.041 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:45.133 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1567 bytes result sent to driver
17:41:45.133 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1524 bytes result sent to driver
17:41:45.133 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1567 bytes result sent to driver
17:41:45.133 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1610 bytes result sent to driver
17:41:45.133 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1567 bytes result sent to driver
17:41:45.133 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1610 bytes result sent to driver
17:41:45.134 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1524 bytes result sent to driver
17:41:45.353 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1689 ms on localhost (executor driver) (1/8)
17:41:45.356 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1696 ms on localhost (executor driver) (2/8)
17:41:45.357 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1693 ms on localhost (executor driver) (3/8)
17:41:45.357 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1693 ms on localhost (executor driver) (4/8)
17:41:45.357 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1706 ms on localhost (executor driver) (5/8)
17:41:45.357 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1694 ms on localhost (executor driver) (6/8)
17:41:45.357 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1696 ms on localhost (executor driver) (7/8)
17:41:48.249 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1782 bytes result sent to driver
17:41:48.250 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 4588 ms on localhost (executor driver) (8/8)
17:41:48.252 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
17:41:48.252 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseCustomer.scala:59) finished in 4.611 s
17:41:48.253 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:41:48.253 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:41:48.254 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
17:41:48.254 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:41:48.256 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseCustomer.scala:59), which has no missing parents
17:41:48.261 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
17:41:48.262 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
17:41:48.264 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:6115 (size: 2.2 KB, free: 1988.7 MB)
17:41:48.264 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
17:41:48.265 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0))
17:41:48.265 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
17:41:48.268 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:41:48.268 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
17:41:48.280 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:41:48.281 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
17:41:48.308 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 11764 bytes result sent to driver
17:41:48.309 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 43 ms on localhost (executor driver) (1/1)
17:41:48.309 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
17:41:48.309 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseCustomer.scala:59) finished in 0.043 s
17:41:48.313 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseCustomer.scala:59, took 4.898211 s
17:41:48.345 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
17:41:48.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:41:48.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:41:48.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:41:48.444 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1
17:41:48.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:41:48.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:41:48.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:41:48.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:41:48.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:41:48.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:41:48.696 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.697 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.697 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.698 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.698 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:48.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:41:48.701 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:41:48.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:41:48.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:41:48.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:41:48.742 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:41:48.743 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
17:41:48.743 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
17:41:48.743 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
17:41:48.744 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:41:48.760 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:41:48.761 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:41:48.806 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.901943 ms
17:41:48.816 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
17:41:48.835 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
17:41:48.835 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:6115 (size: 26.3 KB, free: 1988.6 MB)
17:41:48.837 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
17:41:48.837 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:41:48.910 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
17:41:48.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:35)
17:41:48.911 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 4 output partitions
17:41:48.911 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:35)
17:41:48.911 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
17:41:48.911 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
17:41:48.912 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
17:41:48.917 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 24.0 KB, free 1988.0 MB)
17:41:48.918 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.0 MB)
17:41:48.919 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:6115 (size: 10.4 KB, free: 1988.6 MB)
17:41:48.920 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
17:41:48.920 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:41:48.920 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 8 tasks
17:41:48.921 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 9, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:41:48.922 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 10, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:41:48.922 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 11, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:41:48.922 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 12, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:41:48.923 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 13, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:41:48.924 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 14, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:41:48.924 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 15, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:41:48.924 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 16, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:41:48.925 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 9)
17:41:48.925 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 11)
17:41:48.925 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 10)
17:41:48.925 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 12)
17:41:48.925 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 13)
17:41:48.925 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 15)
17:41:48.925 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 14)
17:41:48.925 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 16)
17:41:48.934 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:41:48.934 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:41:48.937 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:41:48.939 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:41:48.939 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:41:48.941 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:41:48.944 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:41:48.947 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:41:48.955 INFO  [Executor task launch worker for task 14] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:48.982 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 14). 1524 bytes result sent to driver
17:41:48.982 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:48.983 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 14) in 59 ms on localhost (executor driver) (1/8)
17:41:48.986 INFO  [Executor task launch worker for task 12] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:49.007 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 16). 1524 bytes result sent to driver
17:41:49.010 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 16) in 86 ms on localhost (executor driver) (2/8)
17:41:49.020 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:49.021 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:49.023 INFO  [Executor task launch worker for task 13] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:49.065 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 9). 1524 bytes result sent to driver
17:41:49.066 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:49.069 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 9) in 148 ms on localhost (executor driver) (3/8)
17:41:49.081 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 13). 1524 bytes result sent to driver
17:41:49.083 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 13) in 160 ms on localhost (executor driver) (4/8)
17:41:49.086 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 11). 1481 bytes result sent to driver
17:41:49.086 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 11) in 164 ms on localhost (executor driver) (5/8)
17:41:49.095 INFO  [Executor task launch worker for task 15] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
17:41:49.104 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 10). 1481 bytes result sent to driver
17:41:49.105 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 10) in 184 ms on localhost (executor driver) (6/8)
17:41:49.132 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 15). 1567 bytes result sent to driver
17:41:49.134 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 15) in 210 ms on localhost (executor driver) (7/8)
17:41:49.829 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:6115 in memory (size: 10.4 KB, free: 1988.6 MB)
17:41:49.832 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
17:41:49.833 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:6115 in memory (size: 2.2 KB, free: 1988.6 MB)
17:41:49.834 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
17:41:49.835 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:6115 in memory (size: 26.3 KB, free: 1988.7 MB)
17:41:49.836 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
17:41:50.007 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
17:41:50.007 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
17:41:50.007 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
17:41:50.007 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
17:41:50.007 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
17:41:50.007 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
17:41:50.007 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
17:41:52.291 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 12). 1739 bytes result sent to driver
17:41:52.292 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 12) in 3370 ms on localhost (executor driver) (8/8)
17:41:52.292 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
17:41:52.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:35) finished in 3.372 s
17:41:52.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:41:52.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:41:52.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
17:41:52.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:41:52.294 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35), which has no missing parents
17:41:52.341 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
17:41:52.345 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 66.1 KB, free 1988.1 MB)
17:41:52.345 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:6115 (size: 66.1 KB, free: 1988.6 MB)
17:41:52.346 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
17:41:52.346 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
17:41:52.347 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
17:41:52.347 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 17, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:41:52.348 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 18, localhost, executor driver, partition 1, ANY, 4726 bytes)
17:41:52.348 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 19, localhost, executor driver, partition 2, ANY, 4726 bytes)
17:41:52.348 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 20, localhost, executor driver, partition 3, ANY, 4726 bytes)
17:41:52.349 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 19)
17:41:52.349 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 18)
17:41:52.349 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 17)
17:41:52.349 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 20)
17:41:52.397 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:41:52.397 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:41:52.397 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
17:41:52.397 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
17:41:52.397 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:41:52.397 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:41:52.398 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:41:52.399 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
17:41:52.418 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.940946 ms
17:41:52.436 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.074379 ms
17:41:52.562 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:41:52.562 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:41:52.562 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:41:52.562 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:41:52.562 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:41:52.563 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:41:52.564 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:41:52.565 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:41:52.575 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.919117 ms
17:41:52.632 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.299675 ms
17:41:52.650 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.718431 ms
17:41:52.734 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51246c38
17:41:52.735 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@672c6798
17:41:52.735 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7be202e
17:41:52.735 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62cced95
17:41:52.746 INFO  [Executor task launch worker for task 20] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17:41:52.751 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:41:52.751 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:41:52.751 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:41:52.751 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:41:52.751 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174152_0003_m_000001_0/bdp_day=20190924/part-00001-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000
17:41:52.751 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174152_0003_m_000000_0/bdp_day=20190924/part-00000-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000
17:41:52.751 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174152_0003_m_000002_0/bdp_day=20190924/part-00002-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000
17:41:52.751 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174152_0003_m_000003_0/bdp_day=20190924/part-00003-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000
17:41:52.754 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression set to false
17:41:52.754 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression set to false
17:41:52.754 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression set to false
17:41:52.754 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression set to false
17:41:52.754 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:41:52.754 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:41:52.755 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:41:52.755 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:41:52.756 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:41:52.756 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:41:52.756 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:41:52.756 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:41:52.756 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:41:52.756 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:41:52.756 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:41:52.756 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:41:52.756 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:41:52.756 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Validation is off
17:41:52.756 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Validation is off
17:41:52.757 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Validation is off
17:41:52.757 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Validation is off
17:41:52.757 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:41:52.757 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:41:52.757 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:41:52.757 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:41:52.945 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@557fabea
17:41:52.946 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@21730f72
17:41:52.946 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@21e73010
17:41:52.947 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@4938fa2a
17:41:53.345 INFO  [Executor task launch worker for task 17] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,597
17:41:53.346 INFO  [Executor task launch worker for task 20] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,984
17:41:53.348 INFO  [Executor task launch worker for task 19] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,815
17:41:53.352 INFO  [Executor task launch worker for task 18] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,547
17:41:53.530 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.530 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.530 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.530 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.532 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.532 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.532 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.532 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.533 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.533 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.533 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.533 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.533 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
17:41:53.533 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
17:41:53.533 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
17:41:53.533 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,447 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
17:41:53.533 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
17:41:53.534 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
17:41:53.536 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
17:41:53.536 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.536 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
17:41:53.536 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.537 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.537 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
17:41:53.537 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.538 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
17:41:53.538 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.538 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,446 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
17:41:53.538 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.538 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
17:41:53.539 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 3,213B for [area_code] BINARY: 21,446 values, 3,172B raw, 3,172B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
17:41:53.539 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 2,789B for [gender] BINARY: 21,446 values, 2,758B raw, 2,758B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
17:41:53.539 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
17:41:53.539 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 3,227B for [area_code] BINARY: 21,446 values, 3,186B raw, 3,186B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
17:41:53.539 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 2,786B for [gender] BINARY: 21,447 values, 2,755B raw, 2,755B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
17:41:53.539 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
17:41:53.539 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
17:41:53.540 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
17:41:53.540 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 3,222B for [area_code] BINARY: 21,447 values, 3,181B raw, 3,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
17:41:53.540 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
17:41:53.540 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.540 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
17:41:53.540 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.540 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
17:41:53.541 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.541 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
17:41:53.541 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
17:41:53.541 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
17:41:53.541 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.541 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.541 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
17:41:53.541 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:41:53.541 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
17:41:53.542 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.542 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:41:53.542 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
17:41:53.542 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:41:53.543 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
17:41:53.544 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 41 entries, 164B raw, 41B comp}
17:41:53.545 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 2,784B for [gender] BINARY: 21,446 values, 2,753B raw, 2,753B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 10B raw, 2B comp}
17:41:53.546 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 3,247B for [area_code] BINARY: 21,446 values, 3,206B raw, 3,206B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 20B raw, 2B comp}
17:41:53.546 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 479 entries, 7,244B raw, 479B comp}
17:41:53.547 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 339 entries, 5,248B raw, 339B comp}
17:41:53.547 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.548 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 71B raw, 10B comp}
17:41:53.548 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 10 entries, 61B raw, 10B comp}
17:41:53.549 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 5 entries, 40B raw, 5B comp}
17:41:53.549 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:41:53.778 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174152_0003_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/task_20190925174152_0003_m_000000
17:41:53.778 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174152_0003_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/task_20190925174152_0003_m_000002
17:41:53.779 INFO  [Executor task launch worker for task 17] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174152_0003_m_000000_0: Committed
17:41:53.779 INFO  [Executor task launch worker for task 19] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174152_0003_m_000002_0: Committed
17:41:53.782 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 19). 2615 bytes result sent to driver
17:41:53.782 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 17). 2615 bytes result sent to driver
17:41:53.783 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 19) in 1435 ms on localhost (executor driver) (1/4)
17:41:53.783 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 17) in 1436 ms on localhost (executor driver) (2/4)
17:41:54.189 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174152_0003_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/task_20190925174152_0003_m_000003
17:41:54.189 INFO  [Executor task launch worker for task 20] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174152_0003_m_000003_0: Committed
17:41:54.189 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174152_0003_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/_temporary/0/task_20190925174152_0003_m_000001
17:41:54.190 INFO  [Executor task launch worker for task 18] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174152_0003_m_000001_0: Committed
17:41:54.190 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 18). 2572 bytes result sent to driver
17:41:54.190 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 20). 2615 bytes result sent to driver
17:41:54.191 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 18) in 1843 ms on localhost (executor driver) (3/4)
17:41:54.191 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 20) in 1843 ms on localhost (executor driver) (4/4)
17:41:54.191 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
17:41:54.191 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:35) finished in 1.844 s
17:41:54.192 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 5.282915 s
17:41:54.240 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
17:41:54.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:41:54.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:41:54.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:41:54.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:41:54.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.293 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.293 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.293 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.293 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:54.293 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:41:54.296 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:41:54.296 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:41:54.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:41:54.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:41:54.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:41:54.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:41:54.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:41:54.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:41:54.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:41:54.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:41:54.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:41:54.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:41:54.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
17:41:54.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
17:41:54.407 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00000-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
17:41:54.412 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:41:54.437 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00001-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
17:41:54.438 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:41:54.501 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00002-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
17:41:54.502 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:41:54.557 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00003-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
17:41:54.558 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:41:54.639 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
17:41:54.663 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/bdp_day=20190924/part-00000-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00000-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, Status:true
17:41:54.679 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/bdp_day=20190924/part-00001-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00001-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, Status:true
17:41:54.690 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/bdp_day=20190924/part-00002-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00002-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, Status:true
17:41:54.704 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/bdp_day=20190924/part-00003-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00003-a3ab6f96-fed4-45f1-b0a7-818512332bfe.c000, Status:true
17:41:54.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
17:41:54.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
17:41:54.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
17:41:54.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
17:41:54.738 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
17:41:54.794 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
17:41:54.797 WARN  [main] hive.log - Updated size to 5782457
17:41:55.017 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_17-41-48_443_310600674047976667-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
17:41:55.024 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
17:41:55.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
17:41:55.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
17:41:55.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:41:55.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:41:55.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
17:41:55.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
17:41:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:41:55.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:41:55.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:41:55.109 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:41:55.116 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:41:55.118 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
17:41:55.129 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:41:55.244 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:41:55.244 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:41:55.245 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:41:55.248 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:41:55.253 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:41:55.254 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:41:55.255 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-a169492f-ed17-4848-8189-c526ed9a1da2
17:42:53.881 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:42:54.171 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:42:54.191 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
17:42:54.192 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
17:42:54.192 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:42:54.193 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:42:54.193 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
17:42:54.982 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6224.
17:42:55.001 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:42:55.019 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:42:55.022 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:42:55.022 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:42:55.033 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-79a13bf9-d705-489d-9d89-551423f24bf2
17:42:55.048 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
17:42:55.102 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:42:55.202 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3817ms
17:42:55.276 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:42:55.294 INFO  [main] org.spark_project.jetty.server.Server - Started @3912ms
17:42:55.319 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:42:55.319 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:42:55.344 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
17:42:55.345 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
17:42:55.345 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
17:42:55.346 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
17:42:55.346 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
17:42:55.347 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
17:42:55.347 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
17:42:55.348 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
17:42:55.349 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
17:42:55.350 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
17:42:55.350 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
17:42:55.350 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
17:42:55.351 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
17:42:55.351 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
17:42:55.352 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
17:42:55.352 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
17:42:55.353 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
17:42:55.353 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
17:42:55.354 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
17:42:55.354 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:42:55.360 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
17:42:55.361 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
17:42:55.362 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
17:42:55.363 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
17:42:55.364 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
17:42:55.366 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
17:42:55.447 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:42:55.482 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6265.
17:42:55.483 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:6265
17:42:55.484 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:42:55.525 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 6265, None)
17:42:55.528 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:6265 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 6265, None)
17:42:55.530 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 6265, None)
17:42:55.531 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 6265, None)
17:42:55.705 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
17:42:57.284 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
17:42:57.318 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
17:42:57.320 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
17:42:57.329 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL,null,AVAILABLE,@Spark}
17:42:57.330 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL/json,null,AVAILABLE,@Spark}
17:42:57.330 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution,null,AVAILABLE,@Spark}
17:42:57.331 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23468512{/SQL/execution/json,null,AVAILABLE,@Spark}
17:42:57.332 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53747c4a{/static/sql,null,AVAILABLE,@Spark}
17:42:57.848 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:42:58.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:42:58.566 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:42:59.792 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:43:01.133 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:43:01.141 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:43:01.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:43:01.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:43:01.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:43:01.636 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:43:01.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
17:43:01.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:43:01.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:43:01.762 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:43:01.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:43:01.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
17:43:01.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
17:43:01.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:43:01.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:43:02.286 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/3be784c9-23cd-4ed6-8b2e-cfb32407ee59_resources
17:43:02.294 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3be784c9-23cd-4ed6-8b2e-cfb32407ee59
17:43:02.298 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/3be784c9-23cd-4ed6-8b2e-cfb32407ee59
17:43:02.305 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3be784c9-23cd-4ed6-8b2e-cfb32407ee59/_tmp_space.db
17:43:02.309 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:43:02.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:43:02.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
17:43:02.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:43:02.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:43:02.341 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:43:02.506 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/070d3b4e-cc91-416c-b112-32625faa8862_resources
17:43:02.511 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/070d3b4e-cc91-416c-b112-32625faa8862
17:43:02.515 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/070d3b4e-cc91-416c-b112-32625faa8862
17:43:02.523 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/070d3b4e-cc91-416c-b112-32625faa8862/_tmp_space.db
17:43:02.526 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
17:43:02.559 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:43:02.567 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:43:02.791 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:43:02.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:43:02.801 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:43:02.801 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:43:02.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:43:02.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:43:02.937 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.953 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:02.954 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:43:02.967 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:43:03.260 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:43:03.269 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:43:03.270 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:43:03.271 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:43:03.271 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:43:03.272 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:43:03.272 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
17:43:03.273 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
17:43:03.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:43:03.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:43:03.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:43:03.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:43:03.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:43:03.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:43:03.573 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.575 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.575 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.575 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.575 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.576 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.576 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:03.577 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:43:03.603 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:43:03.604 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:43:03.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:43:03.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:43:03.826 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:43:03.829 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
17:43:03.831 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
17:43:03.843 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
17:43:03.848 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:43:04.185 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 216.95306 ms
17:43:04.430 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 46.867 ms
17:43:04.528 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
17:43:05.101 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
17:43:05.105 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:6265 (size: 26.2 KB, free: 1988.7 MB)
17:43:05.108 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseExposure.scala:65
17:43:05.116 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:43:05.224 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseExposure.scala:65
17:43:05.242 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DwReleaseExposure.scala:65)
17:43:05.244 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseExposure.scala:65) with 1 output partitions
17:43:05.245 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseExposure.scala:65)
17:43:05.245 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
17:43:05.247 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
17:43:05.251 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65), which has no missing parents
17:43:05.386 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 17.6 KB, free 1988.4 MB)
17:43:05.389 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.3 MB)
17:43:05.390 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:6265 (size: 7.5 KB, free: 1988.7 MB)
17:43:05.391 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
17:43:05.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:43:05.407 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
17:43:05.444 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:43:05.446 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:43:05.447 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:43:05.447 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:43:05.447 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:43:05.448 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:43:05.448 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:43:05.449 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:43:05.457 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
17:43:05.457 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
17:43:05.457 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
17:43:05.457 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
17:43:05.457 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
17:43:05.457 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
17:43:05.457 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
17:43:05.457 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
17:43:05.549 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:43:05.549 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:43:05.549 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:43:05.549 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:43:05.549 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:43:05.549 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:43:05.549 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:43:05.549 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:43:06.727 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.727 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.727 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.727 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.727 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.727 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.727 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.763 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:06.818 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1414 bytes result sent to driver
17:43:06.818 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1457 bytes result sent to driver
17:43:06.818 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1457 bytes result sent to driver
17:43:06.818 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1414 bytes result sent to driver
17:43:06.818 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1414 bytes result sent to driver
17:43:06.818 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1414 bytes result sent to driver
17:43:06.843 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1371 bytes result sent to driver
17:43:06.853 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
17:43:06.856 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1408 ms on localhost (executor driver) (1/8)
17:43:06.859 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1413 ms on localhost (executor driver) (2/8)
17:43:06.859 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1413 ms on localhost (executor driver) (3/8)
17:43:06.859 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1424 ms on localhost (executor driver) (4/8)
17:43:06.860 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1412 ms on localhost (executor driver) (5/8)
17:43:06.860 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1412 ms on localhost (executor driver) (6/8)
17:43:06.861 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1414 ms on localhost (executor driver) (7/8)
17:43:07.457 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1629 bytes result sent to driver
17:43:07.459 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2012 ms on localhost (executor driver) (8/8)
17:43:07.460 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
17:43:07.462 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseExposure.scala:65) finished in 2.033 s
17:43:07.463 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:43:07.464 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:43:07.464 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
17:43:07.465 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:43:07.468 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65), which has no missing parents
17:43:07.474 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
17:43:07.476 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
17:43:07.477 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:6265 (size: 2.2 KB, free: 1988.7 MB)
17:43:07.478 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
17:43:07.479 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
17:43:07.480 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
17:43:07.482 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:43:07.482 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
17:43:07.494 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:43:07.495 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
17:43:07.519 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 1526 bytes result sent to driver
17:43:07.519 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 39 ms on localhost (executor driver) (1/1)
17:43:07.520 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
17:43:07.520 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseExposure.scala:65) finished in 0.040 s
17:43:07.523 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseExposure.scala:65, took 2.298819 s
17:43:07.542 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
17:43:07.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:43:07.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:43:07.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.570 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.570 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:43:07.635 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1
17:43:07.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:43:07.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:43:07.819 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:43:07.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:43:07.843 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:43:07.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:43:07.863 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.863 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.864 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.864 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.864 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.865 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.865 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.865 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.865 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:07.865 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:43:07.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:43:07.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:43:07.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:43:07.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:43:07.900 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:43:07.900 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
17:43:07.900 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
17:43:07.901 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
17:43:07.901 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
17:43:07.911 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:43:07.912 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:43:07.933 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 311.6 KB, free 1988.0 MB)
17:43:07.951 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
17:43:07.952 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:6265 (size: 26.2 KB, free: 1988.6 MB)
17:43:07.953 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
17:43:07.953 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
17:43:08.018 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
17:43:08.020 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:35)
17:43:08.020 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 4 output partitions
17:43:08.020 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:35)
17:43:08.020 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
17:43:08.020 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
17:43:08.020 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35), which has no missing parents
17:43:08.024 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.6 KB, free 1988.0 MB)
17:43:08.026 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.0 MB)
17:43:08.027 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:6265 (size: 7.5 KB, free: 1988.6 MB)
17:43:08.028 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
17:43:08.029 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17:43:08.029 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 8 tasks
17:43:08.030 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 9, localhost, executor driver, partition 0, ANY, 5391 bytes)
17:43:08.031 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 10, localhost, executor driver, partition 1, ANY, 5391 bytes)
17:43:08.031 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 11, localhost, executor driver, partition 2, ANY, 5391 bytes)
17:43:08.031 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 12, localhost, executor driver, partition 3, ANY, 5391 bytes)
17:43:08.032 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 13, localhost, executor driver, partition 4, ANY, 5391 bytes)
17:43:08.033 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 14, localhost, executor driver, partition 5, ANY, 5391 bytes)
17:43:08.034 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 15, localhost, executor driver, partition 6, ANY, 5391 bytes)
17:43:08.034 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 16, localhost, executor driver, partition 7, ANY, 5391 bytes)
17:43:08.034 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 10)
17:43:08.034 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 9)
17:43:08.034 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 12)
17:43:08.034 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 11)
17:43:08.034 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 13)
17:43:08.034 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 14)
17:43:08.034 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 15)
17:43:08.034 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 16)
17:43:08.038 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
17:43:08.039 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
17:43:08.040 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
17:43:08.041 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
17:43:08.041 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
17:43:08.044 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
17:43:08.045 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
17:43:08.057 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
17:43:08.063 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.075 INFO  [Executor task launch worker for task 12] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.081 INFO  [Executor task launch worker for task 13] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.098 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.108 INFO  [Executor task launch worker for task 15] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.112 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.127 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.137 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 10). 1328 bytes result sent to driver
17:43:08.139 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 10) in 109 ms on localhost (executor driver) (1/8)
17:43:08.142 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 13). 1328 bytes result sent to driver
17:43:08.142 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 13) in 111 ms on localhost (executor driver) (2/8)
17:43:08.146 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 15). 1371 bytes result sent to driver
17:43:08.147 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 15) in 114 ms on localhost (executor driver) (3/8)
17:43:08.148 INFO  [Executor task launch worker for task 14] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
17:43:08.151 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 9). 1371 bytes result sent to driver
17:43:08.153 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 9) in 123 ms on localhost (executor driver) (4/8)
17:43:08.155 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 16). 1414 bytes result sent to driver
17:43:08.156 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 16) in 122 ms on localhost (executor driver) (5/8)
17:43:08.162 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 11). 1371 bytes result sent to driver
17:43:08.162 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 11) in 131 ms on localhost (executor driver) (6/8)
17:43:08.165 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 14). 1371 bytes result sent to driver
17:43:08.167 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 14) in 134 ms on localhost (executor driver) (7/8)
17:43:08.552 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 12). 1543 bytes result sent to driver
17:43:08.553 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 12) in 522 ms on localhost (executor driver) (8/8)
17:43:08.553 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
17:43:08.553 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:35) finished in 0.523 s
17:43:08.553 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
17:43:08.553 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
17:43:08.553 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
17:43:08.553 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
17:43:08.554 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
17:43:08.616 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 167.4 KB, free 1987.8 MB)
17:43:08.618 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 62.2 KB, free 1987.8 MB)
17:43:08.619 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:6265 (size: 62.2 KB, free: 1988.6 MB)
17:43:08.619 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
17:43:08.620 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
17:43:08.620 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
17:43:08.621 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 17, localhost, executor driver, partition 0, ANY, 4726 bytes)
17:43:08.621 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 18, localhost, executor driver, partition 1, ANY, 4726 bytes)
17:43:08.621 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 19, localhost, executor driver, partition 2, ANY, 4726 bytes)
17:43:08.621 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 20, localhost, executor driver, partition 3, ANY, 4726 bytes)
17:43:08.622 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 18)
17:43:08.622 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 19)
17:43:08.622 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 20)
17:43:08.622 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 17)
17:43:08.668 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:43:08.668 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:43:08.668 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:43:08.668 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:43:08.668 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:43:08.668 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:43:08.669 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
17:43:08.669 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:43:08.689 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.690973 ms
17:43:08.715 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.609296 ms
17:43:08.793 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:43:08.793 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:43:08.795 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:43:08.795 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:43:08.795 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:43:08.795 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:43:08.795 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:43:08.796 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:43:08.810 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.125241 ms
17:43:08.880 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 42.006947 ms
17:43:08.898 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.217591 ms
17:43:08.997 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d6875af
17:43:08.999 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30743e0d
17:43:08.999 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3777b9b1
17:43:08.999 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a5bb48e
17:43:09.014 INFO  [Executor task launch worker for task 19] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17:43:09.022 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:43:09.022 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:43:09.022 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:43:09.022 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
17:43:09.022 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174308_0003_m_000000_0/bdp_day=20190924/part-00000-c032be4a-0b0d-41d2-8968-14b420ba8755.c000
17:43:09.022 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174308_0003_m_000001_0/bdp_day=20190924/part-00001-c032be4a-0b0d-41d2-8968-14b420ba8755.c000
17:43:09.022 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174308_0003_m_000002_0/bdp_day=20190924/part-00002-c032be4a-0b0d-41d2-8968-14b420ba8755.c000
17:43:09.022 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/_temporary/attempt_20190925174308_0003_m_000003_0/bdp_day=20190924/part-00003-c032be4a-0b0d-41d2-8968-14b420ba8755.c000
17:43:09.025 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression set to false
17:43:09.025 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression set to false
17:43:09.025 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression set to false
17:43:09.025 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression set to false
17:43:09.026 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:43:09.026 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:43:09.026 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:43:09.026 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
17:43:09.027 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:43:09.027 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:43:09.027 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:43:09.027 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
17:43:09.027 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:43:09.027 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:43:09.027 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:43:09.027 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
17:43:09.027 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:43:09.027 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:43:09.027 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:43:09.027 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
17:43:09.028 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:43:09.028 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:43:09.028 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:43:09.028 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Dictionary is on
17:43:09.028 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Validation is off
17:43:09.028 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Validation is off
17:43:09.028 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Validation is off
17:43:09.028 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Validation is off
17:43:09.028 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:43:09.028 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:43:09.031 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:43:09.031 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
17:43:09.264 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:6265 in memory (size: 7.5 KB, free: 1988.6 MB)
17:43:09.270 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:6265 in memory (size: 2.2 KB, free: 1988.6 MB)
17:43:09.271 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
17:43:09.280 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@bb3a64f
17:43:09.280 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5de34953
17:43:09.283 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@568a0f70
17:43:09.285 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6e2a6eb7
17:43:09.571 INFO  [Executor task launch worker for task 17] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,496
17:43:09.589 INFO  [Executor task launch worker for task 20] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,106
17:43:09.589 INFO  [Executor task launch worker for task 19] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 937,961
17:43:09.592 INFO  [Executor task launch worker for task 18] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,186
17:43:09.702 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.702 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.702 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.703 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.704 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.704 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.704 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.704 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.705 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.705 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.706 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.706 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
17:43:09.706 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
17:43:09.706 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
17:43:09.707 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
17:43:09.708 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
17:43:09.708 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,345 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
17:43:09.708 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
17:43:09.708 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
17:43:09.708 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.709 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
17:43:09.709 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.709 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:43:09.709 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.709 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
17:43:09.709 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:43:09.710 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:43:09.711 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
17:43:09.952 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174308_0003_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/task_20190925174308_0003_m_000003
17:43:09.952 INFO  [Executor task launch worker for task 20] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174308_0003_m_000003_0: Committed
17:43:09.955 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174308_0003_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/task_20190925174308_0003_m_000000
17:43:09.955 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174308_0003_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/task_20190925174308_0003_m_000002
17:43:09.955 INFO  [Executor task launch worker for task 17] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174308_0003_m_000000_0: Committed
17:43:09.955 INFO  [Executor task launch worker for task 19] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174308_0003_m_000002_0: Committed
17:43:09.955 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 20). 2568 bytes result sent to driver
17:43:09.957 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 19). 2525 bytes result sent to driver
17:43:09.957 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 17). 2525 bytes result sent to driver
17:43:09.958 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925174308_0003_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/_temporary/0/task_20190925174308_0003_m_000001
17:43:09.958 INFO  [Executor task launch worker for task 18] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925174308_0003_m_000001_0: Committed
17:43:09.959 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 20) in 1338 ms on localhost (executor driver) (1/4)
17:43:09.959 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 18). 2568 bytes result sent to driver
17:43:09.959 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 19) in 1338 ms on localhost (executor driver) (2/4)
17:43:09.959 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 17) in 1339 ms on localhost (executor driver) (3/4)
17:43:09.960 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 18) in 1339 ms on localhost (executor driver) (4/4)
17:43:09.960 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
17:43:09.960 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:35) finished in 1.340 s
17:43:09.961 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 1.941723 s
17:43:10.007 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
17:43:10.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:43:10.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:43:10.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:43:10.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:43:10.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:43:10.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:43:10.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:43:10.067 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:43:10.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:43:10.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:43:10.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:43:10.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:43:10.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:43:10.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
17:43:10.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
17:43:10.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:43:10.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:43:10.104 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
17:43:10.104 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
17:43:10.131 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00000-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:43:10.135 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:43:10.137 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00001-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:43:10.138 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:43:10.139 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00002-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:43:10.140 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:43:10.142 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00003-183a6b06-f46e-42ba-804f-419e4c886108.c000
17:43:10.143 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
17:43:10.168 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
17:43:10.182 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/bdp_day=20190924/part-00000-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00000-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, Status:true
17:43:10.199 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/bdp_day=20190924/part-00001-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00001-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, Status:true
17:43:10.213 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/bdp_day=20190924/part-00002-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00002-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, Status:true
17:43:10.231 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/bdp_day=20190924/part-00003-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00003-c032be4a-0b0d-41d2-8968-14b420ba8755.c000, Status:true
17:43:10.236 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
17:43:10.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
17:43:10.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_exposure
17:43:10.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_exposure	
17:43:10.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
17:43:10.322 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
17:43:10.324 WARN  [main] hive.log - Updated size to 2286733
17:43:10.531 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_17-43-07_634_5660179204004540453-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
17:43:10.537 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
17:43:10.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
17:43:10.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
17:43:10.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:43:10.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:43:10.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
17:43:10.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
17:43:10.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:43:10.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:43:10.621 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:43:10.628 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:43:10.631 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
17:43:10.643 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:43:10.759 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:43:10.760 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:43:10.761 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:43:10.763 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:43:10.769 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:43:10.769 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:43:10.770 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-b0d3e242-27c4-46c9-9699-af5941ee68b4
18:58:54.104 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
18:58:54.389 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
18:58:54.407 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
18:58:54.408 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
18:58:54.408 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
18:58:54.409 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
18:58:54.409 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
18:58:55.232 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 10171.
18:58:55.251 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
18:58:55.270 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
18:58:55.273 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18:58:55.274 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
18:58:55.287 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-d6fd257b-0468-4976-861e-1829f2d821c5
18:58:55.307 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
18:58:55.354 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
18:58:55.437 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3836ms
18:58:55.500 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
18:58:55.514 INFO  [main] org.spark_project.jetty.server.Server - Started @3914ms
18:58:55.535 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18:58:55.535 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
18:58:55.557 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
18:58:55.558 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
18:58:55.558 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
18:58:55.559 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
18:58:55.560 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
18:58:55.561 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
18:58:55.561 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
18:58:55.562 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
18:58:55.563 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
18:58:55.563 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
18:58:55.564 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
18:58:55.564 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
18:58:55.564 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
18:58:55.565 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
18:58:55.566 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
18:58:55.567 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
18:58:55.568 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
18:58:55.568 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
18:58:55.569 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
18:58:55.569 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
18:58:55.575 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
18:58:55.576 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
18:58:55.578 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
18:58:55.579 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
18:58:55.580 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
18:58:55.582 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
18:58:55.652 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
18:58:55.685 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10212.
18:58:55.686 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:10212
18:58:55.688 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18:58:55.730 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 10212, None)
18:58:55.733 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:10212 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 10212, None)
18:58:55.736 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 10212, None)
18:58:55.736 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 10212, None)
18:58:55.904 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
18:58:57.294 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
18:58:57.317 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
18:58:57.318 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
18:58:57.324 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
18:58:57.325 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
18:58:57.325 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
18:58:57.326 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
18:58:57.327 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
18:58:57.798 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18:58:58.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18:58:58.488 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
18:58:59.902 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18:59:01.148 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
18:59:01.151 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
18:59:01.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
18:59:01.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
18:59:01.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
18:59:01.655 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
18:59:01.656 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
18:59:01.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
18:59:01.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18:59:01.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
18:59:01.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
18:59:01.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
18:59:01.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
18:59:01.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
18:59:01.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
18:59:02.231 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/194b5068-c1a6-4cf1-9d7e-8d3f294caf77_resources
18:59:02.241 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/194b5068-c1a6-4cf1-9d7e-8d3f294caf77
18:59:02.244 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/194b5068-c1a6-4cf1-9d7e-8d3f294caf77
18:59:02.247 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/194b5068-c1a6-4cf1-9d7e-8d3f294caf77/_tmp_space.db
18:59:02.251 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
18:59:02.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:59:02.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
18:59:02.280 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
18:59:02.280 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
18:59:02.284 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
18:59:02.446 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/3e7def24-7685-4a7a-a807-8e5ea7e278d6_resources
18:59:02.464 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3e7def24-7685-4a7a-a807-8e5ea7e278d6
18:59:02.468 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/3e7def24-7685-4a7a-a807-8e5ea7e278d6
18:59:02.475 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3e7def24-7685-4a7a-a807-8e5ea7e278d6/_tmp_space.db
18:59:02.479 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
18:59:02.530 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
18:59:02.537 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
18:59:02.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
18:59:02.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
18:59:02.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
18:59:02.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
18:59:02.846 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
18:59:02.846 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
18:59:02.873 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.887 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.887 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:02.887 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:59:02.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
18:59:03.047 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
18:59:03.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
18:59:03.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
18:59:03.054 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
18:59:03.054 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
18:59:03.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
18:59:03.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
18:59:03.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:59:03.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:59:03.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
18:59:03.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
18:59:03.172 WARN  [main] org.apache.spark.sql.execution.datasources.InMemoryFileIndex - The directory hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924 was not found. Was it deleted very recently?
18:59:03.559 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
18:59:03.576 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
18:59:03.577 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
18:59:03.577 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
18:59:03.578 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
18:59:03.583 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
18:59:03.649 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
18:59:03.717 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
18:59:03.721 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:10212 (size: 26.8 KB, free: 1988.7 MB)
18:59:03.724 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
18:59:03.736 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
18:59:03.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
18:59:03.785 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
18:59:03.797 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
18:59:03.954 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 772 bytes result sent to driver
18:59:03.960 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 196 ms on localhost (executor driver) (1/1)
18:59:03.962 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
18:59:03.966 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 0.211 s
18:59:03.969 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 0.410123 s
18:59:03.972 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_register_users from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
18:59:04.105 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
18:59:04.130 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
18:59:04.131 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
18:59:04.132 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
18:59:04.132 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
18:59:04.132 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
18:59:04.133 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
18:59:04.133 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
18:59:04.134 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
18:59:04.139 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:59:04.139 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
18:59:04.216 ERROR [main] release.etl.dw.DwReleaseCustomer$ - cannot resolve '`exts`' given input columns: [user_id, release_session, channels, ctime, device_type, sources, release_status, device_num, bdp_day]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [user_id, release_session, channels, ctime, device_type, sources, release_status, device_num, bdp_day]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at release.etl.dw.DwReleaseRegisterUsers$.handleReleaseJob(DwReleaseRegisterUsers.scala:58)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:106)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:104)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseRegisterUsers$.handleJobs(DwReleaseRegisterUsers.scala:104)
	at release.etl.dw.DwReleaseRegisterUsers$.main(DwReleaseRegisterUsers.scala:32)
	at release.etl.dw.DwReleaseRegisterUsers.main(DwReleaseRegisterUsers.scala)
18:59:04.226 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
18:59:04.233 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18:59:04.234 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
18:59:04.243 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
18:59:04.260 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
18:59:04.261 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
18:59:04.267 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
18:59:04.270 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
18:59:04.276 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
18:59:04.276 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
18:59:04.277 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-71824414-919a-45ce-8e8f-86b1bfc1b892
19:03:50.942 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:03:51.235 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:03:51.254 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
19:03:51.254 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
19:03:51.255 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:03:51.255 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:03:51.255 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
19:03:52.049 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 10331.
19:03:52.066 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:03:52.085 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:03:52.087 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:03:52.088 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:03:52.097 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-e5835c58-f251-4dd1-9d5c-cb277378f662
19:03:52.113 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
19:03:52.152 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:03:52.236 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3856ms
19:03:52.298 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:03:52.316 INFO  [main] org.spark_project.jetty.server.Server - Started @3937ms
19:03:52.341 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:03:52.341 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:03:52.366 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
19:03:52.366 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
19:03:52.367 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
19:03:52.368 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
19:03:52.368 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
19:03:52.369 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
19:03:52.370 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
19:03:52.371 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
19:03:52.371 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
19:03:52.372 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
19:03:52.373 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
19:03:52.373 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
19:03:52.374 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
19:03:52.375 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
19:03:52.375 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
19:03:52.375 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
19:03:52.376 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
19:03:52.376 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
19:03:52.377 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
19:03:52.378 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:03:52.383 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
19:03:52.384 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
19:03:52.386 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
19:03:52.387 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
19:03:52.387 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
19:03:52.390 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
19:03:52.456 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:03:52.482 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10372.
19:03:52.483 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:10372
19:03:52.484 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:03:52.516 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 10372, None)
19:03:52.519 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:10372 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 10372, None)
19:03:52.522 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 10372, None)
19:03:52.523 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 10372, None)
19:03:52.693 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
19:03:54.081 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
19:03:54.106 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
19:03:54.107 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
19:03:54.113 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
19:03:54.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
19:03:54.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
19:03:54.115 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
19:03:54.116 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
19:03:54.581 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:03:55.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:03:55.245 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:03:56.319 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:03:57.585 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:03:57.588 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:03:57.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:03:57.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:03:57.848 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:03:57.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:03:57.946 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
19:03:57.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:03:57.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:03:58.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:03:58.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:03:58.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:03:58.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:03:58.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:03:58.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:03:58.517 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/7a7a6ddc-e60f-4522-ab80-66ffe42d1ab0_resources
19:03:58.531 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7a7a6ddc-e60f-4522-ab80-66ffe42d1ab0
19:03:58.535 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/7a7a6ddc-e60f-4522-ab80-66ffe42d1ab0
19:03:58.539 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7a7a6ddc-e60f-4522-ab80-66ffe42d1ab0/_tmp_space.db
19:03:58.543 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:03:58.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:03:58.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:03:58.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:03:58.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:03:58.578 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:03:58.768 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/2eb5ada7-64dd-4bab-84a0-2df78bdc587c_resources
19:03:58.805 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/2eb5ada7-64dd-4bab-84a0-2df78bdc587c
19:03:58.808 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/2eb5ada7-64dd-4bab-84a0-2df78bdc587c
19:03:58.813 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/2eb5ada7-64dd-4bab-84a0-2df78bdc587c/_tmp_space.db
19:03:58.816 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:03:58.839 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:03:58.844 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
19:03:59.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:03:59.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:03:59.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:03:59.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:03:59.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:03:59.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:03:59.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.160 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.160 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.161 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.161 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.161 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.161 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.161 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.162 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:03:59.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:03:59.307 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
19:03:59.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:03:59.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:03:59.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:03:59.315 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:03:59.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:03:59.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:03:59.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:03:59.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:03:59.392 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
19:03:59.392 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
19:03:59.449 WARN  [main] org.apache.spark.sql.execution.datasources.InMemoryFileIndex - The directory hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924 was not found. Was it deleted very recently?
19:03:59.877 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
19:03:59.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
19:03:59.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
19:03:59.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
19:03:59.898 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:03:59.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
19:03:59.961 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
19:04:00.015 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
19:04:00.020 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:10372 (size: 26.8 KB, free: 1988.7 MB)
19:04:00.022 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
19:04:00.036 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
19:04:00.037 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
19:04:00.079 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
19:04:00.090 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:04:00.248 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 729 bytes result sent to driver
19:04:00.254 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 192 ms on localhost (executor driver) (1/1)
19:04:00.256 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:04:00.260 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 0.206 s
19:04:00.264 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 0.385906 s
19:04:00.266 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_register_users from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
19:04:00.385 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
19:04:00.409 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:04:00.409 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:04:00.410 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:04:00.410 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:04:00.411 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:04:00.411 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:04:00.412 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:04:00.412 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:04:00.417 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:04:00.417 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:04:00.471 ERROR [main] release.etl.dw.DwReleaseCustomer$ - cannot resolve '`exts`' given input columns: [release_session, device_num, release_status, device_type, bdp_day, user_id, sources, ctime, channels]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [release_session, device_num, release_status, device_type, bdp_day, user_id, sources, ctime, channels]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at release.etl.dw.DwReleaseRegisterUsers$.handleReleaseJob(DwReleaseRegisterUsers.scala:58)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:106)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:104)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseRegisterUsers$.handleJobs(DwReleaseRegisterUsers.scala:104)
	at release.etl.dw.DwReleaseRegisterUsers$.main(DwReleaseRegisterUsers.scala:32)
	at release.etl.dw.DwReleaseRegisterUsers.main(DwReleaseRegisterUsers.scala)
19:04:00.479 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:04:00.486 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:04:00.488 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
19:04:00.497 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:04:00.513 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:04:00.513 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:04:00.519 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:04:00.521 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:04:00.527 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:04:00.527 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:04:00.528 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-56f2fbf3-9a19-4ae7-a43f-b53aa631c5a7
19:05:51.328 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:05:51.612 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:05:51.630 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
19:05:51.631 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
19:05:51.631 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:05:51.632 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:05:51.632 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
19:05:52.440 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 10691.
19:05:52.457 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:05:52.475 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:05:52.477 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:05:52.478 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:05:52.486 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-9ea71390-7a9a-4aae-b6a2-2ca1438bf33a
19:05:52.505 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
19:05:52.547 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:05:52.625 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3775ms
19:05:52.695 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:05:52.712 INFO  [main] org.spark_project.jetty.server.Server - Started @3863ms
19:05:52.737 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@2f72ca11{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:05:52.738 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:05:52.765 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6650813a{/jobs,null,AVAILABLE,@Spark}
19:05:52.766 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30272916{/jobs/json,null,AVAILABLE,@Spark}
19:05:52.767 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job,null,AVAILABLE,@Spark}
19:05:52.768 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job/json,null,AVAILABLE,@Spark}
19:05:52.769 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/stages,null,AVAILABLE,@Spark}
19:05:52.770 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages/json,null,AVAILABLE,@Spark}
19:05:52.770 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/stage,null,AVAILABLE,@Spark}
19:05:52.772 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e8f862{/stages/stage/json,null,AVAILABLE,@Spark}
19:05:52.772 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/pool,null,AVAILABLE,@Spark}
19:05:52.773 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool/json,null,AVAILABLE,@Spark}
19:05:52.774 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/storage,null,AVAILABLE,@Spark}
19:05:52.775 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage/json,null,AVAILABLE,@Spark}
19:05:52.776 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/rdd,null,AVAILABLE,@Spark}
19:05:52.776 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd/json,null,AVAILABLE,@Spark}
19:05:52.777 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/environment,null,AVAILABLE,@Spark}
19:05:52.778 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment/json,null,AVAILABLE,@Spark}
19:05:52.779 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/executors,null,AVAILABLE,@Spark}
19:05:52.780 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors/json,null,AVAILABLE,@Spark}
19:05:52.781 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/threadDump,null,AVAILABLE,@Spark}
19:05:52.781 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:05:52.787 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/static,null,AVAILABLE,@Spark}
19:05:52.788 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/,null,AVAILABLE,@Spark}
19:05:52.789 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/api,null,AVAILABLE,@Spark}
19:05:52.790 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/jobs/job/kill,null,AVAILABLE,@Spark}
19:05:52.791 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/stages/stage/kill,null,AVAILABLE,@Spark}
19:05:52.794 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
19:05:52.868 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:05:52.899 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10732.
19:05:52.900 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:10732
19:05:52.902 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:05:52.936 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 10732, None)
19:05:52.939 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:10732 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 10732, None)
19:05:52.942 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 10732, None)
19:05:52.943 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 10732, None)
19:05:53.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@529cfee5{/metrics/json,null,AVAILABLE,@Spark}
19:05:54.573 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
19:05:54.598 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
19:05:54.599 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
19:05:54.606 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19489b27{/SQL,null,AVAILABLE,@Spark}
19:05:54.607 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL/json,null,AVAILABLE,@Spark}
19:05:54.608 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL/execution,null,AVAILABLE,@Spark}
19:05:54.609 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution/json,null,AVAILABLE,@Spark}
19:05:54.611 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69cd7630{/static/sql,null,AVAILABLE,@Spark}
19:05:55.073 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:05:55.721 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:05:55.754 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:05:56.844 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:05:58.046 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:05:58.049 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:05:58.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:05:58.268 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:05:58.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:05:58.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:05:58.426 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
19:05:58.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:05:58.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:05:58.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:05:58.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:05:58.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:05:58.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:05:58.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:05:58.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:05:58.979 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/ec923095-72d3-4c91-bf58-dff81341f33d_resources
19:05:59.011 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/ec923095-72d3-4c91-bf58-dff81341f33d
19:05:59.014 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/ec923095-72d3-4c91-bf58-dff81341f33d
19:05:59.018 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/ec923095-72d3-4c91-bf58-dff81341f33d/_tmp_space.db
19:05:59.022 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:05:59.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:05:59.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:05:59.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:05:59.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:05:59.057 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:05:59.227 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/c0f0f91a-ce73-4cbd-9cff-da9c74a4751f_resources
19:05:59.265 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/c0f0f91a-ce73-4cbd-9cff-da9c74a4751f
19:05:59.269 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/c0f0f91a-ce73-4cbd-9cff-da9c74a4751f
19:05:59.274 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/c0f0f91a-ce73-4cbd-9cff-da9c74a4751f/_tmp_space.db
19:05:59.276 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:05:59.301 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:05:59.306 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
19:05:59.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:05:59.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:05:59.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:05:59.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:05:59.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:05:59.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:05:59.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.636 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.636 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.636 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.637 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.637 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.637 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.638 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.638 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:05:59.652 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:05:59.833 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
19:05:59.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:05:59.836 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:05:59.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:05:59.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:05:59.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:05:59.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:05:59.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:05:59.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:05:59.914 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
19:05:59.914 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
19:05:59.976 WARN  [main] org.apache.spark.sql.execution.datasources.InMemoryFileIndex - The directory hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924 was not found. Was it deleted very recently?
19:06:00.368 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
19:06:00.385 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
19:06:00.385 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
19:06:00.386 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
19:06:00.387 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:06:00.391 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
19:06:00.446 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
19:06:00.508 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
19:06:00.511 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:10732 (size: 26.8 KB, free: 1988.7 MB)
19:06:00.513 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
19:06:00.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
19:06:00.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
19:06:00.575 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
19:06:00.585 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:06:00.732 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 729 bytes result sent to driver
19:06:00.739 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 184 ms on localhost (executor driver) (1/1)
19:06:00.741 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:06:00.745 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 0.200 s
19:06:00.749 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 0.380566 s
19:06:00.752 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_register_users from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
19:06:00.867 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
19:06:00.895 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:06:00.896 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:06:00.897 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:06:00.897 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:06:00.897 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:06:00.898 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:06:00.899 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:06:00.899 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:06:00.903 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:06:00.903 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:06:00.960 ERROR [main] release.etl.dw.DwReleaseCustomer$ - cannot resolve '`exts`' given input columns: [release_session, channels, ctime, user_id, device_type, sources, device_num, release_status, bdp_day]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [release_session, channels, ctime, user_id, device_type, sources, device_num, release_status, bdp_day]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at release.etl.dw.DwReleaseRegisterUsers$.handleReleaseJob(DwReleaseRegisterUsers.scala:58)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:106)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:104)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseRegisterUsers$.handleJobs(DwReleaseRegisterUsers.scala:104)
	at release.etl.dw.DwReleaseRegisterUsers$.main(DwReleaseRegisterUsers.scala:32)
	at release.etl.dw.DwReleaseRegisterUsers.main(DwReleaseRegisterUsers.scala)
19:06:00.967 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:06:00.974 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@2f72ca11{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:06:00.976 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
19:06:00.987 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:06:01.005 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:06:01.005 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:06:01.011 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:06:01.013 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:06:01.019 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:06:01.019 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:06:01.020 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-5754d5c1-b678-4a05-822a-31847275d0fd
19:10:52.629 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:10:52.924 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:10:52.944 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
19:10:52.944 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
19:10:52.945 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:10:52.945 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:10:52.945 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
19:10:53.766 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 10837.
19:10:53.784 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:10:53.803 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:10:53.807 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:10:53.807 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:10:53.817 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-52e2bbeb-1e58-4245-a604-753525195d77
19:10:53.834 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
19:10:53.877 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:10:53.959 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3802ms
19:10:54.026 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:10:54.040 INFO  [main] org.spark_project.jetty.server.Server - Started @3884ms
19:10:54.063 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:10:54.063 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:10:54.084 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
19:10:54.085 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
19:10:54.086 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
19:10:54.087 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
19:10:54.087 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
19:10:54.088 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
19:10:54.089 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
19:10:54.090 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
19:10:54.090 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
19:10:54.091 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
19:10:54.091 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
19:10:54.092 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
19:10:54.092 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
19:10:54.093 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
19:10:54.094 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
19:10:54.095 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
19:10:54.095 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
19:10:54.096 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
19:10:54.096 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
19:10:54.097 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:10:54.102 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
19:10:54.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
19:10:54.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
19:10:54.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
19:10:54.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
19:10:54.107 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
19:10:54.175 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:10:54.205 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10879.
19:10:54.206 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:10879
19:10:54.207 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:10:54.241 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 10879, None)
19:10:54.244 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:10879 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 10879, None)
19:10:54.248 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 10879, None)
19:10:54.248 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 10879, None)
19:10:54.429 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
19:10:55.844 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
19:10:55.869 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
19:10:55.870 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
19:10:55.877 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
19:10:55.878 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
19:10:55.878 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
19:10:55.879 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
19:10:55.880 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
19:10:56.342 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:10:56.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:10:57.011 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:10:58.094 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:10:59.218 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:10:59.220 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:10:59.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:10:59.556 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:10:59.606 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:10:59.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:10:59.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
19:10:59.721 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:10:59.721 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:10:59.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:10:59.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:10:59.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:10:59.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:10:59.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:10:59.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:11:00.284 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/7529c51d-7b1a-4c3b-a7cc-61e89063e410_resources
19:11:00.294 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7529c51d-7b1a-4c3b-a7cc-61e89063e410
19:11:00.297 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/7529c51d-7b1a-4c3b-a7cc-61e89063e410
19:11:00.301 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7529c51d-7b1a-4c3b-a7cc-61e89063e410/_tmp_space.db
19:11:00.306 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:11:00.328 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:11:00.328 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:11:00.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:11:00.335 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:11:00.339 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:11:00.488 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/7e98c05a-1d99-489b-b6a6-f906f21ac6cb_resources
19:11:00.494 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7e98c05a-1d99-489b-b6a6-f906f21ac6cb
19:11:00.497 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/7e98c05a-1d99-489b-b6a6-f906f21ac6cb
19:11:00.505 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7e98c05a-1d99-489b-b6a6-f906f21ac6cb/_tmp_space.db
19:11:00.508 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:11:00.542 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:11:00.550 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
19:11:00.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:11:00.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:11:00.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:11:00.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:11:00.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:11:00.869 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:11:00.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.914 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.916 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:00.916 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:11:00.929 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:11:01.074 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
19:11:01.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:11:01.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:11:01.082 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:11:01.082 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:11:01.110 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:11:01.110 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:11:01.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.135 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.135 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.135 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.135 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.135 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:11:01.136 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:11:01.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
19:11:01.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
19:11:01.217 WARN  [main] org.apache.spark.sql.execution.datasources.InMemoryFileIndex - The directory hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924 was not found. Was it deleted very recently?
19:11:01.671 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
19:11:01.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
19:11:01.692 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
19:11:01.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
19:11:01.695 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:11:01.701 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
19:11:01.767 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
19:11:01.824 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
19:11:01.827 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:10879 (size: 26.8 KB, free: 1988.7 MB)
19:11:01.830 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
19:11:01.845 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
19:11:01.845 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
19:11:01.892 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
19:11:01.904 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:11:02.067 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 772 bytes result sent to driver
19:11:02.073 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 201 ms on localhost (executor driver) (1/1)
19:11:02.076 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:11:02.079 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 0.217 s
19:11:02.083 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 0.411511 s
19:11:02.085 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_register_users from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
19:11:02.205 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
19:11:02.232 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:11:02.233 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:11:02.234 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:11:02.235 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:11:02.236 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:11:02.236 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:11:02.237 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:11:02.237 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:11:02.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:11:02.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:11:02.289 ERROR [main] release.etl.dw.DwReleaseCustomer$ - cannot resolve '`exts`' given input columns: [release_session, release_status, device_num, channels, sources, ctime, user_id, bdp_day, device_type]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [release_session, release_status, device_num, channels, sources, ctime, user_id, bdp_day, device_type]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at release.etl.dw.DwReleaseRegisterUsers$.handleReleaseJob(DwReleaseRegisterUsers.scala:58)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:106)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:104)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseRegisterUsers$.handleJobs(DwReleaseRegisterUsers.scala:104)
	at release.etl.dw.DwReleaseRegisterUsers$.main(DwReleaseRegisterUsers.scala:32)
	at release.etl.dw.DwReleaseRegisterUsers.main(DwReleaseRegisterUsers.scala)
19:11:02.294 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:11:02.300 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:11:02.302 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
19:11:02.311 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:11:02.327 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:11:02.327 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:11:02.333 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:11:02.335 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:11:02.341 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:11:02.342 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:11:02.343 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-9e9acb09-93fc-44e1-961c-e905dd24b2cd
19:13:39.844 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:13:40.131 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:13:40.150 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
19:13:40.150 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
19:13:40.150 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:13:40.151 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:13:40.151 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
19:13:40.952 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 10965.
19:13:40.971 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:13:40.992 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:13:40.995 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:13:40.996 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:13:41.008 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-351ad879-ee1b-45f3-9f24-a3b27a59453e
19:13:41.033 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
19:13:41.084 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:13:41.165 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4005ms
19:13:41.227 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:13:41.239 INFO  [main] org.spark_project.jetty.server.Server - Started @4082ms
19:13:41.261 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4b2b0b83{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:13:41.261 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:13:41.284 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6650813a{/jobs,null,AVAILABLE,@Spark}
19:13:41.285 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30272916{/jobs/json,null,AVAILABLE,@Spark}
19:13:41.286 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job,null,AVAILABLE,@Spark}
19:13:41.287 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job/json,null,AVAILABLE,@Spark}
19:13:41.287 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/stages,null,AVAILABLE,@Spark}
19:13:41.288 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages/json,null,AVAILABLE,@Spark}
19:13:41.289 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/stage,null,AVAILABLE,@Spark}
19:13:41.290 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e8f862{/stages/stage/json,null,AVAILABLE,@Spark}
19:13:41.291 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/pool,null,AVAILABLE,@Spark}
19:13:41.291 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool/json,null,AVAILABLE,@Spark}
19:13:41.292 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/storage,null,AVAILABLE,@Spark}
19:13:41.293 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage/json,null,AVAILABLE,@Spark}
19:13:41.294 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/rdd,null,AVAILABLE,@Spark}
19:13:41.295 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd/json,null,AVAILABLE,@Spark}
19:13:41.295 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/environment,null,AVAILABLE,@Spark}
19:13:41.296 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment/json,null,AVAILABLE,@Spark}
19:13:41.296 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/executors,null,AVAILABLE,@Spark}
19:13:41.297 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors/json,null,AVAILABLE,@Spark}
19:13:41.297 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/threadDump,null,AVAILABLE,@Spark}
19:13:41.298 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:13:41.305 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/static,null,AVAILABLE,@Spark}
19:13:41.306 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/,null,AVAILABLE,@Spark}
19:13:41.307 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/api,null,AVAILABLE,@Spark}
19:13:41.308 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/jobs/job/kill,null,AVAILABLE,@Spark}
19:13:41.309 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/stages/stage/kill,null,AVAILABLE,@Spark}
19:13:41.311 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
19:13:41.385 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:13:41.417 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11006.
19:13:41.418 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:11006
19:13:41.419 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:13:41.458 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 11006, None)
19:13:41.460 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:11006 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 11006, None)
19:13:41.463 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 11006, None)
19:13:41.463 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 11006, None)
19:13:41.621 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@529cfee5{/metrics/json,null,AVAILABLE,@Spark}
19:13:43.022 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
19:13:43.048 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
19:13:43.049 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
19:13:43.057 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19489b27{/SQL,null,AVAILABLE,@Spark}
19:13:43.058 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL/json,null,AVAILABLE,@Spark}
19:13:43.058 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL/execution,null,AVAILABLE,@Spark}
19:13:43.059 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution/json,null,AVAILABLE,@Spark}
19:13:43.060 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69cd7630{/static/sql,null,AVAILABLE,@Spark}
19:13:43.549 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:13:44.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:13:44.252 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:13:45.440 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:13:46.620 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:13:46.622 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:13:46.920 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:13:46.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:13:46.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:13:47.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:13:47.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
19:13:47.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:13:47.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:13:47.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:13:47.131 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:13:47.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:13:47.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:13:47.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:13:47.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:13:47.649 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/a9fb53f7-5a4a-4a69-8dce-1184676457cb_resources
19:13:47.672 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a9fb53f7-5a4a-4a69-8dce-1184676457cb
19:13:47.675 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/a9fb53f7-5a4a-4a69-8dce-1184676457cb
19:13:47.679 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a9fb53f7-5a4a-4a69-8dce-1184676457cb/_tmp_space.db
19:13:47.682 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:13:47.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:47.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:13:47.711 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:13:47.711 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:13:47.715 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:13:47.881 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/b61ef0c2-17d6-494f-9fc0-aa93402dd025_resources
19:13:47.896 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b61ef0c2-17d6-494f-9fc0-aa93402dd025
19:13:47.899 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/b61ef0c2-17d6-494f-9fc0-aa93402dd025
19:13:47.908 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b61ef0c2-17d6-494f-9fc0-aa93402dd025/_tmp_space.db
19:13:47.911 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:13:47.945 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:13:47.950 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
19:13:48.164 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:13:48.165 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:13:48.172 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:13:48.173 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:13:48.264 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:13:48.264 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:13:48.295 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.308 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.308 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.308 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.309 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.309 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.309 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.309 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.310 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:13:48.324 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:13:48.464 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
19:13:48.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:13:48.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:13:48.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:13:48.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:13:48.503 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:13:48.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:13:48.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.529 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.529 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.529 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.529 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.529 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.530 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.530 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:13:48.530 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:13:48.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
19:13:48.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
19:13:48.611 WARN  [main] org.apache.spark.sql.execution.datasources.InMemoryFileIndex - The directory hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924 was not found. Was it deleted very recently?
19:13:49.028 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
19:13:49.046 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
19:13:49.046 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
19:13:49.047 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
19:13:49.047 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:13:49.052 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
19:13:49.114 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
19:13:49.180 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
19:13:49.183 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:11006 (size: 26.8 KB, free: 1988.7 MB)
19:13:49.186 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
19:13:49.204 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
19:13:49.204 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
19:13:49.252 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
19:13:49.264 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:13:49.425 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 729 bytes result sent to driver
19:13:49.431 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 199 ms on localhost (executor driver) (1/1)
19:13:49.433 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:13:49.437 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 0.214 s
19:13:49.441 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 0.411912 s
19:13:49.444 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_register_users from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
19:13:49.578 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
19:13:49.605 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:13:49.606 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:13:49.606 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:13:49.607 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:13:49.607 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:13:49.608 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:13:49.608 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:13:49.609 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:13:49.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:13:49.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:13:49.669 ERROR [main] release.etl.dw.DwReleaseCustomer$ - cannot resolve '`exts`' given input columns: [release_status, device_type, release_session, sources, user_id, bdp_day, ctime, device_num, channels]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [release_status, device_type, release_session, sources, user_id, bdp_day, ctime, device_num, channels]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at release.etl.dw.DwReleaseRegisterUsers$.handleReleaseJob(DwReleaseRegisterUsers.scala:58)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:106)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:104)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseRegisterUsers$.handleJobs(DwReleaseRegisterUsers.scala:104)
	at release.etl.dw.DwReleaseRegisterUsers$.main(DwReleaseRegisterUsers.scala:32)
	at release.etl.dw.DwReleaseRegisterUsers.main(DwReleaseRegisterUsers.scala)
19:13:49.677 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:13:49.684 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4b2b0b83{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:13:49.686 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
19:13:49.697 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:13:49.715 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:13:49.715 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:13:49.722 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:13:49.724 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:13:49.730 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:13:49.731 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:13:49.731 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-5c87bc38-44e6-4f65-95cf-0c2bdd520b2a
19:19:16.913 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:19:17.227 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:19:17.247 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
19:19:17.247 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
19:19:17.247 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:19:17.248 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:19:17.248 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
19:19:18.135 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 11361.
19:19:18.157 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:19:18.175 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:19:18.179 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:19:18.180 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:19:18.190 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-ea33952c-e928-484c-8d23-e94fff942e87
19:19:18.209 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
19:19:18.254 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:19:18.332 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4152ms
19:19:18.398 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:19:18.413 INFO  [main] org.spark_project.jetty.server.Server - Started @4234ms
19:19:18.434 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@14843e66{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:19:18.434 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:19:18.456 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50cf5a23{/jobs,null,AVAILABLE,@Spark}
19:19:18.457 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/json,null,AVAILABLE,@Spark}
19:19:18.458 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/jobs/job,null,AVAILABLE,@Spark}
19:19:18.459 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/job/json,null,AVAILABLE,@Spark}
19:19:18.460 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages,null,AVAILABLE,@Spark}
19:19:18.461 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/json,null,AVAILABLE,@Spark}
19:19:18.461 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/stage,null,AVAILABLE,@Spark}
19:19:18.464 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/stage/json,null,AVAILABLE,@Spark}
19:19:18.464 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool,null,AVAILABLE,@Spark}
19:19:18.465 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/pool/json,null,AVAILABLE,@Spark}
19:19:18.466 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage,null,AVAILABLE,@Spark}
19:19:18.467 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/json,null,AVAILABLE,@Spark}
19:19:18.468 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd,null,AVAILABLE,@Spark}
19:19:18.469 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd/json,null,AVAILABLE,@Spark}
19:19:18.470 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment,null,AVAILABLE,@Spark}
19:19:18.470 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/environment/json,null,AVAILABLE,@Spark}
19:19:18.471 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors,null,AVAILABLE,@Spark}
19:19:18.472 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/json,null,AVAILABLE,@Spark}
19:19:18.472 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump,null,AVAILABLE,@Spark}
19:19:18.473 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:19:18.479 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/static,null,AVAILABLE,@Spark}
19:19:18.480 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/,null,AVAILABLE,@Spark}
19:19:18.481 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/api,null,AVAILABLE,@Spark}
19:19:18.482 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/jobs/job/kill,null,AVAILABLE,@Spark}
19:19:18.483 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/stages/stage/kill,null,AVAILABLE,@Spark}
19:19:18.485 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
19:19:18.562 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:19:18.591 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11402.
19:19:18.592 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:11402
19:19:18.594 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:19:18.633 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 11402, None)
19:19:18.636 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:11402 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 11402, None)
19:19:18.641 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 11402, None)
19:19:18.641 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 11402, None)
19:19:18.823 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319854f0{/metrics/json,null,AVAILABLE,@Spark}
19:19:20.333 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
19:19:20.359 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
19:19:20.360 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
19:19:20.366 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL,null,AVAILABLE,@Spark}
19:19:20.367 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@125d47c4{/SQL/json,null,AVAILABLE,@Spark}
19:19:20.368 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution,null,AVAILABLE,@Spark}
19:19:20.368 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f2466c{/SQL/execution/json,null,AVAILABLE,@Spark}
19:19:20.369 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b53840a{/static/sql,null,AVAILABLE,@Spark}
19:19:20.854 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:19:21.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:19:21.608 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:19:22.706 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:19:23.902 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:19:23.905 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:19:24.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:19:24.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:19:24.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:19:24.269 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:19:24.271 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
19:19:24.284 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:19:24.284 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:19:24.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:19:24.364 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:19:24.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:19:24.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:19:24.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:19:24.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:19:24.870 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/8b815c81-6a0e-4cd7-8303-326a66f2fdba_resources
19:19:24.944 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8b815c81-6a0e-4cd7-8303-326a66f2fdba
19:19:24.947 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/8b815c81-6a0e-4cd7-8303-326a66f2fdba
19:19:24.953 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8b815c81-6a0e-4cd7-8303-326a66f2fdba/_tmp_space.db
19:19:24.956 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:19:24.977 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:19:24.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:19:24.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:19:24.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:19:24.990 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:19:25.166 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/a37e26d3-20ef-478b-843b-80bfa6b3b209_resources
19:19:25.194 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a37e26d3-20ef-478b-843b-80bfa6b3b209
19:19:25.197 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/a37e26d3-20ef-478b-843b-80bfa6b3b209
19:19:25.202 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a37e26d3-20ef-478b-843b-80bfa6b3b209/_tmp_space.db
19:19:25.205 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:19:25.228 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:19:25.234 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
19:19:25.456 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:19:25.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:19:25.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:19:25.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:19:25.556 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:19:25.556 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:19:25.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.596 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.597 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.597 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.597 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.597 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:19:25.609 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:19:25.728 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
19:19:25.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:19:25.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:19:25.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:19:25.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:19:25.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:19:25.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:19:25.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:19:25.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:19:25.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
19:19:25.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
19:19:25.866 WARN  [main] org.apache.spark.sql.execution.datasources.InMemoryFileIndex - The directory hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924 was not found. Was it deleted very recently?
19:19:26.229 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
19:19:26.245 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
19:19:26.246 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
19:19:26.247 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
19:19:26.247 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:19:26.253 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
19:19:26.303 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
19:19:26.352 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
19:19:26.354 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:11402 (size: 26.8 KB, free: 1988.7 MB)
19:19:26.356 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
19:19:26.371 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
19:19:26.371 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
19:19:26.411 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
19:19:26.420 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:19:26.590 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 729 bytes result sent to driver
19:19:26.597 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 202 ms on localhost (executor driver) (1/1)
19:19:26.599 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:19:26.603 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 0.216 s
19:19:26.607 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 0.377885 s
19:19:26.610 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_register_users from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
19:19:26.716 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
19:19:26.741 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:19:26.742 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:19:26.742 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:19:26.743 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:19:26.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:19:26.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:19:26.745 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:19:26.745 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:19:26.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:19:26.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:19:26.798 ERROR [main] release.etl.dw.DwReleaseCustomer$ - cannot resolve '`exts`' given input columns: [sources, device_num, release_status, ctime, release_session, channels, device_type, user_id, bdp_day]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

org.apache.spark.sql.AnalysisException: cannot resolve '`exts`' given input columns: [sources, device_num, release_status, ctime, release_session, channels, device_type, user_id, bdp_day]; line 1 pos 16;
'Project ['get_json_object('exts, $.user_register) AS user_register#28, release_session#1, release_status#2, device_num#3, device_type#4, sources#5, channels#6, 'ct, bdp_day#8]
+- SubqueryAlias dw_release_register_users
   +- Relation[user_id#0,release_session#1,release_status#2,device_num#3,device_type#4,sources#5,channels#6,ctime#7L,bdp_day#8] parquet

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2888)
	at org.apache.spark.sql.Dataset.select(Dataset.scala:1154)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at release.util.SparkHelper$.readTableData(SparkHelper.scala:26)
	at release.etl.dw.DwReleaseRegisterUsers$.handleReleaseJob(DwReleaseRegisterUsers.scala:58)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:106)
	at release.etl.dw.DwReleaseRegisterUsers$$anonfun$handleJobs$1.apply(DwReleaseRegisterUsers.scala:104)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at release.etl.dw.DwReleaseRegisterUsers$.handleJobs(DwReleaseRegisterUsers.scala:104)
	at release.etl.dw.DwReleaseRegisterUsers$.main(DwReleaseRegisterUsers.scala:32)
	at release.etl.dw.DwReleaseRegisterUsers.main(DwReleaseRegisterUsers.scala)
19:19:26.808 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:19:26.815 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@14843e66{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:19:26.817 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
19:19:26.829 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:19:26.847 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:19:26.847 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:19:26.855 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:19:26.858 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:19:26.865 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:19:26.865 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:19:26.866 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-3b441e82-4ecc-4748-82ff-478a9b44be23
19:22:35.607 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:22:35.904 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:22:35.927 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
19:22:35.927 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
19:22:35.928 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:22:35.928 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:22:35.928 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
19:22:36.793 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 11479.
19:22:36.810 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:22:36.826 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:22:36.829 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:22:36.830 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:22:36.839 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-0e193957-299f-49b3-bf04-8e1da4ed98e1
19:22:36.856 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
19:22:36.907 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:22:36.985 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4006ms
19:22:37.053 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:22:37.072 INFO  [main] org.spark_project.jetty.server.Server - Started @4093ms
19:22:37.097 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6321c85c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:22:37.097 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:22:37.122 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50cf5a23{/jobs,null,AVAILABLE,@Spark}
19:22:37.123 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/json,null,AVAILABLE,@Spark}
19:22:37.123 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/jobs/job,null,AVAILABLE,@Spark}
19:22:37.124 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/job/json,null,AVAILABLE,@Spark}
19:22:37.125 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages,null,AVAILABLE,@Spark}
19:22:37.126 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/json,null,AVAILABLE,@Spark}
19:22:37.127 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/stage,null,AVAILABLE,@Spark}
19:22:37.128 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/stage/json,null,AVAILABLE,@Spark}
19:22:37.129 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool,null,AVAILABLE,@Spark}
19:22:37.130 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/pool/json,null,AVAILABLE,@Spark}
19:22:37.131 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage,null,AVAILABLE,@Spark}
19:22:37.131 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/json,null,AVAILABLE,@Spark}
19:22:37.132 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd,null,AVAILABLE,@Spark}
19:22:37.133 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd/json,null,AVAILABLE,@Spark}
19:22:37.134 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment,null,AVAILABLE,@Spark}
19:22:37.135 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/environment/json,null,AVAILABLE,@Spark}
19:22:37.136 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors,null,AVAILABLE,@Spark}
19:22:37.137 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/json,null,AVAILABLE,@Spark}
19:22:37.138 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump,null,AVAILABLE,@Spark}
19:22:37.138 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:22:37.147 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/static,null,AVAILABLE,@Spark}
19:22:37.148 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/,null,AVAILABLE,@Spark}
19:22:37.149 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/api,null,AVAILABLE,@Spark}
19:22:37.150 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/jobs/job/kill,null,AVAILABLE,@Spark}
19:22:37.151 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/stages/stage/kill,null,AVAILABLE,@Spark}
19:22:37.153 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
19:22:37.225 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:22:37.255 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11520.
19:22:37.256 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:11520
19:22:37.258 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:22:37.297 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 11520, None)
19:22:37.300 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:11520 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 11520, None)
19:22:37.303 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 11520, None)
19:22:37.303 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 11520, None)
19:22:37.493 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319854f0{/metrics/json,null,AVAILABLE,@Spark}
19:22:38.985 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
19:22:39.013 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
19:22:39.014 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
19:22:39.023 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL,null,AVAILABLE,@Spark}
19:22:39.024 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@125d47c4{/SQL/json,null,AVAILABLE,@Spark}
19:22:39.025 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution,null,AVAILABLE,@Spark}
19:22:39.025 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f2466c{/SQL/execution/json,null,AVAILABLE,@Spark}
19:22:39.027 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b53840a{/static/sql,null,AVAILABLE,@Spark}
19:22:39.544 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:22:40.218 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:22:40.244 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:22:41.433 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:22:42.616 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:22:42.618 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:22:42.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:22:42.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:22:42.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:22:42.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:22:42.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
19:22:43.000 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:22:43.000 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:22:43.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:22:43.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:22:43.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:22:43.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:22:43.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:22:43.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:22:43.522 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/57e37241-bbf4-47b0-acf1-78906df57c20_resources
19:22:43.546 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/57e37241-bbf4-47b0-acf1-78906df57c20
19:22:43.549 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/57e37241-bbf4-47b0-acf1-78906df57c20
19:22:43.556 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/57e37241-bbf4-47b0-acf1-78906df57c20/_tmp_space.db
19:22:43.559 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:22:43.582 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:22:43.582 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:22:43.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:22:43.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:22:43.595 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:22:43.747 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/142203fd-f5ca-4e13-a71a-991170f1b86e_resources
19:22:43.756 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/142203fd-f5ca-4e13-a71a-991170f1b86e
19:22:43.760 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/142203fd-f5ca-4e13-a71a-991170f1b86e
19:22:43.766 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/142203fd-f5ca-4e13-a71a-991170f1b86e/_tmp_space.db
19:22:43.785 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:22:43.808 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:22:43.814 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:22:44.010 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:22:44.010 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:22:44.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:22:44.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:22:44.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:22:44.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:22:44.130 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:44.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:22:44.457 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
19:22:44.483 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:22:44.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:22:44.485 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:22:44.485 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:22:44.485 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:22:44.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:22:44.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:22:44.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:22:44.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:22:44.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:22:44.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:22:44.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:22:44.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:22:44.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:22:44.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:22:44.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:22:44.797 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.797 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.799 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.799 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.799 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:44.799 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:44.823 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:22:44.823 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:22:44.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:22:44.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:22:45.019 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
19:22:45.021 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
19:22:45.022 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
19:22:45.033 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
19:22:45.036 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
19:22:45.390 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 229.674453 ms
19:22:45.634 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 45.105023 ms
19:22:45.718 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
19:22:45.821 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
19:22:45.824 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:11520 (size: 26.3 KB, free: 1988.7 MB)
19:22:45.828 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseRegisterUsers.scala:66
19:22:45.835 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
19:22:45.939 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseRegisterUsers.scala:66
19:22:45.953 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DwReleaseRegisterUsers.scala:66)
19:22:45.955 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseRegisterUsers.scala:66) with 1 output partitions
19:22:45.956 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseRegisterUsers.scala:66)
19:22:45.956 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:22:45.958 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
19:22:45.961 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseRegisterUsers.scala:66), which has no missing parents
19:22:46.068 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 21.7 KB, free 1988.3 MB)
19:22:46.074 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.3 MB)
19:22:46.074 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:11520 (size: 9.6 KB, free: 1988.7 MB)
19:22:46.075 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:22:46.088 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19:22:46.089 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
19:22:46.124 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
19:22:46.126 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
19:22:46.126 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
19:22:46.127 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
19:22:46.127 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
19:22:46.128 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
19:22:46.128 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
19:22:46.128 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
19:22:46.137 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
19:22:46.137 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
19:22:46.137 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
19:22:46.137 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
19:22:46.137 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
19:22:46.137 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
19:22:46.137 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
19:22:46.137 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:22:46.258 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.122681 ms
19:22:46.280 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
19:22:46.280 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
19:22:46.280 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
19:22:46.280 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
19:22:46.280 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
19:22:46.280 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
19:22:46.280 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
19:22:46.280 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
19:22:46.333 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
19:22:47.439 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.440 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.440 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.440 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.440 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.440 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.441 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.479 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:47.717 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1610 bytes result sent to driver
19:22:47.718 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1653 bytes result sent to driver
19:22:47.718 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1653 bytes result sent to driver
19:22:47.718 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1610 bytes result sent to driver
19:22:47.718 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1610 bytes result sent to driver
19:22:47.719 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1653 bytes result sent to driver
19:22:47.723 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1567 bytes result sent to driver
19:22:47.733 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1602 ms on localhost (executor driver) (1/8)
19:22:47.736 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1609 ms on localhost (executor driver) (2/8)
19:22:47.736 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1609 ms on localhost (executor driver) (3/8)
19:22:47.736 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1610 ms on localhost (executor driver) (4/8)
19:22:47.737 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1611 ms on localhost (executor driver) (5/8)
19:22:47.738 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1623 ms on localhost (executor driver) (6/8)
19:22:47.738 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1610 ms on localhost (executor driver) (7/8)
19:22:49.374 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1739 bytes result sent to driver
19:22:49.375 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 3248 ms on localhost (executor driver) (8/8)
19:22:49.376 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:22:49.377 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseRegisterUsers.scala:66) finished in 3.270 s
19:22:49.378 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
19:22:49.378 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
19:22:49.378 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
19:22:49.379 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
19:22:49.381 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseRegisterUsers.scala:66), which has no missing parents
19:22:49.386 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
19:22:49.388 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
19:22:49.389 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:11520 (size: 2.2 KB, free: 1988.7 MB)
19:22:49.390 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:22:49.391 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
19:22:49.391 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:22:49.393 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
19:22:49.394 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
19:22:49.405 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:22:49.407 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
19:22:49.423 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 5886 bytes result sent to driver
19:22:49.424 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 32 ms on localhost (executor driver) (1/1)
19:22:49.425 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:22:49.425 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseRegisterUsers.scala:66) finished in 0.033 s
19:22:49.429 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseRegisterUsers.scala:66, took 3.489470 s
19:22:49.455 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
19:22:49.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:49.462 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:49.488 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.488 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.488 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.488 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.488 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.488 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.489 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.489 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.489 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:49.560 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1
19:22:49.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:22:49.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:22:49.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:22:49.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:22:49.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:22:49.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:22:49.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:49.771 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:49.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:22:49.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:22:49.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:22:49.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:22:49.811 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
19:22:49.812 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
19:22:49.812 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
19:22:49.813 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
19:22:49.813 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
19:22:49.828 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:22:49.829 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:22:49.861 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.478038 ms
19:22:49.876 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
19:22:49.901 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
19:22:49.902 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:11520 (size: 26.3 KB, free: 1988.6 MB)
19:22:49.903 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
19:22:49.903 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
19:22:49.984 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
19:22:49.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:35)
19:22:49.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 4 output partitions
19:22:49.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:35)
19:22:49.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
19:22:49.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
19:22:49.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
19:22:49.991 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 21.7 KB, free 1988.0 MB)
19:22:49.993 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.0 MB)
19:22:49.993 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:11520 (size: 9.6 KB, free: 1988.6 MB)
19:22:49.994 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
19:22:49.994 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19:22:49.994 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 8 tasks
19:22:49.995 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 9, localhost, executor driver, partition 0, ANY, 5391 bytes)
19:22:49.995 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 10, localhost, executor driver, partition 1, ANY, 5391 bytes)
19:22:49.996 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 11, localhost, executor driver, partition 2, ANY, 5391 bytes)
19:22:49.996 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 12, localhost, executor driver, partition 3, ANY, 5391 bytes)
19:22:49.996 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 13, localhost, executor driver, partition 4, ANY, 5391 bytes)
19:22:49.997 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 14, localhost, executor driver, partition 5, ANY, 5391 bytes)
19:22:49.997 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 15, localhost, executor driver, partition 6, ANY, 5391 bytes)
19:22:49.997 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 16, localhost, executor driver, partition 7, ANY, 5391 bytes)
19:22:49.997 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 10)
19:22:49.997 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 9)
19:22:49.997 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 11)
19:22:49.997 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 12)
19:22:49.997 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 13)
19:22:49.997 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 15)
19:22:49.997 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 14)
19:22:49.997 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 16)
19:22:50.005 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
19:22:50.005 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
19:22:50.008 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
19:22:50.009 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
19:22:50.011 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
19:22:50.014 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
19:22:50.017 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
19:22:50.020 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
19:22:50.127 INFO  [Executor task launch worker for task 12] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.132 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.133 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.134 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.136 INFO  [Executor task launch worker for task 15] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.136 INFO  [Executor task launch worker for task 14] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.148 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.156 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 9). 1567 bytes result sent to driver
19:22:50.160 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 9) in 165 ms on localhost (executor driver) (1/8)
19:22:50.163 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 11). 1567 bytes result sent to driver
19:22:50.166 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 11) in 170 ms on localhost (executor driver) (2/8)
19:22:50.166 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 15). 1524 bytes result sent to driver
19:22:50.168 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 15) in 171 ms on localhost (executor driver) (3/8)
19:22:50.171 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 16). 1524 bytes result sent to driver
19:22:50.172 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 16) in 174 ms on localhost (executor driver) (4/8)
19:22:50.174 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 10). 1567 bytes result sent to driver
19:22:50.175 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 10) in 180 ms on localhost (executor driver) (5/8)
19:22:50.177 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 14). 1567 bytes result sent to driver
19:22:50.178 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 14) in 182 ms on localhost (executor driver) (6/8)
19:22:50.178 INFO  [Executor task launch worker for task 13] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
19:22:50.201 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 13). 1481 bytes result sent to driver
19:22:50.202 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 13) in 206 ms on localhost (executor driver) (7/8)
19:22:50.474 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:11520 in memory (size: 2.2 KB, free: 1988.6 MB)
19:22:50.477 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
19:22:50.746 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 12). 1782 bytes result sent to driver
19:22:50.747 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 12) in 751 ms on localhost (executor driver) (8/8)
19:22:50.748 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
19:22:50.749 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:35) finished in 0.753 s
19:22:50.749 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
19:22:50.749 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
19:22:50.749 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
19:22:50.749 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
19:22:50.749 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35), which has no missing parents
19:22:50.789 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
19:22:50.791 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
19:22:50.792 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:11520 (size: 64.1 KB, free: 1988.6 MB)
19:22:50.792 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
19:22:50.793 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
19:22:50.793 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
19:22:50.793 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 17, localhost, executor driver, partition 0, ANY, 4726 bytes)
19:22:50.793 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 18, localhost, executor driver, partition 1, ANY, 4726 bytes)
19:22:50.794 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 19, localhost, executor driver, partition 2, ANY, 4726 bytes)
19:22:50.794 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 20, localhost, executor driver, partition 3, ANY, 4726 bytes)
19:22:50.794 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 18)
19:22:50.794 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 17)
19:22:50.794 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 20)
19:22:50.794 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 19)
19:22:50.833 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:22:50.833 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:22:50.833 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:22:50.833 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:22:50.833 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:22:50.833 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:22:50.834 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:22:50.834 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:22:50.852 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.372847 ms
19:22:50.874 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.767912 ms
19:22:50.916 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:22:50.916 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:22:50.916 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:22:50.916 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:22:50.918 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:22:50.918 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:22:50.918 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:22:50.918 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:22:50.929 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.661438 ms
19:22:50.973 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.458389 ms
19:22:50.987 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.393491 ms
19:22:51.072 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4f8bfc2f
19:22:51.073 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69f11e17
19:22:51.073 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2050b6a
19:22:51.073 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d557b62
19:22:51.084 INFO  [Executor task launch worker for task 19] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
19:22:51.089 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:22:51.089 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:22:51.089 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:22:51.089 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:22:51.089 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/_temporary/attempt_20190925192250_0003_m_000002_0/bdp_day=20190924/part-00002-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000
19:22:51.089 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/_temporary/attempt_20190925192250_0003_m_000003_0/bdp_day=20190924/part-00003-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000
19:22:51.089 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/_temporary/attempt_20190925192250_0003_m_000001_0/bdp_day=20190924/part-00001-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000
19:22:51.089 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/_temporary/attempt_20190925192250_0003_m_000000_0/bdp_day=20190924/part-00000-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000
19:22:51.091 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression set to false
19:22:51.091 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression set to false
19:22:51.091 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression set to false
19:22:51.091 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression set to false
19:22:51.092 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:22:51.092 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:22:51.092 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:22:51.092 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:22:51.092 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:22:51.093 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:22:51.093 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:22:51.093 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:22:51.093 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:22:51.093 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:22:51.093 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:22:51.093 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:22:51.093 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:22:51.093 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Validation is off
19:22:51.093 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Validation is off
19:22:51.093 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Validation is off
19:22:51.093 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Validation is off
19:22:51.094 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:22:51.094 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:22:51.094 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:22:51.094 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:22:51.418 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@505f5f01
19:22:51.418 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@472da6ab
19:22:51.419 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5aa40179
19:22:51.419 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@35f75c23
19:22:51.526 INFO  [Executor task launch worker for task 17] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,890
19:22:51.528 INFO  [Executor task launch worker for task 18] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,616
19:22:51.528 INFO  [Executor task launch worker for task 20] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 323,318
19:22:51.529 INFO  [Executor task launch worker for task 19] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,880
19:22:51.607 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:11520 in memory (size: 9.6 KB, free: 1988.6 MB)
19:22:51.639 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.639 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.639 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.639 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.641 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.641 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.642 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.642 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.643 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.643 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.643 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.643 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.643 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.644 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.644 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.644 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.644 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
19:22:51.644 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
19:22:51.644 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
19:22:51.645 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
19:22:51.645 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
19:22:51.645 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
19:22:51.645 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
19:22:51.646 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.646 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.646 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.646 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
19:22:51.646 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.646 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.647 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.647 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
19:22:51.648 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
19:22:51.906 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925192250_0003_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/task_20190925192250_0003_m_000000
19:22:51.906 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925192250_0003_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/task_20190925192250_0003_m_000001
19:22:51.907 INFO  [Executor task launch worker for task 18] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925192250_0003_m_000001_0: Committed
19:22:51.907 INFO  [Executor task launch worker for task 17] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925192250_0003_m_000000_0: Committed
19:22:51.909 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 18). 2658 bytes result sent to driver
19:22:51.909 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 17). 2658 bytes result sent to driver
19:22:51.910 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 18) in 1117 ms on localhost (executor driver) (1/4)
19:22:51.911 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 17) in 1118 ms on localhost (executor driver) (2/4)
19:22:51.928 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925192250_0003_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/task_20190925192250_0003_m_000002
19:22:51.928 INFO  [Executor task launch worker for task 19] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925192250_0003_m_000002_0: Committed
19:22:51.928 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925192250_0003_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/_temporary/0/task_20190925192250_0003_m_000003
19:22:51.928 INFO  [Executor task launch worker for task 20] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925192250_0003_m_000003_0: Committed
19:22:51.928 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 20). 2615 bytes result sent to driver
19:22:51.928 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 19). 2615 bytes result sent to driver
19:22:51.930 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 19) in 1137 ms on localhost (executor driver) (3/4)
19:22:51.930 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 20) in 1136 ms on localhost (executor driver) (4/4)
19:22:51.930 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
19:22:51.930 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:35) finished in 1.137 s
19:22:51.931 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 1.946469 s
19:22:51.969 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
19:22:51.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:51.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:51.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:51.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:52.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:52.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:52.013 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:52.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:52.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:22:52.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:22:52.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:22:52.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:22:52.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:22:52.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:22:52.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:22:52.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:22:52.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:52.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:52.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
19:22:52.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
19:22:52.133 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
19:22:52.145 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/bdp_day=20190924/part-00000-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00000-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, Status:true
19:22:52.165 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/bdp_day=20190924/part-00001-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00001-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, Status:true
19:22:52.177 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/bdp_day=20190924/part-00002-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00002-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, Status:true
19:22:52.189 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/bdp_day=20190924/part-00003-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00003-6b182309-e8d1-4363-bd3e-6f07b6d8c360.c000, Status:true
19:22:52.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
19:22:52.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
19:22:52.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_register_users
19:22:52.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_register_users	
19:22:52.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
19:22:52.273 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
19:22:52.277 WARN  [main] hive.log - Updated size to 872108
19:22:53.035 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_19-22-49_558_3979660115614255764-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
19:22:53.044 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
19:22:53.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:22:53.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:22:53.052 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:53.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:53.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:53.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:53.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.105 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.105 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.105 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.105 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.106 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.106 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.106 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.106 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:53.110 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
19:22:53.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:22:53.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:22:53.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:53.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:53.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:53.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:53.176 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.178 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.179 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.179 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.179 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.179 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:53.179 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:53.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
19:22:53.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
19:22:53.300 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
19:22:53.301 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 1 output partitions
19:22:53.301 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (insertInto at SparkHelper.scala:35)
19:22:53.301 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
19:22:53.301 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:22:53.301 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:35), which has no missing parents
19:22:53.322 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 74.4 KB, free 1987.7 MB)
19:22:53.324 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.9 KB, free 1987.7 MB)
19:22:53.325 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:11520 (size: 26.9 KB, free: 1988.6 MB)
19:22:53.326 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
19:22:53.327 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
19:22:53.327 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
19:22:53.331 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 5055 bytes)
19:22:53.332 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 21)
19:22:53.943 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 21). 1721 bytes result sent to driver
19:22:53.944 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 21) in 617 ms on localhost (executor driver) (1/1)
19:22:53.944 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
19:22:53.944 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (insertInto at SparkHelper.scala:35) finished in 0.617 s
19:22:53.946 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.645264 s
19:22:53.953 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_register_users
19:22:53.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:22:53.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:22:53.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:53.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:53.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:53.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:54.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:54.015 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:54.015 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:54.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:54.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:54.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.065 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.065 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:22:54.065 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:22:54.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
19:22:54.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
19:22:54.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_register_users newtbl=dw_release_register_users
19:22:54.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_register_users newtbl=dw_release_register_users	
19:22:54.267 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:22:54.273 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6321c85c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:22:54.275 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
19:22:54.289 INFO  [dispatcher-event-loop-6] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:22:54.417 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:22:54.417 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:22:54.418 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:22:54.420 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:22:54.425 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:22:54.425 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:22:54.426 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-4a5288f4-8e76-466d-a1de-e484f6c3c2ca
19:35:56.366 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
19:35:56.693 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
19:35:56.713 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
19:35:56.713 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
19:35:56.714 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
19:35:56.714 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
19:35:56.715 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
19:35:57.556 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 12268.
19:35:57.573 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
19:35:57.590 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
19:35:57.593 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19:35:57.594 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
19:35:57.604 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-672a7175-6c18-449a-88f2-c162e292361d
19:35:57.621 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
19:35:57.671 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
19:35:57.752 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3904ms
19:35:57.815 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
19:35:57.829 INFO  [main] org.spark_project.jetty.server.Server - Started @3983ms
19:35:57.854 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:35:57.854 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
19:35:57.877 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
19:35:57.877 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
19:35:57.878 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
19:35:57.879 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
19:35:57.880 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
19:35:57.880 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
19:35:57.881 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
19:35:57.882 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
19:35:57.882 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
19:35:57.883 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
19:35:57.883 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
19:35:57.884 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
19:35:57.884 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
19:35:57.885 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
19:35:57.886 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
19:35:57.886 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
19:35:57.887 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
19:35:57.887 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
19:35:57.887 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
19:35:57.888 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
19:35:57.893 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
19:35:57.894 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
19:35:57.896 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
19:35:57.896 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
19:35:57.897 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
19:35:57.899 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
19:35:57.974 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
19:35:58.001 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12310.
19:35:58.002 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:12310
19:35:58.004 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19:35:58.038 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 12310, None)
19:35:58.042 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:12310 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 12310, None)
19:35:58.045 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 12310, None)
19:35:58.046 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 12310, None)
19:35:58.238 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
19:35:59.642 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
19:35:59.665 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
19:35:59.666 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
19:35:59.672 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
19:35:59.673 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
19:35:59.673 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
19:35:59.674 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
19:35:59.675 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
19:36:00.159 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19:36:00.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19:36:00.932 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
19:36:02.058 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19:36:03.123 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
19:36:03.125 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
19:36:03.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
19:36:03.344 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
19:36:03.395 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
19:36:03.497 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
19:36:03.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
19:36:03.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
19:36:03.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19:36:03.553 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
19:36:03.554 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
19:36:03.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
19:36:03.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
19:36:03.561 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
19:36:03.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
19:36:04.070 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/f8ca79a3-2ce6-4d70-903d-c407a60154ac_resources
19:36:04.082 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f8ca79a3-2ce6-4d70-903d-c407a60154ac
19:36:04.085 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/f8ca79a3-2ce6-4d70-903d-c407a60154ac
19:36:04.090 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f8ca79a3-2ce6-4d70-903d-c407a60154ac/_tmp_space.db
19:36:04.094 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:36:04.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
19:36:04.115 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
19:36:04.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
19:36:04.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
19:36:04.129 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
19:36:04.306 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/dbae7439-19f4-43d4-8a08-42e3a78609e8_resources
19:36:04.317 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/dbae7439-19f4-43d4-8a08-42e3a78609e8
19:36:04.320 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/dbae7439-19f4-43d4-8a08-42e3a78609e8
19:36:04.330 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/dbae7439-19f4-43d4-8a08-42e3a78609e8/_tmp_space.db
19:36:04.333 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
19:36:04.363 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
19:36:04.370 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
19:36:04.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:36:04.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:36:04.575 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:36:04.576 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:36:04.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:36:04.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:36:04.697 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.711 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.711 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.711 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.711 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:04.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:04.727 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
19:36:05.060 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
19:36:05.072 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
19:36:05.073 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
19:36:05.074 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
19:36:05.074 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
19:36:05.074 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
19:36:05.074 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
19:36:05.075 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
19:36:05.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:36:05.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:36:05.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:36:05.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:36:05.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:36:05.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:36:05.372 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.372 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.374 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.374 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.375 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:05.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:05.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:36:05.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:36:05.403 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:36:05.404 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:36:05.594 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
19:36:05.596 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 04)
19:36:05.597 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
19:36:05.607 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
19:36:05.614 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
19:36:05.986 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 248.280295 ms
19:36:06.248 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 46.769025 ms
19:36:06.347 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
19:36:06.453 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
19:36:06.456 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:12310 (size: 26.2 KB, free: 1988.7 MB)
19:36:06.460 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseClick.scala:66
19:36:06.469 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
19:36:06.570 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseClick.scala:66
19:36:06.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at DwReleaseClick.scala:66)
19:36:06.587 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseClick.scala:66) with 1 output partitions
19:36:06.587 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseClick.scala:66)
19:36:06.587 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
19:36:06.589 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
19:36:06.593 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseClick.scala:66), which has no missing parents
19:36:06.669 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 17.6 KB, free 1988.4 MB)
19:36:06.672 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.3 MB)
19:36:06.674 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:12310 (size: 7.5 KB, free: 1988.7 MB)
19:36:06.675 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
19:36:06.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at DwReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19:36:06.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
19:36:06.738 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
19:36:06.742 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
19:36:06.743 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
19:36:06.744 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
19:36:06.745 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
19:36:06.745 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
19:36:06.746 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
19:36:06.746 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
19:36:06.758 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
19:36:06.758 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
19:36:06.758 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
19:36:06.758 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
19:36:06.758 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
19:36:06.758 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
19:36:06.758 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
19:36:06.758 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
19:36:06.793 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
19:36:06.879 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
19:36:06.879 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
19:36:06.879 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
19:36:06.879 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
19:36:06.879 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
19:36:06.879 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
19:36:06.879 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
19:36:06.879 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
19:36:08.076 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.076 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.076 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.076 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.076 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.076 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.077 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.098 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:08.163 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1414 bytes result sent to driver
19:36:08.163 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1414 bytes result sent to driver
19:36:08.163 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1414 bytes result sent to driver
19:36:08.163 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1414 bytes result sent to driver
19:36:08.164 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1457 bytes result sent to driver
19:36:08.164 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1457 bytes result sent to driver
19:36:08.167 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1414 bytes result sent to driver
19:36:08.174 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1427 ms on localhost (executor driver) (1/8)
19:36:08.176 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1449 ms on localhost (executor driver) (2/8)
19:36:08.177 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1435 ms on localhost (executor driver) (3/8)
19:36:08.177 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1436 ms on localhost (executor driver) (4/8)
19:36:08.178 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1434 ms on localhost (executor driver) (5/8)
19:36:08.178 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1433 ms on localhost (executor driver) (6/8)
19:36:08.178 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1432 ms on localhost (executor driver) (7/8)
19:36:08.660 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1586 bytes result sent to driver
19:36:08.661 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1918 ms on localhost (executor driver) (8/8)
19:36:08.662 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
19:36:08.662 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseClick.scala:66) finished in 1.947 s
19:36:08.663 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
19:36:08.663 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
19:36:08.664 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
19:36:08.664 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
19:36:08.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseClick.scala:66), which has no missing parents
19:36:08.671 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
19:36:08.672 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
19:36:08.674 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:12310 (size: 2.2 KB, free: 1988.7 MB)
19:36:08.674 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
19:36:08.676 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at DwReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
19:36:08.676 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
19:36:08.679 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
19:36:08.679 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
19:36:08.690 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:36:08.691 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
19:36:08.731 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 4737 bytes result sent to driver
19:36:08.732 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 56 ms on localhost (executor driver) (1/1)
19:36:08.732 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
19:36:08.733 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseClick.scala:66) finished in 0.056 s
19:36:08.737 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseClick.scala:66, took 2.166903 s
19:36:08.763 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
19:36:08.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:08.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:08.797 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:08.797 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:08.797 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:08.797 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:08.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:08.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:08.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:08.798 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:08.873 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1
19:36:09.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
19:36:09.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
19:36:09.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:36:09.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:36:09.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
19:36:09.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
19:36:09.072 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.072 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.073 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.073 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.073 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.073 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.073 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.073 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:09.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:09.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
19:36:09.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
19:36:09.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
19:36:09.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
19:36:09.115 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
19:36:09.116 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 04)
19:36:09.116 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
19:36:09.116 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
19:36:09.117 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
19:36:09.130 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:36:09.131 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:36:09.188 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 311.6 KB, free 1988.0 MB)
19:36:09.283 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
19:36:09.284 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:12310 (size: 26.2 KB, free: 1988.6 MB)
19:36:09.285 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
19:36:09.285 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
19:36:09.351 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
19:36:09.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:35)
19:36:09.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 4 output partitions
19:36:09.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:35)
19:36:09.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
19:36:09.353 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
19:36:09.353 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35), which has no missing parents
19:36:09.355 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 17.6 KB, free 1988.0 MB)
19:36:09.358 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.0 MB)
19:36:09.359 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:12310 (size: 7.5 KB, free: 1988.6 MB)
19:36:09.359 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
19:36:09.360 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
19:36:09.360 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 8 tasks
19:36:09.361 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 9, localhost, executor driver, partition 0, ANY, 5391 bytes)
19:36:09.362 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 10, localhost, executor driver, partition 1, ANY, 5391 bytes)
19:36:09.362 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 11, localhost, executor driver, partition 2, ANY, 5391 bytes)
19:36:09.362 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 12, localhost, executor driver, partition 3, ANY, 5391 bytes)
19:36:09.363 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 13, localhost, executor driver, partition 4, ANY, 5391 bytes)
19:36:09.363 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 14, localhost, executor driver, partition 5, ANY, 5391 bytes)
19:36:09.364 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 15, localhost, executor driver, partition 6, ANY, 5391 bytes)
19:36:09.364 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 16, localhost, executor driver, partition 7, ANY, 5391 bytes)
19:36:09.364 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 9)
19:36:09.364 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 10)
19:36:09.364 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 14)
19:36:09.364 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 16)
19:36:09.364 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 11)
19:36:09.365 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 12)
19:36:09.366 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 13)
19:36:09.370 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
19:36:09.370 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
19:36:09.370 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
19:36:09.373 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
19:36:09.374 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
19:36:09.375 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
19:36:09.376 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 15)
19:36:09.381 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
19:36:09.383 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
19:36:09.409 INFO  [Executor task launch worker for task 14] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.409 INFO  [Executor task launch worker for task 13] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.409 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.453 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.456 INFO  [Executor task launch worker for task 12] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.461 INFO  [Executor task launch worker for task 15] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.467 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.478 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
19:36:09.487 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 13). 1371 bytes result sent to driver
19:36:09.489 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 10). 1328 bytes result sent to driver
19:36:09.490 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 13) in 128 ms on localhost (executor driver) (1/8)
19:36:09.491 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 10) in 130 ms on localhost (executor driver) (2/8)
19:36:09.493 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 9). 1371 bytes result sent to driver
19:36:09.495 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 9) in 134 ms on localhost (executor driver) (3/8)
19:36:09.497 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 16). 1371 bytes result sent to driver
19:36:09.499 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 16) in 135 ms on localhost (executor driver) (4/8)
19:36:09.502 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 15). 1371 bytes result sent to driver
19:36:09.503 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 15) in 140 ms on localhost (executor driver) (5/8)
19:36:09.505 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 14). 1371 bytes result sent to driver
19:36:09.506 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 14) in 143 ms on localhost (executor driver) (6/8)
19:36:09.509 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 11). 1371 bytes result sent to driver
19:36:09.510 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 11) in 148 ms on localhost (executor driver) (7/8)
19:36:09.734 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 12). 1629 bytes result sent to driver
19:36:09.735 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 12) in 373 ms on localhost (executor driver) (8/8)
19:36:09.735 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
19:36:09.736 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:35) finished in 0.375 s
19:36:09.736 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
19:36:09.736 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
19:36:09.736 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
19:36:09.736 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
19:36:09.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
19:36:09.799 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 167.4 KB, free 1987.8 MB)
19:36:09.802 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 62.0 KB, free 1987.8 MB)
19:36:09.802 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:12310 (size: 62.0 KB, free: 1988.6 MB)
19:36:09.803 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
19:36:09.804 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
19:36:09.804 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
19:36:09.805 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 17, localhost, executor driver, partition 0, ANY, 4726 bytes)
19:36:09.805 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 18, localhost, executor driver, partition 1, ANY, 4726 bytes)
19:36:09.805 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 19, localhost, executor driver, partition 2, ANY, 4726 bytes)
19:36:09.805 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 20, localhost, executor driver, partition 3, ANY, 4726 bytes)
19:36:09.806 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 17)
19:36:09.806 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 18)
19:36:09.806 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 19)
19:36:09.806 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 20)
19:36:09.843 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:36:09.843 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:36:09.844 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:36:09.845 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
19:36:09.845 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:36:09.845 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:36:09.846 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
19:36:09.846 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
19:36:09.865 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.716554 ms
19:36:09.889 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.359418 ms
19:36:09.947 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:36:09.947 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:36:09.947 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:36:09.948 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:36:09.948 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:36:09.948 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:36:09.949 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
19:36:09.949 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
19:36:09.963 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.557638 ms
19:36:10.023 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.997156 ms
19:36:10.036 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.768699 ms
19:36:10.132 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5ca01530
19:36:10.133 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@20656cba
19:36:10.133 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@137195d0
19:36:10.133 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a7b9020
19:36:10.144 INFO  [Executor task launch worker for task 17] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
19:36:10.151 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:36:10.151 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:36:10.151 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:36:10.151 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
19:36:10.151 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/_temporary/attempt_20190925193609_0003_m_000000_0/bdp_day=20190924/part-00000-d625d162-c715-4e03-b50d-71be332b2008.c000
19:36:10.151 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/_temporary/attempt_20190925193609_0003_m_000001_0/bdp_day=20190924/part-00001-d625d162-c715-4e03-b50d-71be332b2008.c000
19:36:10.151 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/_temporary/attempt_20190925193609_0003_m_000002_0/bdp_day=20190924/part-00002-d625d162-c715-4e03-b50d-71be332b2008.c000
19:36:10.151 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/_temporary/attempt_20190925193609_0003_m_000003_0/bdp_day=20190924/part-00003-d625d162-c715-4e03-b50d-71be332b2008.c000
19:36:10.155 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression set to false
19:36:10.155 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression set to false
19:36:10.155 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression set to false
19:36:10.155 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression set to false
19:36:10.155 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:36:10.155 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:36:10.155 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:36:10.156 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
19:36:10.156 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:36:10.157 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:36:10.157 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:36:10.157 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
19:36:10.157 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
19:36:10.157 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:36:10.157 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:36:10.157 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:36:10.157 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Dictionary is on
19:36:10.157 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Validation is off
19:36:10.157 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Validation is off
19:36:10.157 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Validation is off
19:36:10.157 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Validation is off
19:36:10.157 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:36:10.157 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:36:10.161 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:36:10.161 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
19:36:10.234 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:12310 in memory (size: 7.5 KB, free: 1988.6 MB)
19:36:10.238 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
19:36:10.240 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:12310 in memory (size: 2.2 KB, free: 1988.6 MB)
19:36:10.366 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@41a9ac69
19:36:10.367 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@399d9f50
19:36:10.367 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@e8da55a
19:36:10.367 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3317c2c3
19:36:10.543 INFO  [Executor task launch worker for task 20] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,893
19:36:10.543 INFO  [Executor task launch worker for task 18] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,391
19:36:10.547 INFO  [Executor task launch worker for task 19] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,120
19:36:10.547 INFO  [Executor task launch worker for task 17] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,377
19:36:10.644 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.644 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.644 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.644 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 290,389B for [release_session] BINARY: 10,752 values, 290,312B raw, 290,312B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.647 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.647 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.647 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 107,571B for [device_num] BINARY: 10,752 values, 107,528B raw, 107,528B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.647 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.648 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 2,750B for [device_type] BINARY: 10,752 values, 2,719B raw, 2,719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
19:36:10.648 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.648 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.648 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
19:36:10.649 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 4,105B for [sources] BINARY: 10,752 values, 4,063B raw, 4,063B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
19:36:10.649 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.649 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
19:36:10.649 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
19:36:10.650 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.650 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
19:36:10.650 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.650 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 16,206B for [ct] INT64: 10,752 values, 16,159B raw, 16,159B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,420 entries, 19,360B raw, 2,420B comp}
19:36:10.650 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
19:36:10.650 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
19:36:10.651 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,431 entries, 19,448B raw, 2,431B comp}
19:36:10.651 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
19:36:10.651 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.651 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,404 entries, 19,232B raw, 2,404B comp}
19:36:10.654 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
19:36:10.655 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,412 entries, 19,296B raw, 2,412B comp}
19:36:10.913 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925193609_0003_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/task_20190925193609_0003_m_000001
19:36:10.913 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925193609_0003_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/task_20190925193609_0003_m_000000
19:36:10.913 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925193609_0003_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/task_20190925193609_0003_m_000002
19:36:10.914 INFO  [Executor task launch worker for task 18] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925193609_0003_m_000001_0: Committed
19:36:10.914 INFO  [Executor task launch worker for task 19] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925193609_0003_m_000002_0: Committed
19:36:10.914 INFO  [Executor task launch worker for task 17] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925193609_0003_m_000000_0: Committed
19:36:10.917 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 18). 2568 bytes result sent to driver
19:36:10.917 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 17). 2568 bytes result sent to driver
19:36:10.917 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 19). 2568 bytes result sent to driver
19:36:10.919 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 19) in 1114 ms on localhost (executor driver) (1/4)
19:36:10.919 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 17) in 1115 ms on localhost (executor driver) (2/4)
19:36:10.919 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 18) in 1114 ms on localhost (executor driver) (3/4)
19:36:11.291 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925193609_0003_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/_temporary/0/task_20190925193609_0003_m_000003
19:36:11.291 INFO  [Executor task launch worker for task 20] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925193609_0003_m_000003_0: Committed
19:36:11.292 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 20). 2525 bytes result sent to driver
19:36:11.292 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 20) in 1487 ms on localhost (executor driver) (4/4)
19:36:11.292 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
19:36:11.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:35) finished in 1.489 s
19:36:11.293 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 1.941891 s
19:36:11.384 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
19:36:11.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:11.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:11.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:11.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:11.426 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.426 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.428 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:11.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:11.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:36:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:36:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:36:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:36:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:36:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:36:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
19:36:11.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
19:36:11.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:11.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:11.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]
19:36:11.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]	
19:36:11.560 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
19:36:11.572 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/bdp_day=20190924/part-00000-d625d162-c715-4e03-b50d-71be332b2008.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00000-d625d162-c715-4e03-b50d-71be332b2008.c000, Status:true
19:36:11.588 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/bdp_day=20190924/part-00001-d625d162-c715-4e03-b50d-71be332b2008.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00001-d625d162-c715-4e03-b50d-71be332b2008.c000, Status:true
19:36:11.597 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/bdp_day=20190924/part-00002-d625d162-c715-4e03-b50d-71be332b2008.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00002-d625d162-c715-4e03-b50d-71be332b2008.c000, Status:true
19:36:11.608 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/bdp_day=20190924/part-00003-d625d162-c715-4e03-b50d-71be332b2008.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00003-d625d162-c715-4e03-b50d-71be332b2008.c000, Status:true
19:36:11.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]
19:36:11.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]	
19:36:11.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_click
19:36:11.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_click	
19:36:11.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
19:36:11.689 WARN  [main] hive.log - Updating partition stats fast for: dw_release_click
19:36:11.691 WARN  [main] hive.log - Updated size to 1765422
19:36:11.916 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_19-36-08_872_5490483849542623199-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
19:36:11.922 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
19:36:11.927 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:36:11.927 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:36:11.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:11.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:11.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:11.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:11.979 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.979 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.979 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.979 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.979 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.980 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.980 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:11.980 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:11.983 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_click (inference mode: INFER_AND_SAVE)
19:36:11.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:36:11.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:36:11.990 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:11.990 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:12.015 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:12.015 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:12.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:12.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_click
19:36:12.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_click	
19:36:12.146 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
19:36:12.147 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 1 output partitions
19:36:12.147 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (insertInto at SparkHelper.scala:35)
19:36:12.147 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
19:36:12.147 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
19:36:12.147 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:35), which has no missing parents
19:36:12.171 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 74.4 KB, free 1987.7 MB)
19:36:12.173 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1987.7 MB)
19:36:12.173 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:12310 (size: 26.8 KB, free: 1988.6 MB)
19:36:12.174 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
19:36:12.174 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
19:36:12.174 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
19:36:12.177 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 5047 bytes)
19:36:12.178 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 21)
19:36:12.737 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 21). 1691 bytes result sent to driver
19:36:12.738 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 21) in 563 ms on localhost (executor driver) (1/1)
19:36:12.738 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
19:36:12.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (insertInto at SparkHelper.scala:35) finished in 0.563 s
19:36:12.739 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.593026 s
19:36:12.744 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_click
19:36:12.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
19:36:12.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
19:36:12.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:12.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:12.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:12.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:12.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:12.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:12.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:12.811 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:12.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:12.831 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.831 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.831 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.832 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.832 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.832 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.832 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
19:36:12.832 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
19:36:12.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
19:36:12.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
19:36:12.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_click newtbl=dw_release_click
19:36:12.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_click newtbl=dw_release_click	
19:36:12.983 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
19:36:12.989 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19:36:12.992 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
19:36:13.003 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
19:36:13.115 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
19:36:13.116 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
19:36:13.117 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
19:36:13.119 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
19:36:13.125 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
19:36:13.125 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
19:36:13.125 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-dbec2c8e-ca09-4514-ada1-f27adf211cfe
20:06:23.620 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:06:23.927 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
20:06:23.949 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
20:06:23.950 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
20:06:23.951 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:06:23.951 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:06:23.952 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
20:06:24.778 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14493.
20:06:24.797 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:06:24.814 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:06:24.818 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:06:24.819 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:06:24.828 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-5b1cac28-e5fe-4384-83f0-8e948c821a2f
20:06:24.844 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
20:06:24.889 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:06:24.970 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3896ms
20:06:25.039 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:06:25.053 INFO  [main] org.spark_project.jetty.server.Server - Started @3980ms
20:06:25.073 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:06:25.073 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:06:25.094 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
20:06:25.095 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
20:06:25.095 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
20:06:25.096 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
20:06:25.097 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
20:06:25.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
20:06:25.099 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
20:06:25.100 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
20:06:25.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
20:06:25.102 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
20:06:25.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
20:06:25.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
20:06:25.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
20:06:25.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
20:06:25.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
20:06:25.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
20:06:25.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
20:06:25.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
20:06:25.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
20:06:25.109 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:06:25.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
20:06:25.115 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
20:06:25.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
20:06:25.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
20:06:25.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
20:06:25.120 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
20:06:25.192 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:06:25.221 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14534.
20:06:25.222 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:14534
20:06:25.223 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:06:25.258 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 14534, None)
20:06:25.261 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:14534 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 14534, None)
20:06:25.265 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 14534, None)
20:06:25.265 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 14534, None)
20:06:25.440 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
20:06:26.846 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
20:06:26.877 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
20:06:26.878 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
20:06:26.885 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
20:06:26.885 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
20:06:26.886 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
20:06:26.886 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
20:06:26.888 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
20:06:27.362 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:06:28.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:06:28.034 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:06:29.248 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:06:30.489 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:06:30.492 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:06:30.701 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:06:30.705 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:06:30.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:06:30.845 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:06:30.846 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
20:06:30.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:06:30.859 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:06:30.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:06:30.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:06:30.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:06:30.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:06:30.914 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:06:30.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:06:31.415 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/db4552be-2362-4701-84ec-4f0949a2e785_resources
20:06:31.449 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/db4552be-2362-4701-84ec-4f0949a2e785
20:06:31.453 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/db4552be-2362-4701-84ec-4f0949a2e785
20:06:31.457 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/db4552be-2362-4701-84ec-4f0949a2e785/_tmp_space.db
20:06:31.461 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:06:31.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:31.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:31.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:06:31.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:06:31.494 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:06:31.666 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/7e86217d-788b-470d-abd5-b2b62acda3f1_resources
20:06:31.672 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7e86217d-788b-470d-abd5-b2b62acda3f1
20:06:31.675 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/7e86217d-788b-470d-abd5-b2b62acda3f1
20:06:31.680 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/7e86217d-788b-470d-abd5-b2b62acda3f1/_tmp_space.db
20:06:31.683 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:06:31.733 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:06:31.740 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:06:31.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:31.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:31.947 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:31.947 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:32.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:32.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:32.076 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:32.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:06:32.415 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:06:32.428 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:06:32.429 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:06:32.430 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:06:32.430 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:06:32.431 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:06:32.431 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
20:06:32.445 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
20:06:32.465 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
20:06:32.467 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:06:32.467 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:06:32.468 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:06:32.469 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:06:32.469 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:06:32.470 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
20:06:32.470 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
20:06:32.471 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:06:32.471 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:06:32.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.509 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.532 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:32.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:32.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:32.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:32.843 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:32.843 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:32.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:32.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:32.890 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.891 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:32.892 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:32.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:06:32.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:06:32.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:06:32.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:06:33.020 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190921)
20:06:33.023 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
20:06:33.024 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:06:33.035 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
20:06:33.039 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:06:33.436 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 256.551709 ms
20:06:33.687 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 41.351934 ms
20:06:33.782 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
20:06:33.889 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
20:06:33.894 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:14534 (size: 26.3 KB, free: 1988.7 MB)
20:06:33.897 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw01ReleaseCustomer.scala:59
20:06:33.906 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:06:34.002 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:59
20:06:34.021 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at Dw01ReleaseCustomer.scala:59)
20:06:34.023 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw01ReleaseCustomer.scala:59) with 1 output partitions
20:06:34.024 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw01ReleaseCustomer.scala:59)
20:06:34.024 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:06:34.026 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:06:34.031 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:59), which has no missing parents
20:06:34.069 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
20:06:34.073 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
20:06:34.074 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:14534 (size: 2.2 KB, free: 1988.7 MB)
20:06:34.074 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
20:06:34.085 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0))
20:06:34.085 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
20:06:34.124 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:06:34.135 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
20:06:34.210 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:34.212 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
20:06:34.229 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
20:06:34.237 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 123 ms on localhost (executor driver) (1/1)
20:06:34.240 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
20:06:34.245 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw01ReleaseCustomer.scala:59) finished in 0.141 s
20:06:34.249 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw01ReleaseCustomer.scala:59, took 0.246110 s
20:06:34.256 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:59
20:06:34.257 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw01ReleaseCustomer.scala:59) with 3 output partitions
20:06:34.257 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw01ReleaseCustomer.scala:59)
20:06:34.257 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
20:06:34.258 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:06:34.258 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:59), which has no missing parents
20:06:34.260 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
20:06:34.263 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
20:06:34.265 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:14534 (size: 2.2 KB, free: 1988.7 MB)
20:06:34.266 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:06:34.267 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(1, 2, 3))
20:06:34.267 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
20:06:34.268 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:06:34.269 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:06:34.269 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:06:34.269 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
20:06:34.270 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
20:06:34.270 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
20:06:34.275 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:34.275 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:34.275 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:34.276 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:34.276 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:34.276 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:34.277 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
20:06:34.277 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
20:06:34.277 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
20:06:34.279 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 11 ms on localhost (executor driver) (1/3)
20:06:34.280 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 12 ms on localhost (executor driver) (2/3)
20:06:34.281 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 12 ms on localhost (executor driver) (3/3)
20:06:34.282 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
20:06:34.282 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw01ReleaseCustomer.scala:59) finished in 0.015 s
20:06:34.283 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw01ReleaseCustomer.scala:59, took 0.026882 s
20:06:34.304 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
20:06:34.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:34.311 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:34.336 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.336 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.336 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.337 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:06:34.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.338 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.339 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.339 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:34.464 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
20:06:34.468 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-34_466_5709754747732333388-1
20:06:34.479 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:14534 in memory (size: 2.2 KB, free: 1988.7 MB)
20:06:34.482 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
20:06:34.482 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
20:06:34.482 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
20:06:34.485 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
20:06:34.485 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
20:06:34.487 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:14534 in memory (size: 2.2 KB, free: 1988.7 MB)
20:06:34.487 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
20:06:34.487 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:06:34.487 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
20:06:34.487 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
20:06:34.488 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:14534 in memory (size: 26.3 KB, free: 1988.7 MB)
20:06:34.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:34.654 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:34.659 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:34.659 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:34.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:34.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:34.706 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.706 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.706 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.706 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.706 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.707 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.707 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.707 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.707 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:34.707 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:34.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:06:34.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:06:34.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:06:34.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:06:34.734 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190921)
20:06:34.735 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
20:06:34.735 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:06:34.735 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
20:06:34.736 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:06:34.754 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:34.755 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:34.843 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.714089 ms
20:06:34.852 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
20:06:34.931 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
20:06:34.932 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:14534 (size: 26.3 KB, free: 1988.7 MB)
20:06:34.933 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
20:06:34.933 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:06:35.021 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:06:35.021 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:35)
20:06:35.022 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:06:35.022 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
20:06:35.022 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
20:06:35.022 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:06:35.023 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35), which has no missing parents
20:06:35.114 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 180.1 KB, free 1988.2 MB)
20:06:35.117 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 66.0 KB, free 1988.1 MB)
20:06:35.118 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:14534 (size: 66.0 KB, free: 1988.6 MB)
20:06:35.119 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
20:06:35.120 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:06:35.120 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
20:06:35.121 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:06:35.121 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:06:35.121 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:06:35.122 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:06:35.122 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
20:06:35.122 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
20:06:35.122 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
20:06:35.125 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
20:06:35.183 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:35.183 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:35.184 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:35.185 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:35.186 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:35.186 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:35.188 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:06:35.188 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:35.212 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.095915 ms
20:06:35.239 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.549837 ms
20:06:35.260 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:35.260 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:35.260 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:35.260 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:35.262 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:35.262 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:35.262 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:35.262 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:35.381 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
20:06:35.387 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.101932 ms
20:06:35.434 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.993499 ms
20:06:35.453 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.183025 ms
20:06:35.463 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200635_0005_m_000002_0
20:06:35.463 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200635_0005_m_000000_0
20:06:35.463 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200635_0005_m_000003_0
20:06:35.463 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200635_0005_m_000001_0
20:06:35.467 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2631 bytes result sent to driver
20:06:35.467 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2631 bytes result sent to driver
20:06:35.467 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2631 bytes result sent to driver
20:06:35.468 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2631 bytes result sent to driver
20:06:35.469 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 348 ms on localhost (executor driver) (1/4)
20:06:35.469 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 348 ms on localhost (executor driver) (2/4)
20:06:35.470 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 349 ms on localhost (executor driver) (3/4)
20:06:35.470 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 349 ms on localhost (executor driver) (4/4)
20:06:35.470 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
20:06:35.472 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.352 s
20:06:35.472 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.451204 s
20:06:35.545 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:06:35.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:35.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:35.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:35.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:35.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:06:35.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:35.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:35.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:35.619 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
20:06:35.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:35.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:35.665 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
20:06:35.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:06:35.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:06:35.675 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:35.675 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:35.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:35.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:35.741 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.744 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.744 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.744 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:35.775 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:06:35.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:35.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:35.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:35.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:35.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:35.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:35.828 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.829 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.829 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.829 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.829 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.830 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.830 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.830 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.830 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:35.830 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:35.841 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:06:35.842 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:06:35.842 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:06:35.842 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:06:35.842 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:06:35.842 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:06:35.843 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
20:06:35.843 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
20:06:35.844 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
20:06:35.845 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:06:35.846 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:06:35.846 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:06:35.847 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:06:35.848 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:06:35.848 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
20:06:35.848 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
20:06:35.849 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:06:35.849 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:06:35.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.850 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.856 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.871 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.877 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.889 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.890 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.895 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.903 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.903 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.909 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:35.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:35.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:36.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:36.004 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:36.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:36.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:36.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:36.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:36.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:36.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:36.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:06:36.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:06:36.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:06:36.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:06:36.153 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#212),(bdp_day#212 = 20190922)
20:06:36.154 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#205),(release_status#205 = 01)
20:06:36.154 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:06:36.155 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
20:06:36.156 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:06:36.185 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 312.0 KB, free 1987.8 MB)
20:06:36.206 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.8 MB)
20:06:36.208 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:14534 (size: 26.3 KB, free: 1988.6 MB)
20:06:36.208 INFO  [main] org.apache.spark.SparkContext - Created broadcast 5 from show at Dw01ReleaseCustomer.scala:59
20:06:36.209 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:06:36.236 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:59
20:06:36.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 20 (show at Dw01ReleaseCustomer.scala:59)
20:06:36.237 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw01ReleaseCustomer.scala:59) with 1 output partitions
20:06:36.237 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (show at Dw01ReleaseCustomer.scala:59)
20:06:36.237 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
20:06:36.237 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
20:06:36.238 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[20] at show at Dw01ReleaseCustomer.scala:59), which has no missing parents
20:06:36.259 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 24.0 KB, free 1987.8 MB)
20:06:36.261 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1987.8 MB)
20:06:36.262 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:14534 (size: 10.5 KB, free: 1988.6 MB)
20:06:36.263 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
20:06:36.265 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[20] at show at Dw01ReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:06:36.265 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 8 tasks
20:06:36.270 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:06:36.271 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 9, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:06:36.271 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 10, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:06:36.271 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 11, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:06:36.271 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 6.0 (TID 12, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:06:36.272 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 6.0 (TID 13, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:06:36.272 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 6.0 (TID 14, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:06:36.272 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 6.0 (TID 15, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:06:36.272 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 9)
20:06:36.272 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 8)
20:06:36.272 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 11)
20:06:36.272 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 10)
20:06:36.273 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 4.0 in stage 6.0 (TID 12)
20:06:36.274 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 5.0 in stage 6.0 (TID 13)
20:06:36.274 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 6.0 in stage 6.0 (TID 14)
20:06:36.274 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 7.0 in stage 6.0 (TID 15)
20:06:36.331 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.805749 ms
20:06:36.336 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
20:06:36.336 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
20:06:36.336 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
20:06:36.336 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
20:06:36.336 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
20:06:36.336 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
20:06:36.336 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
20:06:36.336 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
20:06:37.520 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.520 INFO  [Executor task launch worker for task 8] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.521 INFO  [Executor task launch worker for task 15] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.521 INFO  [Executor task launch worker for task 14] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.521 INFO  [Executor task launch worker for task 12] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.521 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.532 INFO  [Executor task launch worker for task 13] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.551 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:37.601 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 9). 1567 bytes result sent to driver
20:06:37.601 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 10). 1610 bytes result sent to driver
20:06:37.605 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 9) in 1334 ms on localhost (executor driver) (1/8)
20:06:37.605 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 10) in 1334 ms on localhost (executor driver) (2/8)
20:06:37.606 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 7.0 in stage 6.0 (TID 15). 1567 bytes result sent to driver
20:06:37.609 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 6.0 (TID 15) in 1337 ms on localhost (executor driver) (3/8)
20:06:37.612 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 8). 1524 bytes result sent to driver
20:06:37.613 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 8) in 1345 ms on localhost (executor driver) (4/8)
20:06:37.616 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 5.0 in stage 6.0 (TID 13). 1524 bytes result sent to driver
20:06:37.617 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 6.0 (TID 13) in 1345 ms on localhost (executor driver) (5/8)
20:06:37.623 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 6.0 in stage 6.0 (TID 14). 1567 bytes result sent to driver
20:06:37.623 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 6.0 (TID 14) in 1351 ms on localhost (executor driver) (6/8)
20:06:37.626 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 4.0 in stage 6.0 (TID 12). 1524 bytes result sent to driver
20:06:37.626 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 6.0 (TID 12) in 1355 ms on localhost (executor driver) (7/8)
20:06:38.424 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 82
20:06:38.424 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 83
20:06:38.424 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 93
20:06:38.424 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 88
20:06:38.425 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 92
20:06:38.425 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 90
20:06:38.427 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.32.1:14534 in memory (size: 26.3 KB, free: 1988.6 MB)
20:06:38.427 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 118
20:06:38.428 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 91
20:06:38.428 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 87
20:06:38.428 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 86
20:06:38.428 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 89
20:06:38.429 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:14534 in memory (size: 66.0 KB, free: 1988.7 MB)
20:06:38.430 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 84
20:06:38.430 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 1
20:06:38.430 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 85
20:06:40.667 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 11). 1739 bytes result sent to driver
20:06:40.668 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 11) in 4397 ms on localhost (executor driver) (8/8)
20:06:40.669 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
20:06:40.669 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (show at Dw01ReleaseCustomer.scala:59) finished in 4.401 s
20:06:40.670 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:06:40.670 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:06:40.670 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 7)
20:06:40.671 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:06:40.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[22] at show at Dw01ReleaseCustomer.scala:59), which has no missing parents
20:06:40.676 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
20:06:40.678 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
20:06:40.679 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:14534 (size: 2.2 KB, free: 1988.7 MB)
20:06:40.680 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
20:06:40.680 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at show at Dw01ReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0))
20:06:40.680 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
20:06:40.681 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 16, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:06:40.681 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 16)
20:06:40.686 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:40.686 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:40.712 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 16). 11772 bytes result sent to driver
20:06:40.712 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 16) in 31 ms on localhost (executor driver) (1/1)
20:06:40.713 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
20:06:40.713 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (show at Dw01ReleaseCustomer.scala:59) finished in 0.032 s
20:06:40.713 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw01ReleaseCustomer.scala:59, took 4.477415 s
20:06:40.739 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
20:06:40.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:40.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:40.766 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.766 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:40.775 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1
20:06:40.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:40.822 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:40.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:40.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:40.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:40.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:40.870 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.870 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:40.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:40.872 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:06:40.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:06:40.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:06:40.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:06:40.908 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#212),(bdp_day#212 = 20190922)
20:06:40.908 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#205),(release_status#205 = 01)
20:06:40.908 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:06:40.908 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
20:06:40.909 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:06:40.923 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:40.923 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:40.941 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
20:06:40.963 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
20:06:40.964 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:14534 (size: 26.3 KB, free: 1988.6 MB)
20:06:40.965 INFO  [main] org.apache.spark.SparkContext - Created broadcast 8 from insertInto at SparkHelper.scala:35
20:06:40.965 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:06:41.017 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:06:41.018 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 27 (insertInto at SparkHelper.scala:35)
20:06:41.019 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:06:41.019 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (insertInto at SparkHelper.scala:35)
20:06:41.019 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
20:06:41.019 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 8)
20:06:41.019 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 8 (MapPartitionsRDD[27] at insertInto at SparkHelper.scala:35), which has no missing parents
20:06:41.024 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 24.0 KB, free 1988.0 MB)
20:06:41.025 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1988.0 MB)
20:06:41.026 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:14534 (size: 10.5 KB, free: 1988.6 MB)
20:06:41.026 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
20:06:41.027 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[27] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:06:41.027 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 8 tasks
20:06:41.027 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 17, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:06:41.028 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 18, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:06:41.028 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 8.0 (TID 19, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:06:41.028 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 8.0 (TID 20, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:06:41.028 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 8.0 (TID 21, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:06:41.028 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 8.0 (TID 22, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:06:41.029 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 8.0 (TID 23, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:06:41.029 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 8.0 (TID 24, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:06:41.029 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 18)
20:06:41.029 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 3.0 in stage 8.0 (TID 20)
20:06:41.029 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 17)
20:06:41.029 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 4.0 in stage 8.0 (TID 21)
20:06:41.029 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 2.0 in stage 8.0 (TID 19)
20:06:41.029 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 7.0 in stage 8.0 (TID 24)
20:06:41.029 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 5.0 in stage 8.0 (TID 22)
20:06:41.029 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 6.0 in stage 8.0 (TID 23)
20:06:41.036 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
20:06:41.038 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
20:06:41.038 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
20:06:41.038 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
20:06:41.040 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
20:06:41.042 INFO  [Executor task launch worker for task 24] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
20:06:41.043 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
20:06:41.048 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
20:06:41.059 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.078 INFO  [Executor task launch worker for task 24] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.088 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.089 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.089 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.089 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.090 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.091 INFO  [Executor task launch worker for task 17] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:41.105 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 7.0 in stage 8.0 (TID 24). 1481 bytes result sent to driver
20:06:41.106 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 8.0 (TID 24) in 76 ms on localhost (executor driver) (1/8)
20:06:41.120 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 2.0 in stage 8.0 (TID 19). 1524 bytes result sent to driver
20:06:41.125 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 8.0 (TID 19) in 97 ms on localhost (executor driver) (2/8)
20:06:41.125 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 6.0 in stage 8.0 (TID 23). 1524 bytes result sent to driver
20:06:41.129 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 5.0 in stage 8.0 (TID 22). 1524 bytes result sent to driver
20:06:41.132 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 8.0 (TID 23) in 104 ms on localhost (executor driver) (3/8)
20:06:41.133 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 8.0 (TID 22) in 105 ms on localhost (executor driver) (4/8)
20:06:41.137 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 17). 1524 bytes result sent to driver
20:06:41.147 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 17) in 120 ms on localhost (executor driver) (5/8)
20:06:41.151 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 4.0 in stage 8.0 (TID 21). 1524 bytes result sent to driver
20:06:41.153 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 1.0 in stage 8.0 (TID 18). 1481 bytes result sent to driver
20:06:41.154 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 8.0 (TID 21) in 126 ms on localhost (executor driver) (6/8)
20:06:41.155 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 8.0 (TID 18) in 127 ms on localhost (executor driver) (7/8)
20:06:41.704 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:14534 in memory (size: 2.2 KB, free: 1988.6 MB)
20:06:41.704 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 175
20:06:43.213 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 3.0 in stage 8.0 (TID 20). 1739 bytes result sent to driver
20:06:43.214 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 8.0 (TID 20) in 2186 ms on localhost (executor driver) (8/8)
20:06:43.214 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
20:06:43.215 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 8 (insertInto at SparkHelper.scala:35) finished in 2.187 s
20:06:43.215 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:06:43.215 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:06:43.215 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 9)
20:06:43.215 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:06:43.215 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35), which has no missing parents
20:06:43.246 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 180.2 KB, free 1987.8 MB)
20:06:43.248 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 66.1 KB, free 1987.7 MB)
20:06:43.250 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:14534 (size: 66.1 KB, free: 1988.6 MB)
20:06:43.250 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1004
20:06:43.251 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:06:43.251 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 4 tasks
20:06:43.252 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 25, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:06:43.252 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 26, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:06:43.252 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 27, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:06:43.252 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 28, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:06:43.253 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 25)
20:06:43.253 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 26)
20:06:43.253 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 27)
20:06:43.253 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 28)
20:06:43.278 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:43.278 INFO  [Executor task launch worker for task 25] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:43.278 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:43.278 INFO  [Executor task launch worker for task 27] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:43.283 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:43.283 INFO  [Executor task launch worker for task 28] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:43.285 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:43.286 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:43.435 INFO  [Executor task launch worker for task 25] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:43.436 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:43.437 INFO  [Executor task launch worker for task 27] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:43.437 INFO  [Executor task launch worker for task 26] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:43.438 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:43.438 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:43.439 INFO  [Executor task launch worker for task 28] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:43.440 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:43.563 INFO  [Executor task launch worker for task 28] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4874bdf3
20:06:43.564 INFO  [Executor task launch worker for task 26] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@60bfd5f0
20:06:43.564 INFO  [Executor task launch worker for task 25] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d2e2580
20:06:43.564 INFO  [Executor task launch worker for task 27] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2100f975
20:06:43.577 INFO  [Executor task launch worker for task 28] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
20:06:43.584 INFO  [Executor task launch worker for task 25] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:43.584 INFO  [Executor task launch worker for task 27] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:43.584 INFO  [Executor task launch worker for task 26] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:43.584 INFO  [Executor task launch worker for task 28] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:43.584 INFO  [Executor task launch worker for task 25] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200643_0009_m_000000_0/bdp_day=20190922/part-00000-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
20:06:43.585 INFO  [Executor task launch worker for task 27] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200643_0009_m_000002_0/bdp_day=20190922/part-00002-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
20:06:43.585 INFO  [Executor task launch worker for task 26] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200643_0009_m_000001_0/bdp_day=20190922/part-00001-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
20:06:43.585 INFO  [Executor task launch worker for task 28] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200643_0009_m_000003_0/bdp_day=20190922/part-00003-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
20:06:43.587 INFO  [Executor task launch worker for task 26] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:43.587 INFO  [Executor task launch worker for task 27] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:43.587 INFO  [Executor task launch worker for task 28] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:43.587 INFO  [Executor task launch worker for task 25] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:43.587 INFO  [Executor task launch worker for task 26] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:43.587 INFO  [Executor task launch worker for task 27] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:43.587 INFO  [Executor task launch worker for task 28] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:43.587 INFO  [Executor task launch worker for task 25] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:43.588 INFO  [Executor task launch worker for task 26] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:43.589 INFO  [Executor task launch worker for task 25] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:43.589 INFO  [Executor task launch worker for task 27] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:43.589 INFO  [Executor task launch worker for task 28] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:43.589 INFO  [Executor task launch worker for task 26] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 25] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 27] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 28] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 26] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 25] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 27] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 28] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:43.589 INFO  [Executor task launch worker for task 26] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:43.589 INFO  [Executor task launch worker for task 25] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:43.589 INFO  [Executor task launch worker for task 27] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:43.589 INFO  [Executor task launch worker for task 28] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:43.589 INFO  [Executor task launch worker for task 26] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:43.589 INFO  [Executor task launch worker for task 25] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:43.589 INFO  [Executor task launch worker for task 27] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:43.589 INFO  [Executor task launch worker for task 28] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:43.589 INFO  [Executor task launch worker for task 26] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:43.589 INFO  [Executor task launch worker for task 25] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:43.589 INFO  [Executor task launch worker for task 28] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:43.589 INFO  [Executor task launch worker for task 27] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:43.784 INFO  [Executor task launch worker for task 28] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@75fbfba6
20:06:43.784 INFO  [Executor task launch worker for task 27] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2d54071a
20:06:43.784 INFO  [Executor task launch worker for task 26] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2a33d82
20:06:43.785 INFO  [Executor task launch worker for task 25] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6cfa121c
20:06:44.193 INFO  [Executor task launch worker for task 27] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,987
20:06:44.202 INFO  [Executor task launch worker for task 26] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,847
20:06:44.205 INFO  [Executor task launch worker for task 28] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,892
20:06:44.214 INFO  [Executor task launch worker for task 25] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,753
20:06:44.425 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.425 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.425 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.427 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.427 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.427 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.427 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.427 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.428 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.428 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.428 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.428 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:44.429 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.429 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:44.429 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,447 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:44.430 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:44.430 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:44.430 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:44.430 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:44.430 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.430 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:44.431 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.431 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.434 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.434 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:44.434 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.435 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.435 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:44.436 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:44.437 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:44.437 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 2,780B for [gender] BINARY: 21,446 values, 2,749B raw, 2,749B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:44.438 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:44.438 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:44.438 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 2,792B for [gender] BINARY: 21,446 values, 2,761B raw, 2,761B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:44.439 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 3,223B for [area_code] BINARY: 21,446 values, 3,182B raw, 3,182B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:44.439 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 2,795B for [gender] BINARY: 21,446 values, 2,764B raw, 2,764B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:44.439 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 2,804B for [gender] BINARY: 21,447 values, 2,773B raw, 2,773B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:44.440 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 3,206B for [area_code] BINARY: 21,446 values, 3,165B raw, 3,165B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:44.441 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:44.441 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 3,253B for [area_code] BINARY: 21,446 values, 3,212B raw, 3,212B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:44.442 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 3,205B for [area_code] BINARY: 21,447 values, 3,164B raw, 3,164B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:44.442 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:44.443 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:44.443 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:44.443 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:44.443 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:44.444 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:44.444 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.445 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.445 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.445 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:44.445 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:44.445 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:44.445 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:44.446 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.446 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.446 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.446 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.446 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:44.446 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:44.447 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:44.447 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:44.447 INFO  [Executor task launch worker for task 27] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:06:44.447 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:44.448 INFO  [Executor task launch worker for task 26] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:06:44.453 INFO  [Executor task launch worker for task 25] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:06:44.454 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:44.455 INFO  [Executor task launch worker for task 28] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:06:44.698 INFO  [Executor task launch worker for task 28] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200643_0009_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/task_20190925200643_0009_m_000003
20:06:44.698 INFO  [Executor task launch worker for task 27] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200643_0009_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/task_20190925200643_0009_m_000002
20:06:44.698 INFO  [Executor task launch worker for task 25] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200643_0009_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/task_20190925200643_0009_m_000000
20:06:44.699 INFO  [Executor task launch worker for task 26] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200643_0009_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/_temporary/0/task_20190925200643_0009_m_000001
20:06:44.699 INFO  [Executor task launch worker for task 28] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200643_0009_m_000003_0: Committed
20:06:44.699 INFO  [Executor task launch worker for task 25] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200643_0009_m_000000_0: Committed
20:06:44.699 INFO  [Executor task launch worker for task 27] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200643_0009_m_000002_0: Committed
20:06:44.699 INFO  [Executor task launch worker for task 26] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200643_0009_m_000001_0: Committed
20:06:44.701 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 27). 2658 bytes result sent to driver
20:06:44.701 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 25). 2658 bytes result sent to driver
20:06:44.701 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 28). 2658 bytes result sent to driver
20:06:44.701 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 26). 2658 bytes result sent to driver
20:06:44.702 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 27) in 1450 ms on localhost (executor driver) (1/4)
20:06:44.702 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 28) in 1450 ms on localhost (executor driver) (2/4)
20:06:44.702 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 25) in 1451 ms on localhost (executor driver) (3/4)
20:06:44.702 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 26) in 1450 ms on localhost (executor driver) (4/4)
20:06:44.702 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
20:06:44.703 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (insertInto at SparkHelper.scala:35) finished in 1.452 s
20:06:44.704 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: insertInto at SparkHelper.scala:35, took 3.685443 s
20:06:44.735 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:06:44.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:44.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:44.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:44.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:44.785 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:44.788 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:44.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:44.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:44.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:06:44.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:44.834 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:44.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]
20:06:44.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]	
20:06:44.946 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20:06:44.961 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/bdp_day=20190922/part-00000-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00000-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, Status:true
20:06:44.991 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/bdp_day=20190922/part-00001-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00001-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, Status:true
20:06:45.041 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/bdp_day=20190922/part-00002-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00002-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, Status:true
20:06:45.063 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/bdp_day=20190922/part-00003-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00003-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000, Status:true
20:06:45.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]
20:06:45.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]	
20:06:45.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_customer[20190922]
20:06:45.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_customer[20190922]	
20:06:45.167 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
20:06:45.171 WARN  [main] hive.log - Updated size to 5782454
20:06:45.500 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-40_775_5334959928628271429-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
20:06:45.511 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
20:06:45.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:06:45.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:06:45.560 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:45.560 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:45.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:45.622 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:45.667 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.667 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.668 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.669 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.669 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.669 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.669 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:45.688 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:06:45.688 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:45.688 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:45.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:45.709 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:45.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:45.777 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:45.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:45.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:45.836 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:06:45.837 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:06:45.837 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:06:45.837 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:06:45.837 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:06:45.838 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:06:45.838 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
20:06:45.838 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
20:06:45.839 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
20:06:45.839 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
20:06:45.840 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
20:06:45.840 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
20:06:45.840 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
20:06:45.841 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
20:06:45.841 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
20:06:45.841 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
20:06:45.841 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:06:45.841 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:06:45.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.860 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.860 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.904 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.904 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.910 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.926 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.927 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:45.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:06:45.945 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:06:46.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:46.007 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:46.013 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:46.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:46.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:46.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:46.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.060 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:46.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:46.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:06:46.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:06:46.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:06:46.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:06:46.093 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#405),(bdp_day#405 = 20190923)
20:06:46.094 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#398),(release_status#398 = 01)
20:06:46.094 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:06:46.094 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
20:06:46.094 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:06:46.110 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
20:06:46.128 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
20:06:46.129 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:14534 (size: 26.3 KB, free: 1988.5 MB)
20:06:46.130 INFO  [main] org.apache.spark.SparkContext - Created broadcast 11 from show at Dw01ReleaseCustomer.scala:59
20:06:46.131 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:06:46.144 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:59
20:06:46.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 36 (show at Dw01ReleaseCustomer.scala:59)
20:06:46.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (show at Dw01ReleaseCustomer.scala:59) with 1 output partitions
20:06:46.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (show at Dw01ReleaseCustomer.scala:59)
20:06:46.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
20:06:46.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 10)
20:06:46.145 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 10 (MapPartitionsRDD[36] at show at Dw01ReleaseCustomer.scala:59), which has no missing parents
20:06:46.156 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 24.0 KB, free 1987.4 MB)
20:06:46.159 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1987.4 MB)
20:06:46.160 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:14534 (size: 10.5 KB, free: 1988.5 MB)
20:06:46.160 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
20:06:46.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[36] at show at Dw01ReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:06:46.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 8 tasks
20:06:46.162 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 29, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:06:46.162 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 30, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:06:46.163 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 10.0 (TID 31, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:06:46.163 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 10.0 (TID 32, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:06:46.163 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 10.0 (TID 33, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:06:46.163 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 10.0 (TID 34, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:06:46.164 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 10.0 (TID 35, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:06:46.164 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 10.0 (TID 36, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:06:46.164 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 30)
20:06:46.164 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 29)
20:06:46.164 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 2.0 in stage 10.0 (TID 31)
20:06:46.164 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 4.0 in stage 10.0 (TID 33)
20:06:46.164 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 3.0 in stage 10.0 (TID 32)
20:06:46.164 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 5.0 in stage 10.0 (TID 34)
20:06:46.164 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 7.0 in stage 10.0 (TID 36)
20:06:46.164 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 6.0 in stage 10.0 (TID 35)
20:06:46.174 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:06:46.176 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:06:46.179 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:06:46.183 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:06:46.184 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:06:46.184 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:06:46.187 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:06:46.190 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:06:46.217 INFO  [Executor task launch worker for task 33] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.232 INFO  [Executor task launch worker for task 36] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.232 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.246 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.246 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.248 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.249 INFO  [Executor task launch worker for task 34] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.256 INFO  [Executor task launch worker for task 35] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:46.256 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 7.0 in stage 10.0 (TID 36). 1567 bytes result sent to driver
20:06:46.257 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 10.0 (TID 36) in 93 ms on localhost (executor driver) (1/8)
20:06:46.272 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 6.0 in stage 10.0 (TID 35). 1524 bytes result sent to driver
20:06:46.273 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 10.0 (TID 35) in 109 ms on localhost (executor driver) (2/8)
20:06:48.227 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 4.0 in stage 10.0 (TID 33). 1567 bytes result sent to driver
20:06:48.228 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 10.0 (TID 33) in 2065 ms on localhost (executor driver) (3/8)
20:06:48.230 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 2.0 in stage 10.0 (TID 31). 1524 bytes result sent to driver
20:06:48.231 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 10.0 (TID 31) in 2068 ms on localhost (executor driver) (4/8)
20:06:48.235 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 30). 1567 bytes result sent to driver
20:06:48.235 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 30) in 2073 ms on localhost (executor driver) (5/8)
20:06:48.238 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 29). 1524 bytes result sent to driver
20:06:48.239 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 29) in 2076 ms on localhost (executor driver) (6/8)
20:06:48.241 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 5.0 in stage 10.0 (TID 34). 1481 bytes result sent to driver
20:06:48.241 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 10.0 (TID 34) in 2078 ms on localhost (executor driver) (7/8)
20:06:49.048 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 124
20:06:49.048 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 125
20:06:49.049 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
20:06:49.049 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 236
20:06:49.049 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 180
20:06:49.050 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:14534 in memory (size: 10.5 KB, free: 1988.5 MB)
20:06:49.051 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 184
20:06:49.051 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 183
20:06:49.051 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 178
20:06:49.051 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 121
20:06:49.051 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 177
20:06:49.051 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 186
20:06:49.052 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:14534 in memory (size: 66.1 KB, free: 1988.6 MB)
20:06:49.053 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 185
20:06:49.053 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 181
20:06:49.054 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:14534 in memory (size: 26.3 KB, free: 1988.6 MB)
20:06:49.054 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 176
20:06:49.055 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 123
20:06:49.055 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 119
20:06:49.055 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 182
20:06:49.055 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 179
20:06:49.055 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 120
20:06:49.055 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 122
20:06:49.055 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 187
20:06:49.056 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:14534 in memory (size: 26.3 KB, free: 1988.7 MB)
20:06:49.057 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:14534 in memory (size: 10.5 KB, free: 1988.7 MB)
20:06:49.058 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
20:06:49.058 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 126
20:06:50.623 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 3.0 in stage 10.0 (TID 32). 1739 bytes result sent to driver
20:06:50.623 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 10.0 (TID 32) in 4460 ms on localhost (executor driver) (8/8)
20:06:50.623 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
20:06:50.624 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 10 (show at Dw01ReleaseCustomer.scala:59) finished in 4.462 s
20:06:50.624 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:06:50.624 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:06:50.624 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 11)
20:06:50.624 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:06:50.625 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[38] at show at Dw01ReleaseCustomer.scala:59), which has no missing parents
20:06:50.626 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
20:06:50.628 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
20:06:50.630 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:14534 (size: 2.2 KB, free: 1988.7 MB)
20:06:50.631 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1004
20:06:50.631 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[38] at show at Dw01ReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0))
20:06:50.631 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 1 tasks
20:06:50.632 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 37, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:06:50.632 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 37)
20:06:50.635 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:50.635 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:50.648 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 37). 11823 bytes result sent to driver
20:06:50.648 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 37) in 16 ms on localhost (executor driver) (1/1)
20:06:50.648 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
20:06:50.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (show at Dw01ReleaseCustomer.scala:59) finished in 0.016 s
20:06:50.649 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: show at Dw01ReleaseCustomer.scala:59, took 4.505276 s
20:06:50.667 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
20:06:50.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:06:50.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:50.701 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1
20:06:50.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:06:50.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:06:50.809 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:50.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:50.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:06:50.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:06:50.851 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.851 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.851 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.851 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.851 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.851 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.851 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.852 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.852 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:06:50.852 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:06:50.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:06:50.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:06:50.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:06:50.854 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:06:50.899 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#405),(bdp_day#405 = 20190923)
20:06:50.899 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#398),(release_status#398 = 01)
20:06:50.900 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:06:50.900 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
20:06:50.900 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:06:50.910 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:50.910 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:51.531 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
20:06:51.543 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
20:06:51.543 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:14534 (size: 26.3 KB, free: 1988.6 MB)
20:06:51.544 INFO  [main] org.apache.spark.SparkContext - Created broadcast 14 from insertInto at SparkHelper.scala:35
20:06:51.544 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:06:51.583 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:06:51.583 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 43 (insertInto at SparkHelper.scala:35)
20:06:51.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:06:51.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (insertInto at SparkHelper.scala:35)
20:06:51.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
20:06:51.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
20:06:51.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35), which has no missing parents
20:06:51.589 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 24.0 KB, free 1988.0 MB)
20:06:51.590 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1988.0 MB)
20:06:51.590 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:14534 (size: 10.5 KB, free: 1988.6 MB)
20:06:51.591 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1004
20:06:51.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:06:51.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 8 tasks
20:06:51.591 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 38, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:06:51.592 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 39, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:06:51.592 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 40, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:06:51.592 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 41, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:06:51.592 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 42, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:06:51.593 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 43, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:06:51.593 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 44, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:06:51.593 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 45, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:06:51.593 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 40)
20:06:51.593 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 39)
20:06:51.593 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 38)
20:06:51.593 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 41)
20:06:51.593 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 42)
20:06:51.593 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 45)
20:06:51.593 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 44)
20:06:51.593 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 43)
20:06:51.600 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:06:51.601 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:06:51.601 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:06:51.602 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:06:51.603 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:06:51.605 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:06:51.605 INFO  [Executor task launch worker for task 45] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:06:51.606 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:06:51.630 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.638 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.639 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.640 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.645 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.649 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.651 INFO  [Executor task launch worker for task 45] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.651 INFO  [Executor task launch worker for task 39] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
20:06:51.652 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 38). 1481 bytes result sent to driver
20:06:51.654 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 38) in 63 ms on localhost (executor driver) (1/8)
20:06:52.062 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 293
20:06:52.063 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.32.1:14534 in memory (size: 2.2 KB, free: 1988.6 MB)
20:06:52.198 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 44). 1567 bytes result sent to driver
20:06:52.200 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 44) in 607 ms on localhost (executor driver) (2/8)
20:06:52.203 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 40). 1567 bytes result sent to driver
20:06:52.203 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 40) in 611 ms on localhost (executor driver) (3/8)
20:06:52.207 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 42). 1524 bytes result sent to driver
20:06:52.207 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 42) in 615 ms on localhost (executor driver) (4/8)
20:06:52.210 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 43). 1567 bytes result sent to driver
20:06:52.211 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 43) in 619 ms on localhost (executor driver) (5/8)
20:06:52.214 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 39). 1524 bytes result sent to driver
20:06:52.214 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 39) in 622 ms on localhost (executor driver) (6/8)
20:06:52.217 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 45). 1567 bytes result sent to driver
20:06:52.217 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 45) in 624 ms on localhost (executor driver) (7/8)
20:06:52.486 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 242
20:06:52.487 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
20:06:52.488 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:14534 in memory (size: 26.3 KB, free: 1988.7 MB)
20:06:52.489 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:14534 in memory (size: 10.5 KB, free: 1988.7 MB)
20:06:52.490 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 241
20:06:52.490 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 239
20:06:52.490 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 237
20:06:52.490 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 240
20:06:52.490 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 238
20:06:52.490 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 244
20:06:52.490 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 243
20:06:54.717 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 41). 1739 bytes result sent to driver
20:06:54.718 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 41) in 3126 ms on localhost (executor driver) (8/8)
20:06:54.718 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
20:06:54.718 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (insertInto at SparkHelper.scala:35) finished in 3.127 s
20:06:54.718 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:06:54.718 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:06:54.718 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
20:06:54.719 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:06:54.719 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35), which has no missing parents
20:06:54.757 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
20:06:54.759 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 66.1 KB, free 1988.1 MB)
20:06:54.759 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:14534 (size: 66.1 KB, free: 1988.6 MB)
20:06:54.760 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1004
20:06:54.760 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 13 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:06:54.760 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 4 tasks
20:06:54.761 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 46, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:06:54.761 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 47, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:06:54.761 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 13.0 (TID 48, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:06:54.761 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 13.0 (TID 49, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:06:54.761 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 46)
20:06:54.761 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 13.0 (TID 48)
20:06:54.761 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 47)
20:06:54.761 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 13.0 (TID 49)
20:06:54.783 INFO  [Executor task launch worker for task 47] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:54.783 INFO  [Executor task launch worker for task 47] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:54.784 INFO  [Executor task launch worker for task 46] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:54.784 INFO  [Executor task launch worker for task 46] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:54.785 INFO  [Executor task launch worker for task 48] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:54.785 INFO  [Executor task launch worker for task 48] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:06:54.793 INFO  [Executor task launch worker for task 49] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:06:54.793 INFO  [Executor task launch worker for task 49] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:06:54.841 INFO  [Executor task launch worker for task 46] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:54.841 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:54.844 INFO  [Executor task launch worker for task 47] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:54.844 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:54.848 INFO  [Executor task launch worker for task 46] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c4e3df5
20:06:54.848 INFO  [Executor task launch worker for task 47] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34b59543
20:06:54.849 INFO  [Executor task launch worker for task 47] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:54.849 INFO  [Executor task launch worker for task 46] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:54.849 INFO  [Executor task launch worker for task 47] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200654_0013_m_000001_0/bdp_day=20190923/part-00001-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
20:06:54.849 INFO  [Executor task launch worker for task 46] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200654_0013_m_000000_0/bdp_day=20190923/part-00000-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:54.849 INFO  [Executor task launch worker for task 47] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:54.849 INFO  [Executor task launch worker for task 46] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:54.854 INFO  [Executor task launch worker for task 48] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:54.855 INFO  [Executor task launch worker for task 49] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:06:54.855 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:54.855 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:06:54.857 INFO  [Executor task launch worker for task 49] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@252efec5
20:06:54.857 INFO  [Executor task launch worker for task 48] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12f7c261
20:06:54.857 INFO  [Executor task launch worker for task 48] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:54.857 INFO  [Executor task launch worker for task 49] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:06:54.857 INFO  [Executor task launch worker for task 48] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200654_0013_m_000002_0/bdp_day=20190923/part-00002-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
20:06:54.857 INFO  [Executor task launch worker for task 49] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200654_0013_m_000003_0/bdp_day=20190923/part-00003-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
20:06:54.857 INFO  [Executor task launch worker for task 48] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:54.857 INFO  [Executor task launch worker for task 49] parquet.hadoop.codec.CodecConfig - Compression set to false
20:06:54.857 INFO  [Executor task launch worker for task 48] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:54.857 INFO  [Executor task launch worker for task 49] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:06:54.857 INFO  [Executor task launch worker for task 48] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:54.857 INFO  [Executor task launch worker for task 49] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:06:54.857 INFO  [Executor task launch worker for task 48] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:54.857 INFO  [Executor task launch worker for task 49] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:06:54.857 INFO  [Executor task launch worker for task 49] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:54.857 INFO  [Executor task launch worker for task 48] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:06:54.857 INFO  [Executor task launch worker for task 49] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:54.858 INFO  [Executor task launch worker for task 49] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:54.858 INFO  [Executor task launch worker for task 48] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:06:54.858 INFO  [Executor task launch worker for task 48] parquet.hadoop.ParquetOutputFormat - Validation is off
20:06:54.858 INFO  [Executor task launch worker for task 49] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:54.858 INFO  [Executor task launch worker for task 48] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:06:55.802 INFO  [Executor task launch worker for task 46] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@4c8251d6
20:06:56.016 INFO  [Executor task launch worker for task 46] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,987
20:06:56.036 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.036 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.036 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.037 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,446 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:56.037 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:56.037 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.038 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.038 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:56.038 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 2,800B for [gender] BINARY: 21,446 values, 2,769B raw, 2,769B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:56.038 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 3,224B for [area_code] BINARY: 21,446 values, 3,183B raw, 3,183B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:56.039 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:56.039 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:56.039 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.039 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:56.039 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.040 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:56.040 INFO  [Executor task launch worker for task 46] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:06:56.448 INFO  [Executor task launch worker for task 47] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@51df800b
20:06:56.448 INFO  [Executor task launch worker for task 49] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@59d27e30
20:06:56.448 INFO  [Executor task launch worker for task 48] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@42b2e89c
20:06:56.611 INFO  [Executor task launch worker for task 49] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,668
20:06:56.614 INFO  [Executor task launch worker for task 47] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,857
20:06:56.617 INFO  [Executor task launch worker for task 48] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,947
20:06:56.631 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.632 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.632 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.632 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,447 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:56.632 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:56.632 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.633 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.633 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:56.633 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 2,782B for [gender] BINARY: 21,447 values, 2,751B raw, 2,751B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:56.634 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 3,225B for [area_code] BINARY: 21,447 values, 3,184B raw, 3,184B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:56.634 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:56.634 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:56.634 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.634 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:56.637 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.637 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.637 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.637 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:56.637 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.637 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,446 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:56.637 INFO  [Executor task launch worker for task 49] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:06:56.638 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:56.638 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.638 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.639 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:56.639 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 2,801B for [gender] BINARY: 21,446 values, 2,770B raw, 2,770B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:56.639 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 3,233B for [area_code] BINARY: 21,446 values, 3,192B raw, 3,192B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:56.639 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:56.640 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:56.640 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.641 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:56.641 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.641 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:56.641 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.641 INFO  [Executor task launch worker for task 47] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:06:56.641 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.642 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.642 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,446 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:06:56.642 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:06:56.642 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:06:56.643 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
20:06:56.643 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
20:06:56.643 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 2,796B for [gender] BINARY: 21,446 values, 2,765B raw, 2,765B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
20:06:56.643 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 3,212B for [area_code] BINARY: 21,446 values, 3,171B raw, 3,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
20:06:56.643 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
20:06:56.644 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
20:06:56.644 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.644 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
20:06:56.644 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
20:06:56.644 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
20:06:56.644 INFO  [Executor task launch worker for task 48] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:07:02.240 INFO  [Executor task launch worker for task 46] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200654_0013_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/task_20190925200654_0013_m_000000
20:07:02.240 INFO  [Executor task launch worker for task 46] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200654_0013_m_000000_0: Committed
20:07:02.241 INFO  [Executor task launch worker for task 49] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200654_0013_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/task_20190925200654_0013_m_000003
20:07:02.241 INFO  [Executor task launch worker for task 49] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200654_0013_m_000003_0: Committed
20:07:02.241 INFO  [Executor task launch worker for task 47] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200654_0013_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/task_20190925200654_0013_m_000001
20:07:02.241 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 46). 2572 bytes result sent to driver
20:07:02.241 INFO  [Executor task launch worker for task 47] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200654_0013_m_000001_0: Committed
20:07:02.241 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 13.0 (TID 49). 2572 bytes result sent to driver
20:07:02.242 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 13.0 (TID 47). 2572 bytes result sent to driver
20:07:02.242 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 46) in 7482 ms on localhost (executor driver) (1/4)
20:07:02.242 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 13.0 (TID 49) in 7481 ms on localhost (executor driver) (2/4)
20:07:02.242 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 13.0 (TID 47) in 7481 ms on localhost (executor driver) (3/4)
20:07:02.504 INFO  [Executor task launch worker for task 48] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200654_0013_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/_temporary/0/task_20190925200654_0013_m_000002
20:07:02.504 INFO  [Executor task launch worker for task 48] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200654_0013_m_000002_0: Committed
20:07:02.505 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 13.0 (TID 48). 2615 bytes result sent to driver
20:07:02.506 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 13.0 (TID 48) in 7745 ms on localhost (executor driver) (4/4)
20:07:02.507 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
20:07:02.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (insertInto at SparkHelper.scala:35) finished in 7.747 s
20:07:02.507 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: insertInto at SparkHelper.scala:35, took 10.924427 s
20:07:02.589 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:07:02.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:07:02.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:07:02.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:07:02.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:07:02.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.659 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.660 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:02.660 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:07:02.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:07:02.661 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:07:02.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:07:02.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:07:02.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:07:02.700 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]
20:07:02.700 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]	
20:07:02.731 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/bdp_day=20190923/part-00000-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00000-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, Status:true
20:07:02.748 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/bdp_day=20190923/part-00001-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00001-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, Status:true
20:07:02.759 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/bdp_day=20190923/part-00002-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00002-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, Status:true
20:07:02.771 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/bdp_day=20190923/part-00003-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00003-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000, Status:true
20:07:02.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]
20:07:02.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]	
20:07:02.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_customer[20190923]
20:07:02.784 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_customer[20190923]	
20:07:02.804 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
20:07:02.806 WARN  [main] hive.log - Updated size to 5782475
20:07:03.602 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_20-06-50_700_3721206540099192871-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
20:07:03.604 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
20:07:03.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:07:03.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:07:03.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:07:03.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:07:03.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
20:07:03.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:07:03.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:07:03.663 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
20:07:03.783 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:07:03.814 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
20:07:04.419 INFO  [dispatcher-event-loop-5] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:07:04.561 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:07:04.562 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
20:07:04.562 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:07:04.564 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:07:04.570 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:07:04.570 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:07:04.571 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-68c14b8d-6531-489d-ab10-24077d843746
20:08:02.451 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:08:02.787 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
20:08:02.805 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
20:08:02.806 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
20:08:02.806 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:08:02.807 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:08:02.807 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
20:08:03.638 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 14734.
20:08:03.656 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:08:03.672 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:08:03.675 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:08:03.675 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:08:03.684 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-0188751a-b5c3-4d23-a838-ccb324d2fd65
20:08:03.701 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
20:08:03.745 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:08:03.830 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4086ms
20:08:03.888 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:08:03.902 INFO  [main] org.spark_project.jetty.server.Server - Started @4160ms
20:08:03.925 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:08:03.925 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:08:03.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
20:08:03.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
20:08:03.949 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
20:08:03.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
20:08:03.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
20:08:03.951 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
20:08:03.952 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
20:08:03.953 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
20:08:03.954 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
20:08:03.955 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
20:08:03.956 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
20:08:03.956 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
20:08:03.957 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
20:08:03.958 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
20:08:03.958 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
20:08:03.959 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
20:08:03.959 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
20:08:03.960 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
20:08:03.960 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
20:08:03.961 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:08:03.966 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
20:08:03.967 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
20:08:03.968 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
20:08:03.969 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
20:08:03.970 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
20:08:03.972 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
20:08:04.043 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:08:04.072 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14777.
20:08:04.072 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:14777
20:08:04.073 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:08:04.104 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 14777, None)
20:08:04.107 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:14777 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 14777, None)
20:08:04.110 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 14777, None)
20:08:04.110 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 14777, None)
20:08:04.270 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
20:08:05.723 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
20:08:05.750 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
20:08:05.751 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
20:08:05.757 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
20:08:05.758 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
20:08:05.758 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
20:08:05.759 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
20:08:05.761 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
20:08:06.240 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:08:06.920 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:08:06.955 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:08:08.104 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:08:09.286 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:08:09.289 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:08:09.500 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:08:09.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:08:09.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:08:09.662 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:08:09.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
20:08:09.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:08:09.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:08:09.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:08:09.743 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:08:09.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:08:09.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:08:09.752 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:08:09.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:08:10.254 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/d623e60b-1cf2-41c6-ac97-f04d5862b211_resources
20:08:10.286 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/d623e60b-1cf2-41c6-ac97-f04d5862b211
20:08:10.289 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/d623e60b-1cf2-41c6-ac97-f04d5862b211
20:08:10.293 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/d623e60b-1cf2-41c6-ac97-f04d5862b211/_tmp_space.db
20:08:10.297 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:08:10.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:08:10.319 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:08:10.325 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:08:10.326 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:08:10.331 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:08:10.494 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/4fc71859-9e8d-4016-aebc-986453468b44_resources
20:08:10.511 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/4fc71859-9e8d-4016-aebc-986453468b44
20:08:10.514 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/4fc71859-9e8d-4016-aebc-986453468b44
20:08:10.521 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/4fc71859-9e8d-4016-aebc-986453468b44/_tmp_space.db
20:08:10.523 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:08:10.552 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:08:10.558 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:08:10.766 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:10.766 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:10.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:10.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:10.871 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:10.872 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:10.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.918 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.918 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.918 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:10.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:10.936 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:08:11.217 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:08:11.234 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:08:11.235 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:08:11.235 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:08:11.236 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:08:11.236 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:08:11.237 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:08:11.237 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:08:11.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:11.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:11.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:11.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:11.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:11.513 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:11.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.541 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:11.542 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:11.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:11.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:11.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:11.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:11.673 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
20:08:11.675 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 04)
20:08:11.677 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:11.689 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:11.694 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:08:12.086 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 250.072692 ms
20:08:12.366 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.346899 ms
20:08:12.480 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
20:08:12.606 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
20:08:12.609 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.7 MB)
20:08:12.613 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw04ReleaseClick.scala:66
20:08:12.619 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:08:12.706 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
20:08:12.720 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at Dw04ReleaseClick.scala:66)
20:08:12.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
20:08:12.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw04ReleaseClick.scala:66)
20:08:12.723 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:08:12.725 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:12.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:12.764 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
20:08:12.769 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
20:08:12.770 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:14777 (size: 2.2 KB, free: 1988.7 MB)
20:08:12.770 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
20:08:12.782 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
20:08:12.783 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
20:08:12.827 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:08:12.840 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:08:12.849 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
20:08:12.912 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:12.914 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
20:08:12.932 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
20:08:12.939 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 107 ms on localhost (executor driver) (1/1)
20:08:12.942 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
20:08:12.946 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw04ReleaseClick.scala:66) finished in 0.147 s
20:08:12.950 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw04ReleaseClick.scala:66, took 0.242507 s
20:08:12.957 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
20:08:12.958 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw04ReleaseClick.scala:66) with 3 output partitions
20:08:12.958 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw04ReleaseClick.scala:66)
20:08:12.959 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
20:08:12.959 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:12.959 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:12.961 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
20:08:12.964 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
20:08:12.966 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:14777 (size: 2.2 KB, free: 1988.7 MB)
20:08:12.967 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:08:12.968 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
20:08:12.968 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
20:08:12.969 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:08:12.970 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:08:12.970 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:08:12.971 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
20:08:12.971 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
20:08:12.972 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
20:08:12.975 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:12.976 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:08:12.976 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:12.976 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:12.977 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:12.977 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:12.977 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
20:08:12.977 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
20:08:12.978 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
20:08:12.979 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 10 ms on localhost (executor driver) (1/3)
20:08:12.979 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 9 ms on localhost (executor driver) (2/3)
20:08:12.980 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 11 ms on localhost (executor driver) (3/3)
20:08:12.981 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
20:08:12.981 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw04ReleaseClick.scala:66) finished in 0.013 s
20:08:12.981 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw04ReleaseClick.scala:66, took 0.024060 s
20:08:12.998 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
20:08:13.007 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:13.007 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:13.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:13.223 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-13_221_3837053233789876265-1
20:08:13.437 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:13.438 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:13.443 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:13.443 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:13.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:13.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:13.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:13.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:13.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:13.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:13.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:13.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:13.520 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
20:08:13.521 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 04)
20:08:13.521 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:13.521 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:13.522 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:08:13.536 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:13.537 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:13.554 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 311.6 KB, free 1988.1 MB)
20:08:13.574 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
20:08:13.575 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.6 MB)
20:08:13.576 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
20:08:13.576 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:08:13.648 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:08:13.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:35)
20:08:13.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:08:13.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
20:08:13.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
20:08:13.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:13.650 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:13.722 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 167.3 KB, free 1987.9 MB)
20:08:13.725 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 61.9 KB, free 1987.8 MB)
20:08:13.727 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:14777 (size: 61.9 KB, free: 1988.6 MB)
20:08:13.727 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
20:08:13.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:08:13.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
20:08:13.729 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:08:13.729 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:08:13.730 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:08:13.730 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:08:13.730 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
20:08:13.730 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
20:08:13.730 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
20:08:13.732 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
20:08:13.786 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:13.786 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:13.787 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:13.788 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:08:13.788 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:13.788 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:13.793 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:13.793 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:13.807 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.911319 ms
20:08:13.836 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.288506 ms
20:08:13.854 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:13.854 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:13.854 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:13.854 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:13.856 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:13.856 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:13.856 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:13.856 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:13.867 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.423216 ms
20:08:13.912 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.709054 ms
20:08:13.928 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.79843 ms
20:08:13.937 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200813_0005_m_000001_0
20:08:13.937 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200813_0005_m_000003_0
20:08:13.937 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200813_0005_m_000000_0
20:08:13.937 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200813_0005_m_000002_0
20:08:13.940 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2498 bytes result sent to driver
20:08:13.940 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2498 bytes result sent to driver
20:08:13.940 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2498 bytes result sent to driver
20:08:13.940 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2498 bytes result sent to driver
20:08:13.941 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 212 ms on localhost (executor driver) (1/4)
20:08:13.941 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 212 ms on localhost (executor driver) (2/4)
20:08:13.942 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 213 ms on localhost (executor driver) (3/4)
20:08:13.942 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 212 ms on localhost (executor driver) (4/4)
20:08:13.942 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
20:08:13.944 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.216 s
20:08:13.944 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.296167 s
20:08:14.047 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:08:14.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.072 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.072 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.093 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.094 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.094 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.094 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.094 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.094 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.095 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.095 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:14.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.126 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
20:08:14.244 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 87
20:08:14.244 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 91
20:08:14.244 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
20:08:14.258 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:14777 in memory (size: 26.2 KB, free: 1988.6 MB)
20:08:14.259 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.259 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.260 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
20:08:14.260 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 86
20:08:14.260 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
20:08:14.264 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
20:08:14.265 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.32.1:14777 in memory (size: 26.2 KB, free: 1988.6 MB)
20:08:14.266 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
20:08:14.266 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 85
20:08:14.266 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
20:08:14.266 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 84
20:08:14.268 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:14777 in memory (size: 2.2 KB, free: 1988.6 MB)
20:08:14.269 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 88
20:08:14.269 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 82
20:08:14.269 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
20:08:14.269 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
20:08:14.269 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 83
20:08:14.270 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:14777 in memory (size: 2.2 KB, free: 1988.6 MB)
20:08:14.271 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 92
20:08:14.271 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 90
20:08:14.271 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
20:08:14.271 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 89
20:08:14.271 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 1
20:08:14.271 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
20:08:14.273 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:14777 in memory (size: 61.9 KB, free: 1988.7 MB)
20:08:14.292 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
20:08:14.296 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:14.296 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:14.302 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.302 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.329 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.356 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.356 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.356 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.356 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.356 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.357 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.357 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.357 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:14.362 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_click (inference mode: INFER_AND_SAVE)
20:08:14.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:14.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:14.370 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.370 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:14.397 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:14.420 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.420 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.421 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.421 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.421 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.421 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.421 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:14.422 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:14.422 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_click
20:08:14.422 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_click	
20:08:14.972 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:08:14.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (insertInto at SparkHelper.scala:35) with 1 output partitions
20:08:14.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (insertInto at SparkHelper.scala:35)
20:08:14.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
20:08:14.973 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:14.973 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:15.019 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
20:08:15.021 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
20:08:15.023 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:14777 (size: 26.8 KB, free: 1988.7 MB)
20:08:15.023 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
20:08:15.024 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
20:08:15.024 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
20:08:15.027 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
20:08:15.027 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 8)
20:08:15.260 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 8). 729 bytes result sent to driver
20:08:15.261 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 8) in 236 ms on localhost (executor driver) (1/1)
20:08:15.261 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
20:08:15.262 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (insertInto at SparkHelper.scala:35) finished in 0.236 s
20:08:15.262 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: insertInto at SparkHelper.scala:35, took 0.289626 s
20:08:15.288 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_click from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
20:08:15.314 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:08:15.314 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:15.314 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:15.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:15.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:15.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:15.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:15.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.370 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:15.384 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:08:15.384 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:08:15.385 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:08:15.385 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:08:15.385 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:08:15.385 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:08:15.386 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:08:15.386 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:08:15.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:15.427 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:15.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:15.432 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:15.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:15.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:15.481 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.481 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.482 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.482 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.482 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.482 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.482 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.482 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.483 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.483 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:15.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:15.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:15.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:15.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:15.516 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
20:08:15.516 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 04)
20:08:15.517 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:15.517 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:15.518 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:08:15.540 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 311.6 KB, free 1988.3 MB)
20:08:15.562 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.3 MB)
20:08:15.564 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.6 MB)
20:08:15.565 INFO  [main] org.apache.spark.SparkContext - Created broadcast 6 from show at Dw04ReleaseClick.scala:66
20:08:15.565 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:08:15.574 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
20:08:15.575 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (show at Dw04ReleaseClick.scala:66)
20:08:15.575 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
20:08:15.575 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (show at Dw04ReleaseClick.scala:66)
20:08:15.575 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)
20:08:15.575 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:15.576 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[20] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:15.577 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
20:08:15.580 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
20:08:15.582 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:14777 (size: 2.2 KB, free: 1988.6 MB)
20:08:15.583 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
20:08:15.583 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
20:08:15.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 1 tasks
20:08:15.584 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:08:15.585 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 9)
20:08:15.587 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:15.587 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:15.588 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 9). 1225 bytes result sent to driver
20:08:15.588 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 9) in 4 ms on localhost (executor driver) (1/1)
20:08:15.589 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
20:08:15.589 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (show at Dw04ReleaseClick.scala:66) finished in 0.005 s
20:08:15.589 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at Dw04ReleaseClick.scala:66, took 0.015129 s
20:08:15.592 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
20:08:15.593 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (show at Dw04ReleaseClick.scala:66) with 3 output partitions
20:08:15.593 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (show at Dw04ReleaseClick.scala:66)
20:08:15.593 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9)
20:08:15.593 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:15.593 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[20] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:15.595 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
20:08:15.597 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
20:08:15.599 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:14777 (size: 2.2 KB, free: 1988.6 MB)
20:08:15.599 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1004
20:08:15.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 10 (MapPartitionsRDD[20] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
20:08:15.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 3 tasks
20:08:15.601 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:08:15.601 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 11, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:08:15.601 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 10.0 (TID 12, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:08:15.601 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 2.0 in stage 10.0 (TID 12)
20:08:15.601 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 10)
20:08:15.601 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 11)
20:08:15.603 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:15.603 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:15.603 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:15.603 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:15.603 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:15.603 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:15.604 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 11). 1182 bytes result sent to driver
20:08:15.604 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 10). 1182 bytes result sent to driver
20:08:15.604 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 2.0 in stage 10.0 (TID 12). 1182 bytes result sent to driver
20:08:15.605 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 11) in 4 ms on localhost (executor driver) (1/3)
20:08:15.605 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 10.0 (TID 12) in 4 ms on localhost (executor driver) (2/3)
20:08:15.605 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 10) in 5 ms on localhost (executor driver) (3/3)
20:08:15.605 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
20:08:15.606 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (show at Dw04ReleaseClick.scala:66) finished in 0.006 s
20:08:15.606 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: show at Dw04ReleaseClick.scala:66, took 0.013858 s
20:08:15.612 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
20:08:15.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:15.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:15.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.632 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.633 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.633 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.633 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:15.637 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-15_637_4269379610227131376-1
20:08:15.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:15.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:15.732 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:15.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:15.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:15.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:15.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:15.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:15.782 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:15.782 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:15.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:15.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:15.810 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
20:08:15.810 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 04)
20:08:15.810 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:15.810 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:15.811 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:08:15.817 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:15.817 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:15.830 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 311.6 KB, free 1988.0 MB)
20:08:15.847 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.9 MB)
20:08:15.849 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.6 MB)
20:08:15.850 INFO  [main] org.apache.spark.SparkContext - Created broadcast 9 from insertInto at SparkHelper.scala:35
20:08:15.850 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:08:15.895 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:08:15.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 24 (insertInto at SparkHelper.scala:35)
20:08:15.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:08:15.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 12 (insertInto at SparkHelper.scala:35)
20:08:15.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 11)
20:08:15.896 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:15.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 12 (MapPartitionsRDD[26] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:15.934 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 167.3 KB, free 1987.8 MB)
20:08:15.937 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 61.9 KB, free 1987.7 MB)
20:08:15.939 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:14777 (size: 61.9 KB, free: 1988.6 MB)
20:08:15.940 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1004
20:08:15.940 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 12 (MapPartitionsRDD[26] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:08:15.940 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 4 tasks
20:08:15.941 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:08:15.942 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 14, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:08:15.942 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 15, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:08:15.942 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 16, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:08:15.942 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 13)
20:08:15.942 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 14)
20:08:15.942 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 15)
20:08:15.942 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 16)
20:08:15.964 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:15.964 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:15.965 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:15.965 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:15.966 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:15.966 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:15.967 INFO  [Executor task launch worker for task 14] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:16.034 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:16.034 INFO  [Executor task launch worker for task 15] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:16.034 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:08:16.035 INFO  [Executor task launch worker for task 16] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:08:16.035 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:16.035 INFO  [Executor task launch worker for task 13] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:16.035 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:16.039 INFO  [Executor task launch worker for task 16] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:16.039 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:16.039 INFO  [Executor task launch worker for task 15] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200815_0012_m_000002_0
20:08:16.040 INFO  [Executor task launch worker for task 14] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200815_0012_m_000001_0
20:08:16.040 INFO  [Executor task launch worker for task 13] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200816_0012_m_000000_0
20:08:16.040 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 15). 2412 bytes result sent to driver
20:08:16.041 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 15) in 99 ms on localhost (executor driver) (1/4)
20:08:16.042 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 14). 2455 bytes result sent to driver
20:08:16.042 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 13). 2412 bytes result sent to driver
20:08:16.043 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 14) in 101 ms on localhost (executor driver) (2/4)
20:08:16.043 INFO  [Executor task launch worker for task 16] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200816_0012_m_000003_0
20:08:16.044 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 13) in 103 ms on localhost (executor driver) (3/4)
20:08:16.044 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 16). 2412 bytes result sent to driver
20:08:16.045 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 16) in 103 ms on localhost (executor driver) (4/4)
20:08:16.045 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
20:08:16.045 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 12 (insertInto at SparkHelper.scala:35) finished in 0.104 s
20:08:16.046 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: insertInto at SparkHelper.scala:35, took 0.150641 s
20:08:16.059 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:08:16.059 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.059 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.108 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.108 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.108 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.108 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.108 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.109 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.109 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:16.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.133 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
20:08:16.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.157 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
20:08:16.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:16.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:16.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.164 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.185 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.185 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:16.212 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_click (inference mode: INFER_AND_SAVE)
20:08:16.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:16.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:16.218 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.218 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:16.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:16.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:16.257 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_click
20:08:16.257 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_click	
20:08:16.331 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:08:16.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 7 (insertInto at SparkHelper.scala:35) with 1 output partitions
20:08:16.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (insertInto at SparkHelper.scala:35)
20:08:16.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
20:08:16.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:16.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[28] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:16.357 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 74.4 KB, free 1987.6 MB)
20:08:16.360 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1987.6 MB)
20:08:16.361 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:14777 (size: 26.8 KB, free: 1988.5 MB)
20:08:16.362 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
20:08:16.362 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[28] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
20:08:16.362 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks
20:08:16.362 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 4823 bytes)
20:08:16.363 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 17)
20:08:16.370 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 17). 643 bytes result sent to driver
20:08:16.370 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 17) in 8 ms on localhost (executor driver) (1/1)
20:08:16.370 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
20:08:16.371 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (insertInto at SparkHelper.scala:35) finished in 0.008 s
20:08:16.371 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 7 finished: insertInto at SparkHelper.scala:35, took 0.039794 s
20:08:16.372 WARN  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Unable to infer schema for table dw_release.dw_release_click from file format Parquet (inference mode: INFER_AND_SAVE). Using metastore schema.
20:08:16.382 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:08:16.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:16.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:16.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:16.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:16.409 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:16.409 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:16.434 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.435 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.435 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.435 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.435 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.435 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.436 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.436 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.436 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.436 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:16.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:08:16.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:08:16.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:08:16.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:08:16.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:08:16.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:08:16.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:08:16.445 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:08:16.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:16.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:16.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:16.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:16.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:16.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:16.523 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.523 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.523 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.523 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.523 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.524 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.524 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.524 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.524 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:16.524 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:16.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:16.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:16.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:16.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:16.585 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
20:08:16.585 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 04)
20:08:16.585 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:16.585 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:16.585 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:08:16.599 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 311.6 KB, free 1987.3 MB)
20:08:16.623 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.3 MB)
20:08:16.625 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.5 MB)
20:08:16.626 INFO  [main] org.apache.spark.SparkContext - Created broadcast 12 from show at Dw04ReleaseClick.scala:66
20:08:16.626 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:08:16.645 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
20:08:16.646 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 33 (show at Dw04ReleaseClick.scala:66)
20:08:16.646 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 8 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
20:08:16.646 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (show at Dw04ReleaseClick.scala:66)
20:08:16.646 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
20:08:16.646 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
20:08:16.648 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (MapPartitionsRDD[33] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:16.659 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 17.6 KB, free 1987.3 MB)
20:08:16.661 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.3 MB)
20:08:16.663 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:14777 (size: 7.5 KB, free: 1988.5 MB)
20:08:16.664 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 13 from broadcast at DAGScheduler.scala:1004
20:08:16.667 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[33] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:08:16.667 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 8 tasks
20:08:16.673 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 18, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:08:16.674 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 19, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:08:16.674 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 20, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:08:16.674 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 21, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:08:16.675 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 22, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:08:16.675 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 23, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:08:16.675 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 24, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:08:16.675 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 25, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:08:16.675 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 18)
20:08:16.675 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 19)
20:08:16.675 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 21)
20:08:16.676 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 20)
20:08:16.677 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 25)
20:08:16.677 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 22)
20:08:16.677 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 24)
20:08:16.677 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 23)
20:08:16.696 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
20:08:16.696 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
20:08:16.696 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
20:08:16.696 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
20:08:16.696 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
20:08:16.696 INFO  [Executor task launch worker for task 24] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
20:08:16.696 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
20:08:16.696 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
20:08:17.949 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:17.949 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:17.950 INFO  [Executor task launch worker for task 24] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:17.976 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:17.978 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:17.982 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:17.999 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:17.999 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:18.009 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 25). 1414 bytes result sent to driver
20:08:18.009 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 23). 1414 bytes result sent to driver
20:08:18.013 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 25) in 1338 ms on localhost (executor driver) (1/8)
20:08:18.014 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 23) in 1338 ms on localhost (executor driver) (2/8)
20:08:18.016 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 20). 1414 bytes result sent to driver
20:08:18.017 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 20) in 1343 ms on localhost (executor driver) (3/8)
20:08:18.020 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 18). 1371 bytes result sent to driver
20:08:18.021 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 18) in 1351 ms on localhost (executor driver) (4/8)
20:08:18.025 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 24). 1371 bytes result sent to driver
20:08:18.025 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 24) in 1350 ms on localhost (executor driver) (5/8)
20:08:18.029 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 22). 1371 bytes result sent to driver
20:08:18.029 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 22) in 1355 ms on localhost (executor driver) (6/8)
20:08:18.064 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 19). 1371 bytes result sent to driver
20:08:18.064 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 19) in 1391 ms on localhost (executor driver) (7/8)
20:08:18.895 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 21). 1586 bytes result sent to driver
20:08:18.896 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 21) in 2222 ms on localhost (executor driver) (8/8)
20:08:18.896 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
20:08:18.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (show at Dw04ReleaseClick.scala:66) finished in 2.227 s
20:08:18.898 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:08:18.899 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:08:18.899 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
20:08:18.900 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:08:18.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[35] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:18.905 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 3.7 KB, free 1987.2 MB)
20:08:18.926 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.2 MB)
20:08:18.927 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:14777 (size: 2.3 KB, free: 1988.5 MB)
20:08:18.927 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1004
20:08:18.928 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[35] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
20:08:18.928 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 1 tasks
20:08:18.928 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 26, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:08:18.929 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 26)
20:08:18.934 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:18.934 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 206
20:08:18.934 INFO  [Executor task launch worker for task 26] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:08:18.934 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 200
20:08:18.934 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 145
20:08:18.936 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:14777 in memory (size: 2.2 KB, free: 1988.5 MB)
20:08:18.939 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:14777 in memory (size: 26.8 KB, free: 1988.5 MB)
20:08:18.939 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 199
20:08:18.939 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 144
20:08:18.940 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:14777 in memory (size: 26.2 KB, free: 1988.6 MB)
20:08:18.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
20:08:18.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 201
20:08:18.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 258
20:08:18.942 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:14777 in memory (size: 26.8 KB, free: 1988.6 MB)
20:08:18.942 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 148
20:08:18.944 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:14777 in memory (size: 26.2 KB, free: 1988.6 MB)
20:08:18.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 149
20:08:18.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 141
20:08:18.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 208
20:08:18.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 202
20:08:18.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 143
20:08:18.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 207
20:08:18.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 204
20:08:18.946 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:14777 in memory (size: 2.2 KB, free: 1988.6 MB)
20:08:18.946 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 146
20:08:18.946 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 203
20:08:18.947 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:14777 in memory (size: 61.9 KB, free: 1988.7 MB)
20:08:18.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 198
20:08:18.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 205
20:08:18.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 142
20:08:18.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 209
20:08:18.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
20:08:18.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 147
20:08:18.956 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 26). 4694 bytes result sent to driver
20:08:18.957 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 26) in 29 ms on localhost (executor driver) (1/1)
20:08:18.957 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
20:08:18.957 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (show at Dw04ReleaseClick.scala:66) finished in 0.029 s
20:08:18.957 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 8 finished: show at Dw04ReleaseClick.scala:66, took 2.311956 s
20:08:18.972 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
20:08:18.973 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:18.973 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:18.990 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:18.990 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:18.990 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:18.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:18.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:18.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:18.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:18.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:18.995 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1
20:08:19.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:19.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:19.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:19.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:19.040 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:19.040 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:19.054 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.054 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:19.055 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:19.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:19.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:19.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:19.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:19.080 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
20:08:19.080 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 04)
20:08:19.081 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:19.081 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:19.081 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:08:19.086 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:19.086 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:19.123 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 311.6 KB, free 1988.0 MB)
20:08:19.139 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
20:08:19.140 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.6 MB)
20:08:19.141 INFO  [main] org.apache.spark.SparkContext - Created broadcast 15 from insertInto at SparkHelper.scala:35
20:08:19.141 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:08:19.182 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:08:19.183 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 39 (insertInto at SparkHelper.scala:35)
20:08:19.183 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 9 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:08:19.183 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (insertInto at SparkHelper.scala:35)
20:08:19.184 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
20:08:19.184 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
20:08:19.184 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[39] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:19.186 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 17.6 KB, free 1988.0 MB)
20:08:19.188 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.0 MB)
20:08:19.189 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:14777 (size: 7.5 KB, free: 1988.6 MB)
20:08:19.189 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 16 from broadcast at DAGScheduler.scala:1004
20:08:19.190 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[39] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:08:19.190 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 8 tasks
20:08:19.190 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 27, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:08:19.191 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 28, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:08:19.191 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 29, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:08:19.191 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 30, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:08:19.191 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 31, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:08:19.192 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 32, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:08:19.192 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 33, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:08:19.192 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 16.0 (TID 34, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:08:19.193 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 27)
20:08:19.193 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 28)
20:08:19.193 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 30)
20:08:19.193 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 29)
20:08:19.193 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 31)
20:08:19.193 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 7.0 in stage 16.0 (TID 34)
20:08:19.193 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 32)
20:08:19.193 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 33)
20:08:19.197 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
20:08:19.197 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
20:08:19.199 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
20:08:19.200 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
20:08:19.202 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
20:08:19.203 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
20:08:19.203 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
20:08:19.206 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
20:08:19.235 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.249 INFO  [Executor task launch worker for task 33] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.250 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.253 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.256 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.263 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 27). 1371 bytes result sent to driver
20:08:19.265 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 27) in 75 ms on localhost (executor driver) (1/8)
20:08:19.272 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 33). 1371 bytes result sent to driver
20:08:19.278 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 32). 1371 bytes result sent to driver
20:08:19.278 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.281 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 33) in 89 ms on localhost (executor driver) (2/8)
20:08:19.285 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 29). 1371 bytes result sent to driver
20:08:19.286 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 32) in 94 ms on localhost (executor driver) (3/8)
20:08:19.290 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 29) in 99 ms on localhost (executor driver) (4/8)
20:08:19.290 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 31). 1328 bytes result sent to driver
20:08:19.291 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 31) in 100 ms on localhost (executor driver) (5/8)
20:08:19.299 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.300 INFO  [Executor task launch worker for task 34] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:19.305 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 28). 1414 bytes result sent to driver
20:08:19.306 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 28) in 116 ms on localhost (executor driver) (6/8)
20:08:19.313 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 7.0 in stage 16.0 (TID 34). 1371 bytes result sent to driver
20:08:19.314 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 16.0 (TID 34) in 122 ms on localhost (executor driver) (7/8)
20:08:19.611 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 30). 1543 bytes result sent to driver
20:08:19.612 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 30) in 421 ms on localhost (executor driver) (8/8)
20:08:19.612 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
20:08:19.612 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (insertInto at SparkHelper.scala:35) finished in 0.422 s
20:08:19.612 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:08:19.612 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:08:19.612 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
20:08:19.612 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:08:19.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[41] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:19.645 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 167.4 KB, free 1987.8 MB)
20:08:19.647 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 62.0 KB, free 1987.8 MB)
20:08:19.648 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.32.1:14777 (size: 62.0 KB, free: 1988.6 MB)
20:08:19.648 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1004
20:08:19.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 17 (MapPartitionsRDD[41] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:08:19.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 4 tasks
20:08:19.649 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 35, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:08:19.650 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 17.0 (TID 36, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:08:19.650 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 17.0 (TID 37, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:08:19.650 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 17.0 (TID 38, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:08:19.650 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 3.0 in stage 17.0 (TID 38)
20:08:19.650 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 1.0 in stage 17.0 (TID 36)
20:08:19.650 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 2.0 in stage 17.0 (TID 37)
20:08:19.650 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 35)
20:08:19.673 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:19.673 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:19.673 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:19.673 INFO  [Executor task launch worker for task 38] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:19.674 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:19.674 INFO  [Executor task launch worker for task 37] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:19.674 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:19.674 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:19.740 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:19.740 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:19.741 INFO  [Executor task launch worker for task 38] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:19.742 INFO  [Executor task launch worker for task 37] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:19.742 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:19.742 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:19.743 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:19.743 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:19.838 INFO  [Executor task launch worker for task 38] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3df98f48
20:08:19.839 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@21096171
20:08:19.839 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@9669650
20:08:19.839 INFO  [Executor task launch worker for task 37] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d523e3f
20:08:19.852 INFO  [Executor task launch worker for task 35] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
20:08:19.857 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:19.857 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:19.857 INFO  [Executor task launch worker for task 38] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:19.857 INFO  [Executor task launch worker for task 37] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:19.857 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200819_0017_m_000001_0/bdp_day=20190922/part-00001-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
20:08:19.857 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200819_0017_m_000000_0/bdp_day=20190922/part-00000-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
20:08:19.857 INFO  [Executor task launch worker for task 38] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200819_0017_m_000003_0/bdp_day=20190922/part-00003-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
20:08:19.857 INFO  [Executor task launch worker for task 37] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200819_0017_m_000002_0/bdp_day=20190922/part-00002-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
20:08:19.859 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:19.859 INFO  [Executor task launch worker for task 38] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:19.859 INFO  [Executor task launch worker for task 37] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:19.859 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:19.859 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:19.859 INFO  [Executor task launch worker for task 38] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:19.859 INFO  [Executor task launch worker for task 37] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:19.859 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:19.860 INFO  [Executor task launch worker for task 38] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:19.860 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:19.860 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:19.860 INFO  [Executor task launch worker for task 37] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:19.860 INFO  [Executor task launch worker for task 38] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 37] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 38] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 37] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:19.861 INFO  [Executor task launch worker for task 38] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:19.861 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:19.861 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:19.861 INFO  [Executor task launch worker for task 37] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:19.861 INFO  [Executor task launch worker for task 38] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:19.861 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:19.861 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:19.861 INFO  [Executor task launch worker for task 37] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:19.861 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:19.861 INFO  [Executor task launch worker for task 38] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:19.861 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:19.861 INFO  [Executor task launch worker for task 37] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:20.029 INFO  [Executor task launch worker for task 38] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@4d774f39
20:08:20.029 INFO  [Executor task launch worker for task 37] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@167b1c0e
20:08:20.029 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@66bd9f65
20:08:20.030 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2b4848e2
20:08:20.265 INFO  [Executor task launch worker for task 38] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,839
20:08:20.274 INFO  [Executor task launch worker for task 36] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,711
20:08:20.288 INFO  [Executor task launch worker for task 37] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,238
20:08:20.305 INFO  [Executor task launch worker for task 35] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,283
20:08:20.366 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
20:08:20.368 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:14777 in memory (size: 26.2 KB, free: 1988.6 MB)
20:08:20.377 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.32.1:14777 in memory (size: 7.5 KB, free: 1988.6 MB)
20:08:20.389 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.32.1:14777 in memory (size: 7.5 KB, free: 1988.6 MB)
20:08:20.425 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 192.168.32.1:14777 in memory (size: 2.3 KB, free: 1988.6 MB)
20:08:20.436 INFO  [Executor task launch worker for task 38] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.436 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.437 INFO  [Executor task launch worker for task 37] parquet.hadoop.ColumnChunkPageWriteStore - written 290,389B for [release_session] BINARY: 10,752 values, 290,312B raw, 290,312B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.437 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.438 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.438 INFO  [Executor task launch worker for task 37] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.439 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.439 INFO  [Executor task launch worker for task 37] parquet.hadoop.ColumnChunkPageWriteStore - written 107,571B for [device_num] BINARY: 10,752 values, 107,528B raw, 107,528B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.439 INFO  [Executor task launch worker for task 38] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.439 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:20.439 INFO  [Executor task launch worker for task 37] parquet.hadoop.ColumnChunkPageWriteStore - written 2,751B for [device_type] BINARY: 10,752 values, 2,720B raw, 2,720B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:20.439 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.440 INFO  [Executor task launch worker for task 38] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.441 INFO  [Executor task launch worker for task 37] parquet.hadoop.ColumnChunkPageWriteStore - written 4,105B for [sources] BINARY: 10,752 values, 4,063B raw, 4,063B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:20.442 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:20.442 INFO  [Executor task launch worker for task 38] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:20.443 INFO  [Executor task launch worker for task 37] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.446 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.443 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:20.447 INFO  [Executor task launch worker for task 38] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:20.448 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,416 entries, 19,328B raw, 2,416B comp}
20:08:20.448 INFO  [Executor task launch worker for task 37] parquet.hadoop.ColumnChunkPageWriteStore - written 16,206B for [ct] INT64: 10,752 values, 16,159B raw, 16,159B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,401 entries, 19,208B raw, 2,401B comp}
20:08:20.449 INFO  [Executor task launch worker for task 38] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.449 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 2,750B for [device_type] BINARY: 10,753 values, 2,719B raw, 2,719B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:20.450 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:20.450 INFO  [Executor task launch worker for task 38] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,428 entries, 19,424B raw, 2,428B comp}
20:08:20.451 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:20.461 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,407 entries, 19,256B raw, 2,407B comp}
20:08:20.723 INFO  [Executor task launch worker for task 38] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200819_0017_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/task_20190925200819_0017_m_000003
20:08:20.723 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200819_0017_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/task_20190925200819_0017_m_000001
20:08:20.724 INFO  [Executor task launch worker for task 38] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200819_0017_m_000003_0: Committed
20:08:20.724 INFO  [Executor task launch worker for task 36] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200819_0017_m_000001_0: Committed
20:08:20.725 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 1.0 in stage 17.0 (TID 36). 2568 bytes result sent to driver
20:08:20.725 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 3.0 in stage 17.0 (TID 38). 2568 bytes result sent to driver
20:08:20.726 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 17.0 (TID 38) in 1076 ms on localhost (executor driver) (1/4)
20:08:20.726 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 17.0 (TID 36) in 1077 ms on localhost (executor driver) (2/4)
20:08:21.093 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200819_0017_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/task_20190925200819_0017_m_000000
20:08:21.093 INFO  [Executor task launch worker for task 35] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200819_0017_m_000000_0: Committed
20:08:21.093 INFO  [Executor task launch worker for task 37] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200819_0017_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/_temporary/0/task_20190925200819_0017_m_000002
20:08:21.093 INFO  [Executor task launch worker for task 37] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200819_0017_m_000002_0: Committed
20:08:21.093 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 35). 2525 bytes result sent to driver
20:08:21.094 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 2.0 in stage 17.0 (TID 37). 2525 bytes result sent to driver
20:08:21.094 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 35) in 1445 ms on localhost (executor driver) (3/4)
20:08:21.094 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 17.0 (TID 37) in 1444 ms on localhost (executor driver) (4/4)
20:08:21.094 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
20:08:21.095 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (insertInto at SparkHelper.scala:35) finished in 1.446 s
20:08:21.095 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 9 finished: insertInto at SparkHelper.scala:35, took 1.912267 s
20:08:21.135 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:08:21.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.168 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.169 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:21.170 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.170 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.192 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]
20:08:21.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]	
20:08:21.244 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20:08:21.256 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/bdp_day=20190922/part-00000-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00000-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, Status:true
20:08:21.273 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/bdp_day=20190922/part-00001-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00001-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, Status:true
20:08:21.283 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/bdp_day=20190922/part-00002-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00002-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, Status:true
20:08:21.291 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/bdp_day=20190922/part-00003-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00003-876f2164-ec43-46ba-8bb7-a707ed771d98.c000, Status:true
20:08:21.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]
20:08:21.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]	
20:08:21.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_click[20190922]
20:08:21.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_click[20190922]	
20:08:21.342 WARN  [main] hive.log - Updating partition stats fast for: dw_release_click
20:08:21.345 WARN  [main] hive.log - Updated size to 1765301
20:08:21.464 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-18_994_4008240872625022989-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
20:08:21.466 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
20:08:21.467 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:21.467 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:21.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.510 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.510 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.510 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.511 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:21.514 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_click (inference mode: INFER_AND_SAVE)
20:08:21.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:21.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:21.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:21.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:21.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.562 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.562 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:21.562 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:21.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_click
20:08:21.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_click	
20:08:21.665 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:08:21.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 10 (insertInto at SparkHelper.scala:35) with 1 output partitions
20:08:21.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 18 (insertInto at SparkHelper.scala:35)
20:08:21.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
20:08:21.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:08:21.666 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 18 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:21.687 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 74.4 KB, free 1988.1 MB)
20:08:21.688 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.0 MB)
20:08:21.689 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.32.1:14777 (size: 26.8 KB, free: 1988.6 MB)
20:08:21.690 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1004
20:08:21.690 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
20:08:21.690 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 1 tasks
20:08:21.692 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 5047 bytes)
20:08:21.692 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 39)
20:08:22.228 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 39). 1605 bytes result sent to driver
20:08:22.229 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 39) in 538 ms on localhost (executor driver) (1/1)
20:08:22.230 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
20:08:22.230 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 18 (insertInto at SparkHelper.scala:35) finished in 0.539 s
20:08:22.231 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 10 finished: insertInto at SparkHelper.scala:35, took 0.565383 s
20:08:22.236 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_click
20:08:22.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:22.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:22.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:22.243 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:22.265 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:22.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:22.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:22.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:22.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:22.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:22.318 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.340 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:22.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:22.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:22.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_click newtbl=dw_release_click
20:08:22.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_click newtbl=dw_release_click	
20:08:22.518 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:08:22.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:22.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:22.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:22.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:22.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:22.542 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:22.564 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:22.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:08:22.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:08:22.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:08:22.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:08:22.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:08:22.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:08:22.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:08:22.573 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:08:22.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:22.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:22.607 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:22.607 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:22.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:22.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:22.644 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.645 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.645 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.645 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.645 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.645 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.645 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.645 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.646 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:22.646 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:22.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:22.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:22.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:22.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:22.689 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
20:08:22.689 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 04)
20:08:22.689 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:22.689 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:22.690 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:08:22.701 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 311.6 KB, free 1987.7 MB)
20:08:22.718 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.7 MB)
20:08:22.720 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.6 MB)
20:08:22.721 INFO  [main] org.apache.spark.SparkContext - Created broadcast 19 from show at Dw04ReleaseClick.scala:66
20:08:22.721 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:08:22.729 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
20:08:22.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 48 (show at Dw04ReleaseClick.scala:66)
20:08:22.730 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 11 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
20:08:22.730 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 20 (show at Dw04ReleaseClick.scala:66)
20:08:22.730 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 19)
20:08:22.730 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 19)
20:08:22.730 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 19 (MapPartitionsRDD[48] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:22.734 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 17.6 KB, free 1987.7 MB)
20:08:22.736 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.7 MB)
20:08:22.736 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.32.1:14777 (size: 7.5 KB, free: 1988.6 MB)
20:08:22.737 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1004
20:08:22.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[48] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:08:22.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 8 tasks
20:08:22.738 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 40, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:08:22.739 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 41, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:08:22.739 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 42, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:08:22.739 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 43, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:08:22.739 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 19.0 (TID 44, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:08:22.739 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 19.0 (TID 45, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:08:22.740 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 19.0 (TID 46, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:08:22.740 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 19.0 (TID 47, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:08:22.740 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 40)
20:08:22.740 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 41)
20:08:22.740 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 42)
20:08:22.740 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 4.0 in stage 19.0 (TID 44)
20:08:22.740 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 5.0 in stage 19.0 (TID 45)
20:08:22.740 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 6.0 in stage 19.0 (TID 46)
20:08:22.742 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 43)
20:08:22.742 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 7.0 in stage 19.0 (TID 47)
20:08:22.743 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:08:22.744 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:08:22.744 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:08:22.744 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:08:22.744 INFO  [Executor task launch worker for task 45] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:08:22.745 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:08:22.747 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:08:22.747 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:08:22.775 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.775 INFO  [Executor task launch worker for task 46] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.793 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.794 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 41). 1328 bytes result sent to driver
20:08:22.797 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.798 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.800 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.802 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 6.0 in stage 19.0 (TID 46). 1371 bytes result sent to driver
20:08:22.805 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 41) in 66 ms on localhost (executor driver) (1/8)
20:08:22.806 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 19.0 (TID 46) in 67 ms on localhost (executor driver) (2/8)
20:08:22.807 INFO  [Executor task launch worker for task 45] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.818 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 42). 1328 bytes result sent to driver
20:08:22.820 INFO  [Executor task launch worker for task 47] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:22.823 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 40). 1371 bytes result sent to driver
20:08:22.824 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 42) in 85 ms on localhost (executor driver) (3/8)
20:08:22.825 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 40) in 87 ms on localhost (executor driver) (4/8)
20:08:22.828 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 4.0 in stage 19.0 (TID 44). 1371 bytes result sent to driver
20:08:22.828 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 19.0 (TID 44) in 89 ms on localhost (executor driver) (5/8)
20:08:22.832 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 5.0 in stage 19.0 (TID 45). 1328 bytes result sent to driver
20:08:22.832 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 19.0 (TID 45) in 93 ms on localhost (executor driver) (6/8)
20:08:22.840 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 7.0 in stage 19.0 (TID 47). 1371 bytes result sent to driver
20:08:22.840 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 19.0 (TID 47) in 100 ms on localhost (executor driver) (7/8)
20:08:23.037 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 43). 1543 bytes result sent to driver
20:08:23.037 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 43) in 298 ms on localhost (executor driver) (8/8)
20:08:23.037 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
20:08:23.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 19 (show at Dw04ReleaseClick.scala:66) finished in 0.300 s
20:08:23.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:08:23.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:08:23.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 20)
20:08:23.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:08:23.038 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 20 (MapPartitionsRDD[50] at show at Dw04ReleaseClick.scala:66), which has no missing parents
20:08:23.039 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 3.7 KB, free 1987.7 MB)
20:08:23.040 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.7 MB)
20:08:23.041 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.32.1:14777 (size: 2.3 KB, free: 1988.6 MB)
20:08:23.041 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1004
20:08:23.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[50] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
20:08:23.041 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 1 tasks
20:08:23.042 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 48, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:08:23.042 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 48)
20:08:23.044 INFO  [Executor task launch worker for task 48] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:23.047 INFO  [Executor task launch worker for task 48] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
20:08:23.066 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 48). 4626 bytes result sent to driver
20:08:23.066 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 48) in 24 ms on localhost (executor driver) (1/1)
20:08:23.066 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
20:08:23.067 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 20 (show at Dw04ReleaseClick.scala:66) finished in 0.025 s
20:08:23.067 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 11 finished: show at Dw04ReleaseClick.scala:66, took 0.338503 s
20:08:23.376 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
20:08:23.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:23.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:23.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:23.406 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1
20:08:23.428 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:08:23.428 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:08:23.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:23.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:23.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:08:23.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:08:23.484 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:23.485 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:23.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:08:23.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:08:23.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:08:23.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:08:23.516 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
20:08:23.516 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 04)
20:08:23.517 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:08:23.517 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
20:08:23.517 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:08:23.521 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:23.521 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:23.529 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 311.6 KB, free 1987.4 MB)
20:08:23.549 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.4 MB)
20:08:23.550 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.32.1:14777 (size: 26.2 KB, free: 1988.5 MB)
20:08:23.551 INFO  [main] org.apache.spark.SparkContext - Created broadcast 22 from insertInto at SparkHelper.scala:35
20:08:23.551 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:08:23.591 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:08:23.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 54 (insertInto at SparkHelper.scala:35)
20:08:23.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 12 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:08:23.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 22 (insertInto at SparkHelper.scala:35)
20:08:23.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 21)
20:08:23.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 21)
20:08:23.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 21 (MapPartitionsRDD[54] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:23.594 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 17.6 KB, free 1987.3 MB)
20:08:23.596 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.3 MB)
20:08:23.596 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.32.1:14777 (size: 7.5 KB, free: 1988.5 MB)
20:08:23.597 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1004
20:08:23.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[54] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:08:23.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 8 tasks
20:08:23.599 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 49, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:08:23.600 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 21.0 (TID 50, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:08:23.600 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 21.0 (TID 51, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:08:23.600 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 21.0 (TID 52, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:08:23.600 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 21.0 (TID 53, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:08:23.600 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 21.0 (TID 54, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:08:23.600 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 21.0 (TID 55, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:08:23.600 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 21.0 (TID 56, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:08:23.600 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 2.0 in stage 21.0 (TID 51)
20:08:23.600 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 3.0 in stage 21.0 (TID 52)
20:08:23.600 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 49)
20:08:23.600 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 1.0 in stage 21.0 (TID 50)
20:08:23.600 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 5.0 in stage 21.0 (TID 54)
20:08:23.600 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 7.0 in stage 21.0 (TID 56)
20:08:23.600 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 4.0 in stage 21.0 (TID 53)
20:08:23.600 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 6.0 in stage 21.0 (TID 55)
20:08:23.603 INFO  [Executor task launch worker for task 53] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:08:23.603 INFO  [Executor task launch worker for task 55] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:08:23.603 INFO  [Executor task launch worker for task 56] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:08:23.604 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:08:23.604 INFO  [Executor task launch worker for task 50] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:08:23.604 INFO  [Executor task launch worker for task 51] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:08:23.604 INFO  [Executor task launch worker for task 54] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:08:23.606 INFO  [Executor task launch worker for task 52] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:08:23.625 INFO  [Executor task launch worker for task 56] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.626 INFO  [Executor task launch worker for task 50] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.629 INFO  [Executor task launch worker for task 55] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.636 INFO  [Executor task launch worker for task 53] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.649 INFO  [Executor task launch worker for task 52] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.652 INFO  [Executor task launch worker for task 49] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.653 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 7.0 in stage 21.0 (TID 56). 1371 bytes result sent to driver
20:08:23.654 INFO  [Executor task launch worker for task 51] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.656 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 1.0 in stage 21.0 (TID 50). 1371 bytes result sent to driver
20:08:23.657 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 21.0 (TID 56) in 57 ms on localhost (executor driver) (1/8)
20:08:23.657 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 21.0 (TID 50) in 58 ms on localhost (executor driver) (2/8)
20:08:23.660 INFO  [Executor task launch worker for task 54] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
20:08:23.663 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 4.0 in stage 21.0 (TID 53). 1328 bytes result sent to driver
20:08:23.664 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 21.0 (TID 53) in 64 ms on localhost (executor driver) (3/8)
20:08:23.676 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 6.0 in stage 21.0 (TID 55). 1371 bytes result sent to driver
20:08:23.677 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 2.0 in stage 21.0 (TID 51). 1328 bytes result sent to driver
20:08:23.677 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 21.0 (TID 55) in 77 ms on localhost (executor driver) (4/8)
20:08:23.678 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 21.0 (TID 51) in 78 ms on localhost (executor driver) (5/8)
20:08:23.682 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 5.0 in stage 21.0 (TID 54). 1371 bytes result sent to driver
20:08:23.683 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 21.0 (TID 54) in 83 ms on localhost (executor driver) (6/8)
20:08:23.686 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 49). 1371 bytes result sent to driver
20:08:23.687 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 49) in 88 ms on localhost (executor driver) (7/8)
20:08:23.845 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 3.0 in stage 21.0 (TID 52). 1543 bytes result sent to driver
20:08:23.845 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 21.0 (TID 52) in 245 ms on localhost (executor driver) (8/8)
20:08:23.845 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
20:08:23.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 21 (insertInto at SparkHelper.scala:35) finished in 0.246 s
20:08:23.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:08:23.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:08:23.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 22)
20:08:23.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:08:23.846 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 22 (MapPartitionsRDD[56] at insertInto at SparkHelper.scala:35), which has no missing parents
20:08:23.881 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 167.4 KB, free 1987.2 MB)
20:08:23.885 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 62.0 KB, free 1987.1 MB)
20:08:23.885 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.32.1:14777 (size: 62.0 KB, free: 1988.5 MB)
20:08:23.886 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1004
20:08:23.886 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 22 (MapPartitionsRDD[56] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:08:23.887 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 4 tasks
20:08:23.887 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 57, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:08:23.887 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 58, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:08:23.887 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 59, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:08:23.888 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 60, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:08:23.888 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 59)
20:08:23.888 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 58)
20:08:23.888 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 60)
20:08:23.888 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 57)
20:08:23.909 INFO  [Executor task launch worker for task 59] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:23.909 INFO  [Executor task launch worker for task 59] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:23.909 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:23.909 INFO  [Executor task launch worker for task 60] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:23.909 INFO  [Executor task launch worker for task 58] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:08:23.909 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:23.909 INFO  [Executor task launch worker for task 60] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:23.909 INFO  [Executor task launch worker for task 58] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:08:23.944 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 401
20:08:23.946 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 400
20:08:23.946 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 402
20:08:23.948 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.32.1:14777 in memory (size: 26.2 KB, free: 1988.5 MB)
20:08:23.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 406
20:08:23.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 407
20:08:23.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 456
20:08:23.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 399
20:08:23.949 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 403
20:08:23.950 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 6
20:08:23.952 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 5
20:08:23.965 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:23.965 INFO  [Executor task launch worker for task 57] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:23.966 INFO  [Executor task launch worker for task 60] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:23.967 INFO  [Executor task launch worker for task 59] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:23.967 INFO  [Executor task launch worker for task 60] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:23.967 INFO  [Executor task launch worker for task 59] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:23.968 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.32.1:14777 in memory (size: 62.0 KB, free: 1988.5 MB)
20:08:23.968 INFO  [Executor task launch worker for task 58] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:08:23.969 INFO  [Executor task launch worker for task 58] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:08:23.972 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7db48794
20:08:23.972 INFO  [Executor task launch worker for task 58] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65f68766
20:08:23.972 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:23.972 INFO  [Executor task launch worker for task 58] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:23.972 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200823_0022_m_000000_0/bdp_day=20190923/part-00000-11269de4-04b4-4795-b146-2e84393028c1.c000
20:08:23.972 INFO  [Executor task launch worker for task 58] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200823_0022_m_000001_0/bdp_day=20190923/part-00001-11269de4-04b4-4795-b146-2e84393028c1.c000
20:08:23.973 INFO  [Executor task launch worker for task 58] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:23.973 INFO  [Executor task launch worker for task 58] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:23.973 INFO  [Executor task launch worker for task 58] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:23.973 INFO  [Executor task launch worker for task 58] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:23.973 INFO  [Executor task launch worker for task 58] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:23.973 INFO  [Executor task launch worker for task 58] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:23.974 INFO  [Executor task launch worker for task 58] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:23.974 INFO  [Executor task launch worker for task 58] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:23.975 INFO  [Executor task launch worker for task 59] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@49d977e8
20:08:23.975 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 192.168.32.1:14777 in memory (size: 7.5 KB, free: 1988.6 MB)
20:08:23.975 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:23.976 INFO  [Executor task launch worker for task 60] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@112bf8d4
20:08:23.978 INFO  [Executor task launch worker for task 59] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:23.978 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:23.979 INFO  [Executor task launch worker for task 58] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@76b61e4a
20:08:23.979 INFO  [Executor task launch worker for task 60] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:08:23.980 INFO  [Executor task launch worker for task 59] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200823_0022_m_000002_0/bdp_day=20190923/part-00002-11269de4-04b4-4795-b146-2e84393028c1.c000
20:08:23.980 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:23.980 INFO  [Executor task launch worker for task 60] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200823_0022_m_000003_0/bdp_day=20190923/part-00003-11269de4-04b4-4795-b146-2e84393028c1.c000
20:08:23.980 INFO  [Executor task launch worker for task 59] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:23.980 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:23.980 INFO  [Executor task launch worker for task 60] parquet.hadoop.codec.CodecConfig - Compression set to false
20:08:23.980 INFO  [Executor task launch worker for task 59] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:23.980 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:23.980 INFO  [Executor task launch worker for task 60] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:08:23.980 INFO  [Executor task launch worker for task 59] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:23.980 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:23.980 INFO  [Executor task launch worker for task 60] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:08:23.980 INFO  [Executor task launch worker for task 59] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:23.980 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:23.980 INFO  [Executor task launch worker for task 60] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:08:23.980 INFO  [Executor task launch worker for task 59] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:23.980 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:23.980 INFO  [Executor task launch worker for task 60] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:08:23.981 INFO  [Executor task launch worker for task 59] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:23.981 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 192.168.32.1:14777 in memory (size: 7.5 KB, free: 1988.6 MB)
20:08:23.982 INFO  [Executor task launch worker for task 59] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:23.982 INFO  [Executor task launch worker for task 59] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:23.983 INFO  [Executor task launch worker for task 60] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:08:23.983 INFO  [Executor task launch worker for task 60] parquet.hadoop.ParquetOutputFormat - Validation is off
20:08:23.983 INFO  [Executor task launch worker for task 60] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:08:23.987 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.32.1:14777 in memory (size: 26.8 KB, free: 1988.6 MB)
20:08:23.988 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 404
20:08:23.989 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.32.1:14777 in memory (size: 2.3 KB, free: 1988.6 MB)
20:08:23.991 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 192.168.32.1:14777 in memory (size: 26.2 KB, free: 1988.6 MB)
20:08:23.992 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 405
20:08:23.994 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@7da72c55
20:08:23.995 INFO  [Executor task launch worker for task 59] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@162fb0a9
20:08:23.996 INFO  [Executor task launch worker for task 60] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3e2b2bf4
20:08:24.125 INFO  [Executor task launch worker for task 59] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,172
20:08:24.125 INFO  [Executor task launch worker for task 57] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,209
20:08:24.133 INFO  [Executor task launch worker for task 60] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,353
20:08:24.137 INFO  [Executor task launch worker for task 58] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,911
20:08:24.143 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.144 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.146 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.148 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:24.148 INFO  [Executor task launch worker for task 59] parquet.hadoop.ColumnChunkPageWriteStore - written 290,389B for [release_session] BINARY: 10,752 values, 290,312B raw, 290,312B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.148 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:24.148 INFO  [Executor task launch worker for task 59] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.148 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.149 INFO  [Executor task launch worker for task 59] parquet.hadoop.ColumnChunkPageWriteStore - written 107,571B for [device_num] BINARY: 10,752 values, 107,528B raw, 107,528B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.149 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,403 entries, 19,224B raw, 2,403B comp}
20:08:24.149 INFO  [Executor task launch worker for task 59] parquet.hadoop.ColumnChunkPageWriteStore - written 2,751B for [device_type] BINARY: 10,752 values, 2,720B raw, 2,720B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:24.151 INFO  [Executor task launch worker for task 59] parquet.hadoop.ColumnChunkPageWriteStore - written 4,105B for [sources] BINARY: 10,752 values, 4,063B raw, 4,063B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:24.152 INFO  [Executor task launch worker for task 59] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.152 INFO  [Executor task launch worker for task 59] parquet.hadoop.ColumnChunkPageWriteStore - written 16,206B for [ct] INT64: 10,752 values, 16,159B raw, 16,159B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,404 entries, 19,232B raw, 2,404B comp}
20:08:24.159 INFO  [Executor task launch worker for task 58] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.159 INFO  [Executor task launch worker for task 60] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.159 INFO  [Executor task launch worker for task 58] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.160 INFO  [Executor task launch worker for task 60] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.165 INFO  [Executor task launch worker for task 58] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.166 INFO  [Executor task launch worker for task 60] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:08:24.166 INFO  [Executor task launch worker for task 60] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:24.167 INFO  [Executor task launch worker for task 60] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:24.168 INFO  [Executor task launch worker for task 58] parquet.hadoop.ColumnChunkPageWriteStore - written 2,751B for [device_type] BINARY: 10,753 values, 2,720B raw, 2,720B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:08:24.168 INFO  [Executor task launch worker for task 58] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:08:24.168 INFO  [Executor task launch worker for task 58] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.172 INFO  [Executor task launch worker for task 60] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:08:24.172 INFO  [Executor task launch worker for task 60] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,426 entries, 19,408B raw, 2,426B comp}
20:08:24.174 INFO  [Executor task launch worker for task 58] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2,412 entries, 19,296B raw, 2,412B comp}
20:08:24.198 INFO  [Executor task launch worker for task 59] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200823_0022_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/task_20190925200823_0022_m_000002
20:08:24.198 INFO  [Executor task launch worker for task 59] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200823_0022_m_000002_0: Committed
20:08:24.199 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 59). 2568 bytes result sent to driver
20:08:24.200 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 59) in 313 ms on localhost (executor driver) (1/4)
20:08:24.200 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200823_0022_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/task_20190925200823_0022_m_000000
20:08:24.200 INFO  [Executor task launch worker for task 57] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200823_0022_m_000000_0: Committed
20:08:24.200 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 57). 2525 bytes result sent to driver
20:08:24.201 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 57) in 314 ms on localhost (executor driver) (2/4)
20:08:24.208 INFO  [Executor task launch worker for task 58] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200823_0022_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/task_20190925200823_0022_m_000001
20:08:24.208 INFO  [Executor task launch worker for task 58] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200823_0022_m_000001_0: Committed
20:08:24.208 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 58). 2525 bytes result sent to driver
20:08:24.209 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 58) in 322 ms on localhost (executor driver) (3/4)
20:08:24.217 INFO  [Executor task launch worker for task 60] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200823_0022_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/_temporary/0/task_20190925200823_0022_m_000003
20:08:24.217 INFO  [Executor task launch worker for task 60] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200823_0022_m_000003_0: Committed
20:08:24.217 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 60). 2525 bytes result sent to driver
20:08:24.218 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 60) in 330 ms on localhost (executor driver) (4/4)
20:08:24.218 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
20:08:24.219 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 22 (insertInto at SparkHelper.scala:35) finished in 0.332 s
20:08:24.220 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 12 finished: insertInto at SparkHelper.scala:35, took 0.628006 s
20:08:24.259 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:08:24.259 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:24.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:24.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:24.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.287 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:24.288 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:24.288 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:24.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:24.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:24.324 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]
20:08:24.325 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]	
20:08:24.346 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/bdp_day=20190923/part-00000-11269de4-04b4-4795-b146-2e84393028c1.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00000-11269de4-04b4-4795-b146-2e84393028c1.c000, Status:true
20:08:24.356 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/bdp_day=20190923/part-00001-11269de4-04b4-4795-b146-2e84393028c1.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00001-11269de4-04b4-4795-b146-2e84393028c1.c000, Status:true
20:08:24.369 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/bdp_day=20190923/part-00002-11269de4-04b4-4795-b146-2e84393028c1.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00002-11269de4-04b4-4795-b146-2e84393028c1.c000, Status:true
20:08:24.384 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/bdp_day=20190923/part-00003-11269de4-04b4-4795-b146-2e84393028c1.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00003-11269de4-04b4-4795-b146-2e84393028c1.c000, Status:true
20:08:24.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]
20:08:24.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]	
20:08:24.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_click[20190923]
20:08:24.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_click[20190923]	
20:08:24.417 WARN  [main] hive.log - Updating partition stats fast for: dw_release_click
20:08:24.419 WARN  [main] hive.log - Updated size to 1765246
20:08:24.521 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-25_20-08-23_406_7436147689746167321-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
20:08:24.526 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
20:08:24.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:08:24.526 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:08:24.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:24.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:24.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
20:08:24.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
20:08:24.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:08:24.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:08:24.574 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
20:08:24.583 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:08:24.585 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
20:08:24.595 INFO  [dispatcher-event-loop-6] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:08:24.771 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:08:24.772 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
20:08:24.773 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:08:24.775 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:08:24.780 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:08:24.780 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:08:24.781 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-4d30ff67-2459-4b24-b192-44b974a29b91
20:09:09.906 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:09:10.188 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
20:09:10.207 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
20:09:10.208 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
20:09:10.208 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:09:10.208 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:09:10.209 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
20:09:11.019 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 1085.
20:09:11.039 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:09:11.055 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:09:11.058 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:09:11.058 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:09:11.067 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-721e7378-7e97-49fa-bfcf-d4f6b8f533b1
20:09:11.084 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
20:09:11.128 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:09:11.206 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3798ms
20:09:11.268 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:09:11.282 INFO  [main] org.spark_project.jetty.server.Server - Started @3875ms
20:09:11.305 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:09:11.305 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:09:11.330 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
20:09:11.331 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
20:09:11.332 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
20:09:11.333 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
20:09:11.334 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
20:09:11.335 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
20:09:11.335 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
20:09:11.337 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
20:09:11.337 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
20:09:11.338 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
20:09:11.339 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
20:09:11.339 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
20:09:11.340 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
20:09:11.341 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
20:09:11.341 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
20:09:11.342 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
20:09:11.343 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
20:09:11.344 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
20:09:11.345 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
20:09:11.345 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:09:11.352 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
20:09:11.353 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
20:09:11.354 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
20:09:11.354 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
20:09:11.355 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
20:09:11.357 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
20:09:11.428 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:09:11.455 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1126.
20:09:11.456 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:1126
20:09:11.457 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:09:11.490 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 1126, None)
20:09:11.493 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:1126 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 1126, None)
20:09:11.496 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 1126, None)
20:09:11.497 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 1126, None)
20:09:11.666 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
20:09:13.086 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
20:09:13.110 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
20:09:13.111 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
20:09:13.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
20:09:13.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
20:09:13.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
20:09:13.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
20:09:13.120 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
20:09:13.561 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:09:14.218 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:09:14.246 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:09:15.389 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:09:16.550 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:09:16.552 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:09:16.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:09:16.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:09:16.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:09:16.918 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:09:16.920 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
20:09:16.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:09:16.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:09:16.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:09:16.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:09:16.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:09:16.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:09:16.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:09:16.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:09:17.479 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/f6a6d6de-d70b-4212-98b4-5f81c2b6643c_resources
20:09:17.500 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f6a6d6de-d70b-4212-98b4-5f81c2b6643c
20:09:17.503 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/f6a6d6de-d70b-4212-98b4-5f81c2b6643c
20:09:17.507 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f6a6d6de-d70b-4212-98b4-5f81c2b6643c/_tmp_space.db
20:09:17.511 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:09:17.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:09:17.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:09:17.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:09:17.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:09:17.545 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:09:17.697 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/15f5ea05-5ecc-464f-95b5-501818092611_resources
20:09:17.701 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/15f5ea05-5ecc-464f-95b5-501818092611
20:09:17.705 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/15f5ea05-5ecc-464f-95b5-501818092611
20:09:17.710 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/15f5ea05-5ecc-464f-95b5-501818092611/_tmp_space.db
20:09:17.713 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:09:17.757 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:09:17.762 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:09:17.947 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:17.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:17.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:17.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:18.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:18.056 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:18.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.099 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:18.115 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:09:18.437 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:09:18.451 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:09:18.451 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:09:18.452 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:09:18.452 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:09:18.453 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:09:18.453 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:09:18.454 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:09:18.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:18.644 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:18.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:18.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:18.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:18.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:18.698 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.698 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.699 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.700 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:18.700 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:18.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:18.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:18.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:18.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:18.821 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
20:09:18.823 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
20:09:18.825 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:18.835 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:18.839 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:09:19.159 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 193.632153 ms
20:09:19.396 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 44.022948 ms
20:09:19.497 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
20:09:19.592 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
20:09:19.596 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.7 MB)
20:09:19.600 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw03ReleaseExposure.scala:65
20:09:19.608 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:09:19.689 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
20:09:19.704 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at Dw03ReleaseExposure.scala:65)
20:09:19.706 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
20:09:19.707 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw03ReleaseExposure.scala:65)
20:09:19.707 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:09:19.709 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:09:19.712 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:19.751 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
20:09:19.754 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
20:09:19.755 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:1126 (size: 2.2 KB, free: 1988.7 MB)
20:09:19.755 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
20:09:19.766 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
20:09:19.766 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
20:09:19.803 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:09:19.812 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
20:09:19.879 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:19.881 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
20:09:19.900 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
20:09:19.910 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 115 ms on localhost (executor driver) (1/1)
20:09:19.915 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
20:09:19.919 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw03ReleaseExposure.scala:65) finished in 0.134 s
20:09:19.924 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw03ReleaseExposure.scala:65, took 0.233755 s
20:09:19.930 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
20:09:19.932 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw03ReleaseExposure.scala:65) with 3 output partitions
20:09:19.932 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw03ReleaseExposure.scala:65)
20:09:19.932 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
20:09:19.932 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:09:19.932 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:19.934 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
20:09:19.938 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
20:09:19.940 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:1126 (size: 2.2 KB, free: 1988.7 MB)
20:09:19.941 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:09:19.942 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(1, 2, 3))
20:09:19.942 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
20:09:19.943 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:09:19.944 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:09:19.945 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:09:19.945 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
20:09:19.946 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
20:09:19.946 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
20:09:19.951 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:19.951 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:19.951 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:19.951 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:19.952 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:19.952 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:19.953 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1225 bytes result sent to driver
20:09:19.953 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1225 bytes result sent to driver
20:09:19.953 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
20:09:19.955 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 11 ms on localhost (executor driver) (1/3)
20:09:19.956 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 11 ms on localhost (executor driver) (2/3)
20:09:19.956 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 13 ms on localhost (executor driver) (3/3)
20:09:19.956 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
20:09:19.957 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw03ReleaseExposure.scala:65) finished in 0.014 s
20:09:19.957 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw03ReleaseExposure.scala:65, took 0.026509 s
20:09:19.972 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:09:19.979 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:19.979 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:20.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:20.090 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-20_088_4802473475145453635-1
20:09:20.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:20.283 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:20.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:20.317 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:20.319 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
20:09:20.319 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
20:09:20.319 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:09:20.319 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
20:09:20.319 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
20:09:20.338 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:1126 in memory (size: 2.2 KB, free: 1988.7 MB)
20:09:20.341 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
20:09:20.341 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
20:09:20.344 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
20:09:20.346 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:1126 in memory (size: 2.2 KB, free: 1988.7 MB)
20:09:20.347 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
20:09:20.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:20.348 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.7 MB)
20:09:20.349 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:20.349 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
20:09:20.372 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.372 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.372 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.372 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:20.373 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:20.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:20.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:20.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:20.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:20.401 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
20:09:20.402 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
20:09:20.402 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:20.402 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:20.403 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:09:20.415 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:20.416 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:20.484 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
20:09:20.508 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
20:09:20.509 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.7 MB)
20:09:20.510 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
20:09:20.510 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:09:20.579 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:09:20.580 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:35)
20:09:20.580 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:09:20.580 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
20:09:20.580 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
20:09:20.580 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:09:20.580 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:20.647 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 167.4 KB, free 1988.2 MB)
20:09:20.649 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1988.1 MB)
20:09:20.651 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:1126 (size: 62.1 KB, free: 1988.6 MB)
20:09:20.652 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
20:09:20.652 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:09:20.652 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
20:09:20.653 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:09:20.654 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:09:20.654 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:09:20.654 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:09:20.655 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
20:09:20.655 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
20:09:20.655 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
20:09:20.655 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
20:09:20.702 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:20.702 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:20.702 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:20.702 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:20.703 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:20.703 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:20.703 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:20.703 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:20.722 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.850578 ms
20:09:20.747 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.758626 ms
20:09:20.766 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:20.766 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:20.766 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:20.767 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:20.768 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:20.768 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:20.768 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:20.768 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:20.779 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.987067 ms
20:09:20.830 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.584613 ms
20:09:20.846 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.769985 ms
20:09:20.855 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200920_0005_m_000000_0
20:09:20.855 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200920_0005_m_000001_0
20:09:20.855 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200920_0005_m_000003_0
20:09:20.855 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200920_0005_m_000002_0
20:09:20.858 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2498 bytes result sent to driver
20:09:20.858 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2498 bytes result sent to driver
20:09:20.858 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2498 bytes result sent to driver
20:09:20.859 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2498 bytes result sent to driver
20:09:20.860 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 206 ms on localhost (executor driver) (1/4)
20:09:20.860 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 207 ms on localhost (executor driver) (2/4)
20:09:20.860 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 206 ms on localhost (executor driver) (3/4)
20:09:20.861 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 208 ms on localhost (executor driver) (4/4)
20:09:20.861 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
20:09:20.862 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.209 s
20:09:20.863 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.284333 s
20:09:21.024 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:1126 in memory (size: 62.1 KB, free: 1988.7 MB)
20:09:21.024 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
20:09:21.044 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:09:21.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.093 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.093 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.093 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:21.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.128 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
20:09:21.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.176 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
20:09:21.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:09:21.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:09:21.184 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.184 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.232 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.232 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.232 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.233 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.233 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.233 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.233 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.234 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:21.265 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:09:21.265 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:21.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:21.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:21.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:21.298 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:21.298 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:21.325 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.326 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.326 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.326 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.326 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.327 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.327 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.327 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.327 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.327 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:21.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:09:21.340 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:09:21.340 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:09:21.340 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:09:21.340 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:09:21.340 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:09:21.340 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:09:21.341 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:09:21.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:21.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:21.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:21.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:21.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:21.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:21.426 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.426 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.427 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.428 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:21.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:21.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:21.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:21.430 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:21.455 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
20:09:21.456 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 03)
20:09:21.456 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:21.456 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:21.457 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:09:21.473 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 311.6 KB, free 1988.1 MB)
20:09:21.493 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
20:09:21.494 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.6 MB)
20:09:21.495 INFO  [main] org.apache.spark.SparkContext - Created broadcast 5 from show at Dw03ReleaseExposure.scala:65
20:09:21.495 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:09:21.506 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
20:09:21.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (show at Dw03ReleaseExposure.scala:65)
20:09:21.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
20:09:21.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (show at Dw03ReleaseExposure.scala:65)
20:09:21.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
20:09:21.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:09:21.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:21.508 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
20:09:21.511 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
20:09:21.512 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:1126 (size: 2.2 KB, free: 1988.6 MB)
20:09:21.512 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
20:09:21.513 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
20:09:21.513 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
20:09:21.515 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:09:21.515 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 8)
20:09:21.519 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.519 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:21.520 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 8). 1182 bytes result sent to driver
20:09:21.521 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 8) in 7 ms on localhost (executor driver) (1/1)
20:09:21.521 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
20:09:21.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (show at Dw03ReleaseExposure.scala:65) finished in 0.007 s
20:09:21.522 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw03ReleaseExposure.scala:65, took 0.015876 s
20:09:21.524 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
20:09:21.525 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at Dw03ReleaseExposure.scala:65) with 3 output partitions
20:09:21.525 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at Dw03ReleaseExposure.scala:65)
20:09:21.525 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
20:09:21.525 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:09:21.525 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:21.527 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
20:09:21.529 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
20:09:21.532 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:1126 (size: 2.2 KB, free: 1988.6 MB)
20:09:21.532 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
20:09:21.533 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(1, 2, 3))
20:09:21.533 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 3 tasks
20:09:21.534 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:09:21.534 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:09:21.534 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:09:21.534 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 10)
20:09:21.534 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 9)
20:09:21.534 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 11)
20:09:21.536 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.536 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.536 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:21.536 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:21.536 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.537 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:21.537 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 10). 1182 bytes result sent to driver
20:09:21.537 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 11). 1182 bytes result sent to driver
20:09:21.537 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 9). 1182 bytes result sent to driver
20:09:21.538 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 10) in 4 ms on localhost (executor driver) (1/3)
20:09:21.538 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 11) in 4 ms on localhost (executor driver) (2/3)
20:09:21.539 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 9) in 6 ms on localhost (executor driver) (3/3)
20:09:21.539 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
20:09:21.539 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at Dw03ReleaseExposure.scala:65) finished in 0.006 s
20:09:21.540 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at Dw03ReleaseExposure.scala:65, took 0.015607 s
20:09:21.546 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:09:21.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:21.574 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-21_574_8919680232363547030-1
20:09:21.663 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:21.663 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:21.672 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:21.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:21.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:21.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:21.715 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.715 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.715 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.715 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.716 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.716 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.716 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.716 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.716 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.716 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:21.718 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:21.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:21.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:21.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:21.743 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
20:09:21.743 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 03)
20:09:21.743 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:21.743 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:21.744 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
20:09:21.750 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:21.751 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:21.796 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 311.6 KB, free 1987.7 MB)
20:09:21.809 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.7 MB)
20:09:21.810 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.6 MB)
20:09:21.811 INFO  [main] org.apache.spark.SparkContext - Created broadcast 8 from insertInto at SparkHelper.scala:35
20:09:21.811 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
20:09:21.851 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:09:21.852 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (insertInto at SparkHelper.scala:35)
20:09:21.852 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:09:21.852 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (insertInto at SparkHelper.scala:35)
20:09:21.852 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
20:09:21.853 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:09:21.853 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[24] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:21.884 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 167.4 KB, free 1987.5 MB)
20:09:21.888 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.5 MB)
20:09:21.889 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:1126 (size: 62.1 KB, free: 1988.6 MB)
20:09:21.890 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
20:09:21.891 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 11 (MapPartitionsRDD[24] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:09:21.891 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 4 tasks
20:09:21.891 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
20:09:21.892 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
20:09:21.892 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
20:09:21.893 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
20:09:21.893 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 13)
20:09:21.893 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 15)
20:09:21.893 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 12)
20:09:21.893 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 14)
20:09:21.916 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.916 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:21.916 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.916 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:21.919 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.919 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:21.920 INFO  [Executor task launch worker for task 15] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:21.920 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
20:09:21.920 INFO  [Executor task launch worker for task 14] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:21.920 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:21.920 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:21.921 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:21.921 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:21.922 INFO  [Executor task launch worker for task 13] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:21.922 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:21.923 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:21.924 INFO  [Executor task launch worker for task 14] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200921_0011_m_000002_0
20:09:21.924 INFO  [Executor task launch worker for task 15] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200921_0011_m_000003_0
20:09:21.926 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 14). 2412 bytes result sent to driver
20:09:21.926 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 15). 2455 bytes result sent to driver
20:09:21.926 INFO  [Executor task launch worker for task 13] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200921_0011_m_000001_0
20:09:21.926 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925200921_0011_m_000000_0
20:09:21.926 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 14) in 34 ms on localhost (executor driver) (1/4)
20:09:21.927 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 15) in 34 ms on localhost (executor driver) (2/4)
20:09:21.927 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 13). 2455 bytes result sent to driver
20:09:21.927 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 12). 2455 bytes result sent to driver
20:09:21.928 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 13) in 36 ms on localhost (executor driver) (3/4)
20:09:21.928 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 12) in 37 ms on localhost (executor driver) (4/4)
20:09:21.928 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
20:09:21.929 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (insertInto at SparkHelper.scala:35) finished in 0.037 s
20:09:21.929 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: insertInto at SparkHelper.scala:35, took 0.078272 s
20:09:21.940 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:09:21.940 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.941 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.964 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:21.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.991 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.992 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.992 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.992 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.992 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.992 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:21.992 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:21.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:21.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:22.021 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
20:09:22.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:22.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:22.041 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
20:09:22.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:09:22.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:09:22.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:22.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:22.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:22.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:22.078 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.079 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.079 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.079 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.079 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.079 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.079 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:22.093 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:09:22.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:22.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:22.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:22.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:22.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:22.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:22.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:22.153 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:09:22.153 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:09:22.153 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:09:22.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:09:22.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:09:22.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:09:22.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:09:22.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:09:22.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:22.188 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:22.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:22.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:22.215 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:22.215 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:22.233 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.233 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.234 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.234 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.234 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.234 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.234 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.235 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.235 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:22.235 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:22.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:22.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:22.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:22.237 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:22.294 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
20:09:22.295 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 03)
20:09:22.295 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:22.295 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:22.295 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:09:22.309 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 311.6 KB, free 1987.2 MB)
20:09:22.323 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
20:09:22.325 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.5 MB)
20:09:22.325 INFO  [main] org.apache.spark.SparkContext - Created broadcast 10 from show at Dw03ReleaseExposure.scala:65
20:09:22.326 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:09:22.348 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
20:09:22.348 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 29 (show at Dw03ReleaseExposure.scala:65)
20:09:22.349 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
20:09:22.349 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (show at Dw03ReleaseExposure.scala:65)
20:09:22.349 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
20:09:22.349 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
20:09:22.350 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[29] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:22.361 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 17.6 KB, free 1987.1 MB)
20:09:22.363 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.1 MB)
20:09:22.365 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:1126 (size: 7.5 KB, free: 1988.5 MB)
20:09:22.365 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
20:09:22.367 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[29] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:09:22.368 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 8 tasks
20:09:22.374 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:09:22.374 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 17, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:09:22.374 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 18, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:09:22.375 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 19, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:09:22.375 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 20, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:09:22.375 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 21, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:09:22.375 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 22, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:09:22.375 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 23, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:09:22.376 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 17)
20:09:22.376 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)
20:09:22.376 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 18)
20:09:22.376 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 19)
20:09:22.378 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 21)
20:09:22.378 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 22)
20:09:22.379 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 23)
20:09:22.379 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 20)
20:09:22.397 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
20:09:22.397 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
20:09:22.397 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
20:09:22.397 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
20:09:22.397 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
20:09:22.397 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
20:09:22.397 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
20:09:22.397 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
20:09:23.624 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.624 INFO  [Executor task launch worker for task 17] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.624 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.624 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.624 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.624 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.624 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.655 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:23.686 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 20). 1414 bytes result sent to driver
20:09:23.686 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 23). 1414 bytes result sent to driver
20:09:23.688 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 23) in 1313 ms on localhost (executor driver) (1/8)
20:09:23.688 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 20) in 1313 ms on localhost (executor driver) (2/8)
20:09:23.688 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 22). 1371 bytes result sent to driver
20:09:23.689 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 22) in 1314 ms on localhost (executor driver) (3/8)
20:09:23.693 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 18). 1371 bytes result sent to driver
20:09:23.694 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 18) in 1320 ms on localhost (executor driver) (4/8)
20:09:23.696 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 1371 bytes result sent to driver
20:09:23.697 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 1326 ms on localhost (executor driver) (5/8)
20:09:23.699 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 21). 1371 bytes result sent to driver
20:09:23.700 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 21) in 1325 ms on localhost (executor driver) (6/8)
20:09:23.703 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 17). 1371 bytes result sent to driver
20:09:23.704 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 17) in 1330 ms on localhost (executor driver) (7/8)
20:09:24.338 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 19). 1543 bytes result sent to driver
20:09:24.338 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 19) in 1964 ms on localhost (executor driver) (8/8)
20:09:24.339 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
20:09:24.339 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (show at Dw03ReleaseExposure.scala:65) finished in 1.968 s
20:09:24.340 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:09:24.340 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:09:24.340 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
20:09:24.341 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:09:24.343 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[31] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:24.345 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 1987.1 MB)
20:09:24.346 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.1 MB)
20:09:24.347 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:1126 (size: 2.3 KB, free: 1988.5 MB)
20:09:24.347 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
20:09:24.348 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
20:09:24.348 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks
20:09:24.348 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 24, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:09:24.349 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 24)
20:09:24.352 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:24.352 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:24.367 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 24). 4663 bytes result sent to driver
20:09:24.368 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 24) in 20 ms on localhost (executor driver) (1/1)
20:09:24.368 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
20:09:24.368 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (show at Dw03ReleaseExposure.scala:65) finished in 0.020 s
20:09:24.369 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: show at Dw03ReleaseExposure.scala:65, took 2.020459 s
20:09:24.382 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:09:24.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:24.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:24.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:24.405 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1
20:09:24.471 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:24.471 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:24.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:24.476 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:24.497 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:24.497 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:24.514 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.514 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:24.515 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:24.517 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:24.517 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:24.517 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:24.517 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:24.548 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
20:09:24.548 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 03)
20:09:24.549 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:24.549 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:24.549 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:09:24.554 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:24.554 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:24.567 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 311.6 KB, free 1986.8 MB)
20:09:24.584 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1986.8 MB)
20:09:24.586 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.5 MB)
20:09:24.587 INFO  [main] org.apache.spark.SparkContext - Created broadcast 13 from insertInto at SparkHelper.scala:35
20:09:24.587 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:09:24.634 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:09:24.635 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 35 (insertInto at SparkHelper.scala:35)
20:09:24.635 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 7 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:09:24.635 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (insertInto at SparkHelper.scala:35)
20:09:24.635 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
20:09:24.635 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
20:09:24.636 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (MapPartitionsRDD[35] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:24.638 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 17.6 KB, free 1986.8 MB)
20:09:24.640 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1986.8 MB)
20:09:24.641 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:1126 (size: 7.5 KB, free: 1988.5 MB)
20:09:24.641 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1004
20:09:24.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[35] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:09:24.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 8 tasks
20:09:24.642 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 25, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:09:24.642 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 26, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:09:24.643 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 27, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:09:24.643 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 28, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:09:24.643 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 29, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:09:24.643 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 30, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:09:24.643 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 31, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:09:24.644 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 32, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:09:24.644 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 25)
20:09:24.644 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 26)
20:09:24.644 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 27)
20:09:24.644 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 28)
20:09:24.644 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 29)
20:09:24.644 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 30)
20:09:24.644 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 31)
20:09:24.644 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 32)
20:09:24.648 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
20:09:24.648 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
20:09:24.649 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
20:09:24.649 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
20:09:24.651 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
20:09:24.652 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
20:09:24.652 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
20:09:24.655 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
20:09:24.664 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.665 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.682 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.688 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.691 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.692 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 27). 1371 bytes result sent to driver
20:09:24.692 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.692 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.692 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 27) in 50 ms on localhost (executor driver) (1/8)
20:09:24.694 INFO  [Executor task launch worker for task 26] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:24.697 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 25). 1371 bytes result sent to driver
20:09:24.698 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 25) in 56 ms on localhost (executor driver) (2/8)
20:09:24.716 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 32). 1371 bytes result sent to driver
20:09:24.721 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 30). 1328 bytes result sent to driver
20:09:24.725 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 30) in 82 ms on localhost (executor driver) (3/8)
20:09:24.726 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 32) in 82 ms on localhost (executor driver) (4/8)
20:09:24.726 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 29). 1414 bytes result sent to driver
20:09:24.727 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 29) in 84 ms on localhost (executor driver) (5/8)
20:09:24.737 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 31). 1371 bytes result sent to driver
20:09:24.738 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 31) in 95 ms on localhost (executor driver) (6/8)
20:09:24.740 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 26). 1371 bytes result sent to driver
20:09:24.741 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 26) in 99 ms on localhost (executor driver) (7/8)
20:09:25.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 218
20:09:25.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 177
20:09:25.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 121
20:09:25.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 214
20:09:25.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 123
20:09:25.247 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
20:09:25.247 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 180
20:09:25.248 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:1126 in memory (size: 2.2 KB, free: 1988.5 MB)
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 215
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 217
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 216
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 210
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 267
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 182
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 117
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 174
20:09:25.249 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
20:09:25.251 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:1126 in memory (size: 2.2 KB, free: 1988.5 MB)
20:09:25.255 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:1126 in memory (size: 7.5 KB, free: 1988.5 MB)
20:09:25.256 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 179
20:09:25.257 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.5 MB)
20:09:25.260 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:1126 in memory (size: 62.1 KB, free: 1988.6 MB)
20:09:25.260 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 213
20:09:25.262 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.6 MB)
20:09:25.264 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:1126 in memory (size: 2.3 KB, free: 1988.6 MB)
20:09:25.264 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 181
20:09:25.266 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.6 MB)
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 212
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 118
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 183
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 125
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 175
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 119
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 185
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 124
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 184
20:09:25.267 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 211
20:09:25.268 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 122
20:09:25.268 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 120
20:09:25.268 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 178
20:09:25.268 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 176
20:09:25.382 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 28). 1586 bytes result sent to driver
20:09:25.383 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 28) in 740 ms on localhost (executor driver) (8/8)
20:09:25.383 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
20:09:25.383 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (insertInto at SparkHelper.scala:35) finished in 0.741 s
20:09:25.384 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:09:25.384 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:09:25.384 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
20:09:25.384 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:09:25.384 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[37] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:25.417 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 167.4 KB, free 1987.9 MB)
20:09:25.418 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.8 MB)
20:09:25.419 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:1126 (size: 62.1 KB, free: 1988.6 MB)
20:09:25.420 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1004
20:09:25.420 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:09:25.420 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 4 tasks
20:09:25.421 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 33, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:09:25.421 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 34, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:09:25.421 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 15.0 (TID 35, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:09:25.422 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 15.0 (TID 36, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:09:25.422 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 34)
20:09:25.422 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 2.0 in stage 15.0 (TID 35)
20:09:25.422 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 33)
20:09:25.422 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 3.0 in stage 15.0 (TID 36)
20:09:25.442 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:25.442 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:25.442 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:25.442 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:25.443 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:25.443 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:25.443 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:25.444 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:25.541 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:25.541 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:25.541 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:25.541 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:25.542 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:25.542 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:25.542 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:25.542 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:25.642 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42df5e94
20:09:25.643 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c930572
20:09:25.643 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5af154e1
20:09:25.643 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@74b085a8
20:09:25.656 INFO  [Executor task launch worker for task 34] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
20:09:25.662 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:25.662 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:25.662 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:25.662 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:25.662 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200925_0015_m_000003_0/bdp_day=20190922/part-00003-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
20:09:25.662 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200925_0015_m_000000_0/bdp_day=20190922/part-00000-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
20:09:25.662 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200925_0015_m_000002_0/bdp_day=20190922/part-00002-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
20:09:25.662 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200925_0015_m_000001_0/bdp_day=20190922/part-00001-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
20:09:25.665 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:25.665 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:25.665 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:25.665 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:25.666 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:25.665 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:25.666 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:25.666 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:25.667 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:25.667 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:25.667 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:25.667 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:25.667 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:25.667 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:25.667 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:25.667 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:25.667 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:25.667 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:25.668 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:25.668 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:25.668 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:25.668 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:25.668 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:25.668 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:25.668 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:25.853 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5d0fb4e9
20:09:25.853 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@74a648c2
20:09:25.853 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@702d6d1
20:09:25.854 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1c2b4af8
20:09:26.070 INFO  [Executor task launch worker for task 34] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,556
20:09:26.070 INFO  [Executor task launch worker for task 33] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,296
20:09:26.077 INFO  [Executor task launch worker for task 36] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,356
20:09:26.077 INFO  [Executor task launch worker for task 35] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,271
20:09:26.182 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.182 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.182 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.182 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.183 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.183 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.183 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.183 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.184 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.184 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.184 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.185 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,345 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:26.185 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:26.185 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:26.185 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:26.185 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:26.185 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:26.185 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.185 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.185 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.186 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:26.186 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:26.186 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:26.186 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:26.187 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:26.187 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:26.187 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:26.188 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:26.426 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200925_0015_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/task_20190925200925_0015_m_000003
20:09:26.426 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200925_0015_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/task_20190925200925_0015_m_000002
20:09:26.426 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200925_0015_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/task_20190925200925_0015_m_000001
20:09:26.426 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200925_0015_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/_temporary/0/task_20190925200925_0015_m_000000
20:09:26.427 INFO  [Executor task launch worker for task 33] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200925_0015_m_000000_0: Committed
20:09:26.427 INFO  [Executor task launch worker for task 36] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200925_0015_m_000003_0: Committed
20:09:26.427 INFO  [Executor task launch worker for task 35] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200925_0015_m_000002_0: Committed
20:09:26.427 INFO  [Executor task launch worker for task 34] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200925_0015_m_000001_0: Committed
20:09:26.428 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 3.0 in stage 15.0 (TID 36). 2525 bytes result sent to driver
20:09:26.428 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 34). 2525 bytes result sent to driver
20:09:26.428 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 2.0 in stage 15.0 (TID 35). 2525 bytes result sent to driver
20:09:26.428 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 33). 2525 bytes result sent to driver
20:09:26.429 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 15.0 (TID 36) in 1008 ms on localhost (executor driver) (1/4)
20:09:26.429 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 15.0 (TID 35) in 1008 ms on localhost (executor driver) (2/4)
20:09:26.430 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 34) in 1009 ms on localhost (executor driver) (3/4)
20:09:26.430 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 33) in 1009 ms on localhost (executor driver) (4/4)
20:09:26.430 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
20:09:26.430 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (insertInto at SparkHelper.scala:35) finished in 1.009 s
20:09:26.431 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 7 finished: insertInto at SparkHelper.scala:35, took 1.797080 s
20:09:26.539 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.32.1:1126 in memory (size: 62.1 KB, free: 1988.6 MB)
20:09:26.541 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 192.168.32.1:1126 in memory (size: 7.5 KB, free: 1988.6 MB)
20:09:26.547 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:09:26.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:26.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:26.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:26.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:26.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:26.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:26.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:26.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:26.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:26.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:26.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:26.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:26.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:26.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:26.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:26.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:26.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:26.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]
20:09:26.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]	
20:09:26.666 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20:09:26.680 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/bdp_day=20190922/part-00000-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00000-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, Status:true
20:09:26.696 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/bdp_day=20190922/part-00001-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00001-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, Status:true
20:09:26.708 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/bdp_day=20190922/part-00002-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00002-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, Status:true
20:09:26.722 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/bdp_day=20190922/part-00003-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00003-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000, Status:true
20:09:26.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]
20:09:26.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]	
20:09:26.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_exposure[20190922]
20:09:26.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_exposure[20190922]	
20:09:26.755 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
20:09:26.757 WARN  [main] hive.log - Updated size to 2286731
20:09:26.911 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-24_405_5757037956636505771-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
20:09:26.914 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
20:09:26.914 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:09:26.914 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:09:26.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:26.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:26.940 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:26.940 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:26.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:26.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:26.971 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:09:26.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:26.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:26.977 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:26.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:26.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:26.997 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:27.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:27.025 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:09:27.025 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:09:27.025 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:09:27.025 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:09:27.025 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:09:27.026 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:09:27.026 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:09:27.026 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:09:27.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:27.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:27.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:27.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:27.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:27.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:27.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.092 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:27.093 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:27.093 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:27.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:27.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:27.125 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
20:09:27.125 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 03)
20:09:27.125 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:27.125 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:27.126 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:09:27.137 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 311.6 KB, free 1987.7 MB)
20:09:27.151 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.7 MB)
20:09:27.152 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.6 MB)
20:09:27.152 INFO  [main] org.apache.spark.SparkContext - Created broadcast 16 from show at Dw03ReleaseExposure.scala:65
20:09:27.153 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:09:27.160 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
20:09:27.160 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 42 (show at Dw03ReleaseExposure.scala:65)
20:09:27.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 8 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
20:09:27.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (show at Dw03ReleaseExposure.scala:65)
20:09:27.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
20:09:27.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
20:09:27.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[42] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:27.165 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 17.6 KB, free 1987.7 MB)
20:09:27.167 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.7 MB)
20:09:27.168 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.32.1:1126 (size: 7.5 KB, free: 1988.6 MB)
20:09:27.168 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1004
20:09:27.169 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[42] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:09:27.169 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 8 tasks
20:09:27.169 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 37, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:09:27.169 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 38, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:09:27.170 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 39, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:09:27.170 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 40, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:09:27.170 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 41, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:09:27.171 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 42, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:09:27.171 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 43, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:09:27.171 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 16.0 (TID 44, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:09:27.171 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 39)
20:09:27.171 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 37)
20:09:27.171 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 38)
20:09:27.171 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 40)
20:09:27.171 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 41)
20:09:27.171 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 43)
20:09:27.171 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 42)
20:09:27.171 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 7.0 in stage 16.0 (TID 44)
20:09:27.174 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:09:27.175 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:09:27.175 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:09:27.176 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:09:27.176 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:09:27.178 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:09:27.178 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:09:27.180 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:09:27.189 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.193 INFO  [Executor task launch worker for task 39] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.196 INFO  [Executor task launch worker for task 37] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.207 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 38). 1371 bytes result sent to driver
20:09:27.207 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.208 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 38) in 39 ms on localhost (executor driver) (1/8)
20:09:27.209 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.217 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.217 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.220 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.224 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 39). 1328 bytes result sent to driver
20:09:27.231 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 37). 1371 bytes result sent to driver
20:09:27.235 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 42). 1371 bytes result sent to driver
20:09:27.236 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 39) in 66 ms on localhost (executor driver) (2/8)
20:09:27.237 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 37) in 68 ms on localhost (executor driver) (3/8)
20:09:27.237 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 42) in 67 ms on localhost (executor driver) (4/8)
20:09:27.241 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 7.0 in stage 16.0 (TID 44). 1371 bytes result sent to driver
20:09:27.242 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 16.0 (TID 44) in 70 ms on localhost (executor driver) (5/8)
20:09:27.245 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 41). 1371 bytes result sent to driver
20:09:27.245 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 41) in 75 ms on localhost (executor driver) (6/8)
20:09:27.248 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 43). 1371 bytes result sent to driver
20:09:27.249 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 43) in 78 ms on localhost (executor driver) (7/8)
20:09:27.533 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 40). 1586 bytes result sent to driver
20:09:27.533 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 40) in 363 ms on localhost (executor driver) (8/8)
20:09:27.533 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
20:09:27.534 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (show at Dw03ReleaseExposure.scala:65) finished in 0.364 s
20:09:27.534 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:09:27.534 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:09:27.534 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
20:09:27.534 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:09:27.534 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[44] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:27.535 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 1987.7 MB)
20:09:27.536 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.7 MB)
20:09:27.536 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.32.1:1126 (size: 2.3 KB, free: 1988.6 MB)
20:09:27.536 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1004
20:09:27.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[44] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
20:09:27.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks
20:09:27.537 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 45, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:09:27.537 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 45)
20:09:27.538 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:27.539 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:27.542 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 45). 4508 bytes result sent to driver
20:09:27.543 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 45) in 6 ms on localhost (executor driver) (1/1)
20:09:27.543 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
20:09:27.543 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (show at Dw03ReleaseExposure.scala:65) finished in 0.006 s
20:09:27.543 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 8 finished: show at Dw03ReleaseExposure.scala:65, took 0.383148 s
20:09:27.549 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:09:27.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:27.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:27.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:27.570 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1
20:09:27.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:27.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:27.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:27.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:27.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:27.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.625 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:27.625 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:27.625 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:27.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:27.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:27.626 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:27.649 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
20:09:27.650 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 03)
20:09:27.650 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:27.650 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:27.650 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:09:27.654 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:27.654 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:27.664 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 311.6 KB, free 1987.4 MB)
20:09:27.767 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.4 MB)
20:09:27.867 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.6 MB)
20:09:27.869 INFO  [main] org.apache.spark.SparkContext - Created broadcast 19 from insertInto at SparkHelper.scala:35
20:09:27.869 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:09:27.901 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:09:27.901 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 48 (insertInto at SparkHelper.scala:35)
20:09:27.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 9 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:09:27.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (insertInto at SparkHelper.scala:35)
20:09:27.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 18)
20:09:27.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 18)
20:09:27.902 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 18 (MapPartitionsRDD[48] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:27.904 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 17.6 KB, free 1987.3 MB)
20:09:27.905 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.3 MB)
20:09:27.906 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.32.1:1126 (size: 7.5 KB, free: 1988.6 MB)
20:09:27.906 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1004
20:09:27.907 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[48] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:09:27.907 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 8 tasks
20:09:27.907 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 46, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:09:27.907 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 47, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:09:27.907 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 18.0 (TID 48, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:09:27.907 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 18.0 (TID 49, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:09:27.907 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 18.0 (TID 50, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:09:27.908 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 18.0 (TID 51, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:09:27.908 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 18.0 (TID 52, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:09:27.908 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 18.0 (TID 53, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:09:27.908 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 46)
20:09:27.908 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 18.0 (TID 48)
20:09:27.908 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 18.0 (TID 49)
20:09:27.908 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 47)
20:09:27.908 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 4.0 in stage 18.0 (TID 50)
20:09:27.908 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 5.0 in stage 18.0 (TID 51)
20:09:27.908 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 7.0 in stage 18.0 (TID 53)
20:09:27.908 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 6.0 in stage 18.0 (TID 52)
20:09:27.910 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:09:27.910 INFO  [Executor task launch worker for task 51] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:09:27.910 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:09:27.910 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:09:27.911 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:09:27.911 INFO  [Executor task launch worker for task 52] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:09:27.912 INFO  [Executor task launch worker for task 50] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:09:27.912 INFO  [Executor task launch worker for task 53] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:09:27.923 INFO  [Executor task launch worker for task 49] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.930 INFO  [Executor task launch worker for task 50] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.930 INFO  [Executor task launch worker for task 46] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.932 INFO  [Executor task launch worker for task 47] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.936 INFO  [Executor task launch worker for task 52] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.940 INFO  [Executor task launch worker for task 51] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.940 INFO  [Executor task launch worker for task 48] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.942 INFO  [Executor task launch worker for task 53] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:27.949 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 6.0 in stage 18.0 (TID 52). 1328 bytes result sent to driver
20:09:27.950 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 18.0 (TID 52) in 42 ms on localhost (executor driver) (1/8)
20:09:27.971 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 7.0 in stage 18.0 (TID 53). 1328 bytes result sent to driver
20:09:27.971 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 18.0 (TID 53) in 63 ms on localhost (executor driver) (2/8)
20:09:27.996 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 46). 1328 bytes result sent to driver
20:09:27.997 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 46) in 90 ms on localhost (executor driver) (3/8)
20:09:28.001 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 18.0 (TID 48). 1371 bytes result sent to driver
20:09:28.002 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 18.0 (TID 48) in 95 ms on localhost (executor driver) (4/8)
20:09:28.005 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 5.0 in stage 18.0 (TID 51). 1328 bytes result sent to driver
20:09:28.006 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 18.0 (TID 51) in 99 ms on localhost (executor driver) (5/8)
20:09:28.010 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 4.0 in stage 18.0 (TID 50). 1371 bytes result sent to driver
20:09:28.011 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 18.0 (TID 50) in 104 ms on localhost (executor driver) (6/8)
20:09:28.014 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 47). 1371 bytes result sent to driver
20:09:28.015 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 47) in 108 ms on localhost (executor driver) (7/8)
20:09:28.166 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 18.0 (TID 49). 1543 bytes result sent to driver
20:09:28.167 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 18.0 (TID 49) in 260 ms on localhost (executor driver) (8/8)
20:09:28.167 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
20:09:28.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 18 (insertInto at SparkHelper.scala:35) finished in 0.260 s
20:09:28.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:09:28.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:09:28.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 19)
20:09:28.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:09:28.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[50] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:28.195 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 167.4 KB, free 1987.2 MB)
20:09:28.196 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.1 MB)
20:09:28.197 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.32.1:1126 (size: 62.1 KB, free: 1988.5 MB)
20:09:28.197 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1004
20:09:28.198 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 19 (MapPartitionsRDD[50] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:09:28.198 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 4 tasks
20:09:28.198 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 54, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:09:28.199 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 55, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:09:28.199 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 56, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:09:28.199 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 57, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:09:28.199 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 55)
20:09:28.199 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 54)
20:09:28.199 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 56)
20:09:28.199 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 57)
20:09:28.215 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:28.216 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:09:28.216 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:28.216 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:28.216 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:28.216 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:28.218 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:28.218 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:28.238 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:28.239 INFO  [Executor task launch worker for task 57] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:28.241 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:28.241 INFO  [Executor task launch worker for task 55] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:28.244 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:28.245 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:28.245 INFO  [Executor task launch worker for task 56] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:28.245 INFO  [Executor task launch worker for task 54] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:28.245 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d91bee8
20:09:28.245 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@55cd5dfc
20:09:28.245 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:28.246 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:28.246 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200928_0019_m_000003_0/bdp_day=20190923/part-00003-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
20:09:28.246 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200928_0019_m_000001_0/bdp_day=20190923/part-00001-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:28.246 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:28.246 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:28.246 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:28.247 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:28.247 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@351644bd
20:09:28.247 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:28.247 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57292ef2
20:09:28.247 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:28.247 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:28.247 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:28.247 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:28.247 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:28.247 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:28.247 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200928_0019_m_000000_0/bdp_day=20190923/part-00000-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
20:09:28.247 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200928_0019_m_000002_0/bdp_day=20190923/part-00002-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
20:09:28.247 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:28.247 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:28.247 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:28.247 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:28.247 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:28.247 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:28.247 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:28.247 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:28.247 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:28.248 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:28.248 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:28.248 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:28.248 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:28.248 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:28.249 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:28.249 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:28.256 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@7e4816e
20:09:28.263 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3ea1ea69
20:09:28.263 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@51efa5d
20:09:28.264 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5f5ab1ad
20:09:28.414 INFO  [Executor task launch worker for task 55] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,366
20:09:28.417 INFO  [Executor task launch worker for task 57] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,336
20:09:28.418 INFO  [Executor task launch worker for task 54] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,536
20:09:28.419 INFO  [Executor task launch worker for task 56] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,231
20:09:28.454 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.480 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.482 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.482 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:28.483 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:28.483 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.484 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:28.494 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.5 MB)
20:09:28.494 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.494 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.495 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.496 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:28.496 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:28.497 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.497 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:28.498 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.499 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.500 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.501 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,345 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:28.502 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:28.504 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.504 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.505 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.505 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:28.505 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:28.507 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 5
20:09:28.507 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:28.508 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:28.508 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:28.509 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.6 MB)
20:09:28.509 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:28.511 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 192.168.32.1:1126 in memory (size: 7.5 KB, free: 1988.6 MB)
20:09:28.519 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.32.1:1126 in memory (size: 7.5 KB, free: 1988.6 MB)
20:09:28.522 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 6
20:09:28.524 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.32.1:1126 in memory (size: 2.3 KB, free: 1988.6 MB)
20:09:28.549 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200928_0019_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/task_20190925200928_0019_m_000001
20:09:28.549 INFO  [Executor task launch worker for task 55] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200928_0019_m_000001_0: Committed
20:09:28.550 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 55). 2525 bytes result sent to driver
20:09:28.550 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 55) in 352 ms on localhost (executor driver) (1/4)
20:09:28.575 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200928_0019_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/task_20190925200928_0019_m_000000
20:09:28.575 INFO  [Executor task launch worker for task 54] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200928_0019_m_000000_0: Committed
20:09:28.576 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 54). 2525 bytes result sent to driver
20:09:28.576 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 54) in 378 ms on localhost (executor driver) (2/4)
20:09:28.578 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200928_0019_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/task_20190925200928_0019_m_000003
20:09:28.578 INFO  [Executor task launch worker for task 57] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200928_0019_m_000003_0: Committed
20:09:28.578 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 57). 2525 bytes result sent to driver
20:09:28.579 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 57) in 380 ms on localhost (executor driver) (3/4)
20:09:28.963 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200928_0019_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/_temporary/0/task_20190925200928_0019_m_000002
20:09:28.963 INFO  [Executor task launch worker for task 56] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200928_0019_m_000002_0: Committed
20:09:28.963 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 56). 2525 bytes result sent to driver
20:09:28.963 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 56) in 764 ms on localhost (executor driver) (4/4)
20:09:28.963 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
20:09:28.964 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 19 (insertInto at SparkHelper.scala:35) finished in 0.766 s
20:09:28.964 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 9 finished: insertInto at SparkHelper.scala:35, took 1.063272 s
20:09:28.993 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:09:28.993 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:28.993 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:29.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:29.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:29.027 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.028 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:29.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:29.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:29.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:29.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:29.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:29.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]
20:09:29.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]	
20:09:29.095 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/bdp_day=20190923/part-00000-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00000-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, Status:true
20:09:29.111 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/bdp_day=20190923/part-00001-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00001-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, Status:true
20:09:29.121 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/bdp_day=20190923/part-00002-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00002-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, Status:true
20:09:29.132 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/bdp_day=20190923/part-00003-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00003-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000, Status:true
20:09:29.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]
20:09:29.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]	
20:09:29.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_exposure[20190923]
20:09:29.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_exposure[20190923]	
20:09:29.162 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
20:09:29.164 WARN  [main] hive.log - Updated size to 2286732
20:09:29.239 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-27_570_2223461923667701400-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
20:09:29.240 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
20:09:29.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:09:29.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:09:29.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:29.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:29.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:29.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:29.278 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.279 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.279 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.279 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.279 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.279 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.279 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.279 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:29.289 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:09:29.290 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:29.290 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:29.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:29.296 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:29.314 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:29.314 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:29.332 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:09:29.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:09:29.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:29.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:29.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:29.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:29.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:29.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:29.404 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.406 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.406 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:29.407 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:29.407 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:29.407 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:29.408 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:29.441 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#391),(bdp_day#391 = 20190924)
20:09:29.441 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#384),(release_status#384 = 03)
20:09:29.441 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:29.442 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:29.442 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:09:29.452 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 311.6 KB, free 1987.5 MB)
20:09:29.470 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.5 MB)
20:09:29.470 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.6 MB)
20:09:29.471 INFO  [main] org.apache.spark.SparkContext - Created broadcast 22 from show at Dw03ReleaseExposure.scala:65
20:09:29.471 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:09:29.481 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
20:09:29.481 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 55 (show at Dw03ReleaseExposure.scala:65)
20:09:29.482 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 10 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
20:09:29.482 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (show at Dw03ReleaseExposure.scala:65)
20:09:29.482 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 20)
20:09:29.482 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 20)
20:09:29.482 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:29.484 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 17.6 KB, free 1987.5 MB)
20:09:29.486 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.5 MB)
20:09:29.487 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.32.1:1126 (size: 7.5 KB, free: 1988.6 MB)
20:09:29.487 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1004
20:09:29.488 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:09:29.488 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 8 tasks
20:09:29.489 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 58, localhost, executor driver, partition 0, ANY, 5391 bytes)
20:09:29.489 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 59, localhost, executor driver, partition 1, ANY, 5391 bytes)
20:09:29.489 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 20.0 (TID 60, localhost, executor driver, partition 2, ANY, 5391 bytes)
20:09:29.489 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 20.0 (TID 61, localhost, executor driver, partition 3, ANY, 5391 bytes)
20:09:29.489 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 20.0 (TID 62, localhost, executor driver, partition 4, ANY, 5391 bytes)
20:09:29.489 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 20.0 (TID 63, localhost, executor driver, partition 5, ANY, 5391 bytes)
20:09:29.490 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 20.0 (TID 64, localhost, executor driver, partition 6, ANY, 5391 bytes)
20:09:29.490 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 20.0 (TID 65, localhost, executor driver, partition 7, ANY, 5391 bytes)
20:09:29.490 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 58)
20:09:29.490 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 59)
20:09:29.490 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 2.0 in stage 20.0 (TID 60)
20:09:29.490 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Running task 3.0 in stage 20.0 (TID 61)
20:09:29.490 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Running task 4.0 in stage 20.0 (TID 62)
20:09:29.490 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Running task 5.0 in stage 20.0 (TID 63)
20:09:29.490 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Running task 6.0 in stage 20.0 (TID 64)
20:09:29.490 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Running task 7.0 in stage 20.0 (TID 65)
20:09:29.493 INFO  [Executor task launch worker for task 61] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
20:09:29.493 INFO  [Executor task launch worker for task 58] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
20:09:29.494 INFO  [Executor task launch worker for task 63] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
20:09:29.494 INFO  [Executor task launch worker for task 62] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
20:09:29.494 INFO  [Executor task launch worker for task 64] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
20:09:29.494 INFO  [Executor task launch worker for task 59] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
20:09:29.494 INFO  [Executor task launch worker for task 60] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
20:09:29.494 INFO  [Executor task launch worker for task 65] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
20:09:29.544 INFO  [Executor task launch worker for task 65] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.544 INFO  [Executor task launch worker for task 62] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.545 INFO  [Executor task launch worker for task 63] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.545 INFO  [Executor task launch worker for task 58] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.547 INFO  [Executor task launch worker for task 61] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.556 INFO  [Executor task launch worker for task 64] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.558 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Finished task 4.0 in stage 20.0 (TID 62). 1371 bytes result sent to driver
20:09:29.559 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 20.0 (TID 62) in 70 ms on localhost (executor driver) (1/8)
20:09:29.561 INFO  [Executor task launch worker for task 59] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.562 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Finished task 5.0 in stage 20.0 (TID 63). 1371 bytes result sent to driver
20:09:29.562 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 20.0 (TID 63) in 73 ms on localhost (executor driver) (2/8)
20:09:29.565 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 58). 1371 bytes result sent to driver
20:09:29.566 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 58) in 78 ms on localhost (executor driver) (3/8)
20:09:29.569 INFO  [Executor task launch worker for task 60] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:29.570 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Finished task 7.0 in stage 20.0 (TID 65). 1371 bytes result sent to driver
20:09:29.571 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 20.0 (TID 65) in 81 ms on localhost (executor driver) (4/8)
20:09:29.574 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Finished task 6.0 in stage 20.0 (TID 64). 1371 bytes result sent to driver
20:09:29.574 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 20.0 (TID 64) in 85 ms on localhost (executor driver) (5/8)
20:09:29.578 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 59). 1371 bytes result sent to driver
20:09:29.579 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 59) in 90 ms on localhost (executor driver) (6/8)
20:09:29.582 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 2.0 in stage 20.0 (TID 60). 1371 bytes result sent to driver
20:09:29.583 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 20.0 (TID 60) in 94 ms on localhost (executor driver) (7/8)
20:09:29.963 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Finished task 3.0 in stage 20.0 (TID 61). 1543 bytes result sent to driver
20:09:29.963 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 20.0 (TID 61) in 474 ms on localhost (executor driver) (8/8)
20:09:29.963 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
20:09:29.964 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 20 (show at Dw03ReleaseExposure.scala:65) finished in 0.476 s
20:09:29.964 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:09:29.964 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:09:29.964 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 21)
20:09:29.964 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:09:29.964 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[57] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
20:09:29.965 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1987.5 MB)
20:09:29.966 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.5 MB)
20:09:29.967 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.32.1:1126 (size: 2.3 KB, free: 1988.6 MB)
20:09:29.967 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1004
20:09:29.968 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[57] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
20:09:29.968 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks
20:09:29.969 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 66, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:09:29.969 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 66)
20:09:29.970 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:29.970 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:29.974 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 66). 4510 bytes result sent to driver
20:09:29.975 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 66) in 6 ms on localhost (executor driver) (1/1)
20:09:29.975 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
20:09:29.975 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (show at Dw03ReleaseExposure.scala:65) finished in 0.007 s
20:09:29.975 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 10 finished: show at Dw03ReleaseExposure.scala:65, took 0.493768 s
20:09:29.981 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
20:09:29.982 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:29.982 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:29.998 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:29.999 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.000 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:30.003 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1
20:09:30.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:09:30.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:09:30.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:30.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:30.090 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:09:30.090 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.107 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:30.108 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:09:30.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:09:30.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:09:30.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:09:30.134 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#391),(bdp_day#391 = 20190924)
20:09:30.134 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#384),(release_status#384 = 03)
20:09:30.134 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
20:09:30.134 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
20:09:30.135 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:09:30.137 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:30.137 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:30.146 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 311.6 KB, free 1987.2 MB)
20:09:30.158 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
20:09:30.159 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 192.168.32.1:1126 (size: 26.2 KB, free: 1988.5 MB)
20:09:30.159 INFO  [main] org.apache.spark.SparkContext - Created broadcast 25 from insertInto at SparkHelper.scala:35
20:09:30.159 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:09:30.188 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:09:30.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 61 (insertInto at SparkHelper.scala:35)
20:09:30.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 11 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:09:30.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (insertInto at SparkHelper.scala:35)
20:09:30.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 22)
20:09:30.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 22)
20:09:30.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 22 (MapPartitionsRDD[61] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:30.191 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 17.6 KB, free 1987.1 MB)
20:09:30.192 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.1 MB)
20:09:30.192 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 192.168.32.1:1126 (size: 7.5 KB, free: 1988.5 MB)
20:09:30.193 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1004
20:09:30.193 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[61] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:09:30.193 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 8 tasks
20:09:30.193 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 67, localhost, executor driver, partition 0, ANY, 5391 bytes)
20:09:30.194 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 68, localhost, executor driver, partition 1, ANY, 5391 bytes)
20:09:30.194 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 69, localhost, executor driver, partition 2, ANY, 5391 bytes)
20:09:30.194 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 70, localhost, executor driver, partition 3, ANY, 5391 bytes)
20:09:30.194 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 71, localhost, executor driver, partition 4, ANY, 5391 bytes)
20:09:30.194 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 72, localhost, executor driver, partition 5, ANY, 5391 bytes)
20:09:30.194 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 73, localhost, executor driver, partition 6, ANY, 5391 bytes)
20:09:30.194 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 74, localhost, executor driver, partition 7, ANY, 5391 bytes)
20:09:30.195 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 68)
20:09:30.195 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 69)
20:09:30.195 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 70)
20:09:30.195 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 67)
20:09:30.195 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 74)
20:09:30.195 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 71)
20:09:30.195 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 72)
20:09:30.195 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 73)
20:09:30.197 INFO  [Executor task launch worker for task 69] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
20:09:30.197 INFO  [Executor task launch worker for task 71] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
20:09:30.197 INFO  [Executor task launch worker for task 67] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
20:09:30.197 INFO  [Executor task launch worker for task 70] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
20:09:30.197 INFO  [Executor task launch worker for task 72] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
20:09:30.197 INFO  [Executor task launch worker for task 68] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
20:09:30.197 INFO  [Executor task launch worker for task 73] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
20:09:30.198 INFO  [Executor task launch worker for task 74] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
20:09:30.209 INFO  [Executor task launch worker for task 71] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.209 INFO  [Executor task launch worker for task 72] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.209 INFO  [Executor task launch worker for task 67] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.211 INFO  [Executor task launch worker for task 73] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.231 INFO  [Executor task launch worker for task 69] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.232 INFO  [Executor task launch worker for task 74] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.233 INFO  [Executor task launch worker for task 68] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.234 INFO  [Executor task launch worker for task 70] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
20:09:30.240 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 73). 1371 bytes result sent to driver
20:09:30.241 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 73) in 47 ms on localhost (executor driver) (1/8)
20:09:30.243 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 72). 1371 bytes result sent to driver
20:09:30.244 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 72) in 50 ms on localhost (executor driver) (2/8)
20:09:30.247 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 71). 1328 bytes result sent to driver
20:09:30.247 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 71) in 53 ms on localhost (executor driver) (3/8)
20:09:30.250 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 67). 1371 bytes result sent to driver
20:09:30.251 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 67) in 58 ms on localhost (executor driver) (4/8)
20:09:30.290 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 69). 1371 bytes result sent to driver
20:09:30.290 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 69) in 96 ms on localhost (executor driver) (5/8)
20:09:30.293 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 68). 1328 bytes result sent to driver
20:09:30.293 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 68) in 99 ms on localhost (executor driver) (6/8)
20:09:30.296 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 74). 1371 bytes result sent to driver
20:09:30.296 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 74) in 102 ms on localhost (executor driver) (7/8)
20:09:30.466 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 70). 1543 bytes result sent to driver
20:09:30.467 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 70) in 273 ms on localhost (executor driver) (8/8)
20:09:30.467 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
20:09:30.467 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 22 (insertInto at SparkHelper.scala:35) finished in 0.274 s
20:09:30.467 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:09:30.467 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:09:30.467 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 23)
20:09:30.467 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:09:30.468 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[63] at insertInto at SparkHelper.scala:35), which has no missing parents
20:09:30.495 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 167.4 KB, free 1986.9 MB)
20:09:30.496 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1986.9 MB)
20:09:30.496 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 192.168.32.1:1126 (size: 62.1 KB, free: 1988.5 MB)
20:09:30.497 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1004
20:09:30.497 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[63] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:09:30.497 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 4 tasks
20:09:30.498 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 75, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:09:30.498 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 76, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:09:30.498 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 77, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:09:30.498 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 78, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:09:30.498 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 76)
20:09:30.498 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 77)
20:09:30.498 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 75)
20:09:30.498 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 78)
20:09:30.511 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:30.511 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:30.511 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:30.511 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:30.512 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:30.512 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:30.513 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:09:30.513 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:09:30.522 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:30.522 INFO  [Executor task launch worker for task 75] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:30.523 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:30.524 INFO  [Executor task launch worker for task 77] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:30.524 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4452d139
20:09:30.524 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:30.524 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200930_0023_m_000000_0/bdp_day=20190924/part-00000-56422161-4456-42ce-91a2-2da4efa8049b.c000
20:09:30.524 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:30.524 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:30.525 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:30.525 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:30.525 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:30.525 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:30.525 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:30.525 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:30.525 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:30.525 INFO  [Executor task launch worker for task 78] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:30.525 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:09:30.525 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69e509aa
20:09:30.525 INFO  [Executor task launch worker for task 76] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:09:30.526 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:30.526 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200930_0023_m_000002_0/bdp_day=20190924/part-00002-56422161-4456-42ce-91a2-2da4efa8049b.c000
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:30.526 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:30.526 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c51016b
20:09:30.526 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:30.526 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200930_0023_m_000003_0/bdp_day=20190924/part-00003-56422161-4456-42ce-91a2-2da4efa8049b.c000
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:30.526 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:30.527 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7999cf7d
20:09:30.527 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:09:30.527 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/_temporary/attempt_20190925200930_0023_m_000001_0/bdp_day=20190924/part-00001-56422161-4456-42ce-91a2-2da4efa8049b.c000
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression set to false
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Validation is off
20:09:30.528 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:09:30.528 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5b858f51
20:09:30.530 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@74a5235a
20:09:30.531 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@75ff724d
20:09:30.532 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@40d0a632
20:09:30.573 INFO  [Executor task launch worker for task 75] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,496
20:09:30.587 INFO  [Executor task launch worker for task 76] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,186
20:09:30.588 INFO  [Executor task launch worker for task 78] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,106
20:09:30.590 INFO  [Executor task launch worker for task 77] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 937,961
20:09:30.596 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.596 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.597 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.597 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:30.597 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:30.597 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.598 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:30.618 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.618 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.619 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.619 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:30.619 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:30.619 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.620 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:30.624 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.625 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.626 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.663 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:30.664 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:30.664 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.664 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:30.668 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.668 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 192.168.32.1:1126 in memory (size: 2.3 KB, free: 1988.5 MB)
20:09:30.668 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.669 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
20:09:30.669 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,345 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
20:09:30.669 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 446
20:09:30.669 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 452
20:09:30.672 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
20:09:30.672 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
20:09:30.672 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
20:09:30.673 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.32.1:1126 in memory (size: 62.1 KB, free: 1988.5 MB)
20:09:30.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 8
20:09:30.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 447
20:09:30.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 445
20:09:30.677 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 192.168.32.1:1126 in memory (size: 7.5 KB, free: 1988.5 MB)
20:09:30.678 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 451
20:09:30.678 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 450
20:09:30.678 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 449
20:09:30.679 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.6 MB)
20:09:30.680 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200930_0023_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/task_20190925200930_0023_m_000000
20:09:30.680 INFO  [Executor task launch worker for task 75] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200930_0023_m_000000_0: Committed
20:09:30.681 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 75). 2525 bytes result sent to driver
20:09:30.682 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 192.168.32.1:1126 in memory (size: 26.2 KB, free: 1988.6 MB)
20:09:30.683 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 75) in 186 ms on localhost (executor driver) (1/4)
20:09:30.684 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 501
20:09:30.684 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 448
20:09:30.684 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 444
20:09:30.687 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 192.168.32.1:1126 in memory (size: 7.5 KB, free: 1988.6 MB)
20:09:30.689 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 7
20:09:30.712 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200930_0023_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/task_20190925200930_0023_m_000001
20:09:30.712 INFO  [Executor task launch worker for task 76] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200930_0023_m_000001_0: Committed
20:09:30.713 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 76). 2482 bytes result sent to driver
20:09:30.713 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 76) in 215 ms on localhost (executor driver) (2/4)
20:09:30.714 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200930_0023_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/task_20190925200930_0023_m_000003
20:09:30.715 INFO  [Executor task launch worker for task 78] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200930_0023_m_000003_0: Committed
20:09:30.715 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925200930_0023_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/_temporary/0/task_20190925200930_0023_m_000002
20:09:30.715 INFO  [Executor task launch worker for task 77] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925200930_0023_m_000002_0: Committed
20:09:30.715 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 77). 2482 bytes result sent to driver
20:09:30.716 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 77) in 218 ms on localhost (executor driver) (3/4)
20:09:30.730 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 78). 2525 bytes result sent to driver
20:09:30.730 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 78) in 232 ms on localhost (executor driver) (4/4)
20:09:30.730 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
20:09:30.731 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (insertInto at SparkHelper.scala:35) finished in 0.234 s
20:09:30.731 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 11 finished: insertInto at SparkHelper.scala:35, took 0.542423 s
20:09:30.759 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:09:30.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:30.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:30.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:30.775 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:30.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:30.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:30.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:30.795 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:30.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:30.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:30.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
20:09:30.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
20:09:30.861 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/bdp_day=20190924/part-00000-56422161-4456-42ce-91a2-2da4efa8049b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00000-56422161-4456-42ce-91a2-2da4efa8049b.c000, Status:true
20:09:30.870 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/bdp_day=20190924/part-00001-56422161-4456-42ce-91a2-2da4efa8049b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00001-56422161-4456-42ce-91a2-2da4efa8049b.c000, Status:true
20:09:30.879 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/bdp_day=20190924/part-00002-56422161-4456-42ce-91a2-2da4efa8049b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00002-56422161-4456-42ce-91a2-2da4efa8049b.c000, Status:true
20:09:30.889 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/bdp_day=20190924/part-00003-56422161-4456-42ce-91a2-2da4efa8049b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00003-56422161-4456-42ce-91a2-2da4efa8049b.c000, Status:true
20:09:30.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
20:09:30.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
20:09:30.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_exposure
20:09:30.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_exposure	
20:09:30.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
20:09:30.952 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
20:09:30.954 WARN  [main] hive.log - Updated size to 2286733
20:09:31.133 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-25_20-09-30_003_2703362682435369752-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
20:09:31.135 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
20:09:31.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:09:31.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:09:31.140 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:31.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:31.159 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
20:09:31.159 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
20:09:31.176 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:31.176 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:31.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:31.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:31.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:31.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:31.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:09:31.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:09:31.186 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
20:09:31.193 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:09:31.195 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
20:09:31.206 INFO  [dispatcher-event-loop-6] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:09:31.365 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:09:31.366 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
20:09:31.367 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:09:31.369 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:09:31.375 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:09:31.376 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:09:31.377 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-f6e64939-d18b-4185-b580-ca879c6a9ec0
20:10:30.627 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
20:10:30.943 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
20:10:30.962 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
20:10:30.963 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
20:10:30.963 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
20:10:30.964 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
20:10:30.964 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
20:10:31.826 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 1233.
20:10:31.845 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
20:10:31.866 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
20:10:31.869 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20:10:31.869 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
20:10:31.879 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-99242be6-f245-467b-9483-31667c80aead
20:10:31.896 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
20:10:31.946 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
20:10:32.028 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3960ms
20:10:32.090 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
20:10:32.105 INFO  [main] org.spark_project.jetty.server.Server - Started @4037ms
20:10:32.129 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@46ce4881{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:10:32.129 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
20:10:32.151 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50cf5a23{/jobs,null,AVAILABLE,@Spark}
20:10:32.152 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/json,null,AVAILABLE,@Spark}
20:10:32.153 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/jobs/job,null,AVAILABLE,@Spark}
20:10:32.154 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/job/json,null,AVAILABLE,@Spark}
20:10:32.156 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages,null,AVAILABLE,@Spark}
20:10:32.156 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/json,null,AVAILABLE,@Spark}
20:10:32.157 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/stage,null,AVAILABLE,@Spark}
20:10:32.158 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/stage/json,null,AVAILABLE,@Spark}
20:10:32.159 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool,null,AVAILABLE,@Spark}
20:10:32.160 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/pool/json,null,AVAILABLE,@Spark}
20:10:32.160 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage,null,AVAILABLE,@Spark}
20:10:32.161 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/json,null,AVAILABLE,@Spark}
20:10:32.162 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd,null,AVAILABLE,@Spark}
20:10:32.163 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd/json,null,AVAILABLE,@Spark}
20:10:32.163 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment,null,AVAILABLE,@Spark}
20:10:32.164 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/environment/json,null,AVAILABLE,@Spark}
20:10:32.165 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors,null,AVAILABLE,@Spark}
20:10:32.166 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/json,null,AVAILABLE,@Spark}
20:10:32.166 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump,null,AVAILABLE,@Spark}
20:10:32.167 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/threadDump/json,null,AVAILABLE,@Spark}
20:10:32.174 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/static,null,AVAILABLE,@Spark}
20:10:32.175 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/,null,AVAILABLE,@Spark}
20:10:32.176 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/api,null,AVAILABLE,@Spark}
20:10:32.177 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/jobs/job/kill,null,AVAILABLE,@Spark}
20:10:32.177 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/stages/stage/kill,null,AVAILABLE,@Spark}
20:10:32.179 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
20:10:32.259 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
20:10:32.293 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1274.
20:10:32.294 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:1274
20:10:32.296 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20:10:32.335 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 1274, None)
20:10:32.339 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:1274 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 1274, None)
20:10:32.343 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 1274, None)
20:10:32.343 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 1274, None)
20:10:32.503 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319854f0{/metrics/json,null,AVAILABLE,@Spark}
20:10:33.993 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
20:10:34.022 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
20:10:34.023 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
20:10:34.031 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL,null,AVAILABLE,@Spark}
20:10:34.031 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@125d47c4{/SQL/json,null,AVAILABLE,@Spark}
20:10:34.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution,null,AVAILABLE,@Spark}
20:10:34.032 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f2466c{/SQL/execution/json,null,AVAILABLE,@Spark}
20:10:34.034 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b53840a{/static/sql,null,AVAILABLE,@Spark}
20:10:34.523 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20:10:35.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20:10:35.264 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
20:10:36.543 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20:10:37.724 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
20:10:37.726 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
20:10:37.936 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
20:10:37.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
20:10:37.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
20:10:38.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
20:10:38.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
20:10:38.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
20:10:38.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20:10:38.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
20:10:38.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
20:10:38.150 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
20:10:38.151 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
20:10:38.154 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
20:10:38.154 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
20:10:38.638 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/dbe89c0e-9995-4f1b-b15b-9c96be1e8768_resources
20:10:38.658 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/dbe89c0e-9995-4f1b-b15b-9c96be1e8768
20:10:38.662 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/dbe89c0e-9995-4f1b-b15b-9c96be1e8768
20:10:38.666 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/dbe89c0e-9995-4f1b-b15b-9c96be1e8768/_tmp_space.db
20:10:38.670 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:10:38.691 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:10:38.692 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:10:38.700 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
20:10:38.700 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
20:10:38.704 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
20:10:38.895 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/3092c53f-976b-4d36-9af4-68a2ca6ed49a_resources
20:10:38.902 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3092c53f-976b-4d36-9af4-68a2ca6ed49a
20:10:38.918 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/3092c53f-976b-4d36-9af4-68a2ca6ed49a
20:10:38.926 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3092c53f-976b-4d36-9af4-68a2ca6ed49a/_tmp_space.db
20:10:38.928 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
20:10:38.959 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
20:10:38.965 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:10:39.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:10:39.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:10:39.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:39.178 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:39.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:39.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:39.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.320 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.320 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.320 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.321 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.321 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.321 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.321 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.321 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:39.321 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:39.335 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
20:10:39.645 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
20:10:39.669 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:10:39.670 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:10:39.671 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:10:39.672 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:10:39.672 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:10:39.673 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:10:39.673 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:10:39.674 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:10:39.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:10:39.678 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:10:39.946 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:10:39.946 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:10:39.951 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:39.952 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:39.980 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:39.980 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:40.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:40.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:40.032 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:10:40.032 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:10:40.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:10:40.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:10:40.254 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190923)
20:10:40.256 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
20:10:40.258 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:10:40.270 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
20:10:40.275 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:10:40.668 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 268.814434 ms
20:10:40.943 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 42.523688 ms
20:10:41.038 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
20:10:41.164 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
20:10:41.167 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:1274 (size: 26.3 KB, free: 1988.7 MB)
20:10:41.204 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw05ReleaseRegisterUsers.scala:66
20:10:41.207 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
20:10:41.212 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:10:41.319 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
20:10:41.334 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at Dw05ReleaseRegisterUsers.scala:66)
20:10:41.336 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
20:10:41.337 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw05ReleaseRegisterUsers.scala:66)
20:10:41.337 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
20:10:41.339 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
20:10:41.342 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
20:10:41.455 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 21.7 KB, free 1988.3 MB)
20:10:41.459 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.3 MB)
20:10:41.460 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:1274 (size: 9.6 KB, free: 1988.7 MB)
20:10:41.460 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
20:10:41.471 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:10:41.471 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
20:10:41.506 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:10:41.508 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:10:41.509 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:10:41.509 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:10:41.509 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:10:41.510 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:10:41.510 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:10:41.511 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:10:41.520 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
20:10:41.520 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
20:10:41.520 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
20:10:41.520 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
20:10:41.520 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
20:10:41.520 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
20:10:41.520 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
20:10:41.520 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
20:10:41.622 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.258582 ms
20:10:41.642 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:10:41.642 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:10:41.642 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:10:41.642 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:10:41.642 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:10:41.642 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:10:41.642 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:10:41.642 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:10:42.778 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.778 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.778 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.778 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.778 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.777 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.777 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.803 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:42.854 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1610 bytes result sent to driver
20:10:42.854 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1567 bytes result sent to driver
20:10:42.854 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1567 bytes result sent to driver
20:10:42.854 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1610 bytes result sent to driver
20:10:42.854 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1567 bytes result sent to driver
20:10:42.856 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1524 bytes result sent to driver
20:10:42.860 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1524 bytes result sent to driver
20:10:42.865 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1356 ms on localhost (executor driver) (1/8)
20:10:42.867 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1358 ms on localhost (executor driver) (2/8)
20:10:42.867 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1358 ms on localhost (executor driver) (3/8)
20:10:42.867 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1371 ms on localhost (executor driver) (4/8)
20:10:42.868 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1356 ms on localhost (executor driver) (5/8)
20:10:42.868 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1358 ms on localhost (executor driver) (6/8)
20:10:42.868 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1360 ms on localhost (executor driver) (7/8)
20:10:43.685 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1782 bytes result sent to driver
20:10:43.689 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2180 ms on localhost (executor driver) (8/8)
20:10:43.691 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
20:10:43.694 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 2.205 s
20:10:43.696 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:10:43.697 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:10:43.698 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
20:10:43.699 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:10:43.704 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
20:10:43.713 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
20:10:43.715 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
20:10:43.716 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:1274 (size: 2.2 KB, free: 1988.7 MB)
20:10:43.717 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
20:10:43.719 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
20:10:43.719 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
20:10:43.722 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:10:43.722 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
20:10:43.737 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:43.739 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
20:10:43.760 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 6001 bytes result sent to driver
20:10:43.761 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 41 ms on localhost (executor driver) (1/1)
20:10:43.761 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
20:10:43.761 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.041 s
20:10:43.767 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 2.448102 s
20:10:43.799 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
20:10:43.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:43.806 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:43.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.838 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.838 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:43.838 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:43.923 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1
20:10:44.093 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:10:44.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:10:44.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:44.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:44.121 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:44.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:44.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:44.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:44.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:10:44.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:10:44.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:10:44.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:10:44.184 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190923)
20:10:44.185 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
20:10:44.186 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:10:44.186 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
20:10:44.187 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:10:44.208 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:44.209 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:44.237 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.672898 ms
20:10:44.249 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
20:10:44.273 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
20:10:44.274 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:1274 (size: 26.3 KB, free: 1988.6 MB)
20:10:44.275 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
20:10:44.275 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:10:44.345 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:10:44.346 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:35)
20:10:44.347 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:10:44.347 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:35)
20:10:44.347 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
20:10:44.347 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
20:10:44.348 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
20:10:44.352 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 21.7 KB, free 1988.0 MB)
20:10:44.354 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.0 MB)
20:10:44.355 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:1274 (size: 9.6 KB, free: 1988.6 MB)
20:10:44.356 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
20:10:44.356 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:10:44.356 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 8 tasks
20:10:44.357 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 9, localhost, executor driver, partition 0, ANY, 5381 bytes)
20:10:44.358 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 10, localhost, executor driver, partition 1, ANY, 5381 bytes)
20:10:44.358 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 11, localhost, executor driver, partition 2, ANY, 5381 bytes)
20:10:44.358 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 12, localhost, executor driver, partition 3, ANY, 5381 bytes)
20:10:44.359 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 13, localhost, executor driver, partition 4, ANY, 5381 bytes)
20:10:44.359 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 14, localhost, executor driver, partition 5, ANY, 5381 bytes)
20:10:44.360 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 15, localhost, executor driver, partition 6, ANY, 5381 bytes)
20:10:44.360 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 16, localhost, executor driver, partition 7, ANY, 5381 bytes)
20:10:44.360 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 9)
20:10:44.360 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 10)
20:10:44.360 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 12)
20:10:44.360 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 15)
20:10:44.360 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 16)
20:10:44.361 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 11)
20:10:44.361 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 13)
20:10:44.361 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 14)
20:10:44.367 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
20:10:44.370 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
20:10:44.371 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
20:10:44.372 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
20:10:44.372 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
20:10:44.373 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
20:10:44.375 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
20:10:44.381 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
20:10:44.387 INFO  [Executor task launch worker for task 12] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.392 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.396 INFO  [Executor task launch worker for task 11] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.397 INFO  [Executor task launch worker for task 13] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.399 INFO  [Executor task launch worker for task 15] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.400 INFO  [Executor task launch worker for task 10] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.400 INFO  [Executor task launch worker for task 9] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.403 INFO  [Executor task launch worker for task 14] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:44.422 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
20:10:44.429 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 16). 1567 bytes result sent to driver
20:10:44.432 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 16) in 72 ms on localhost (executor driver) (1/8)
20:10:44.450 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 15). 1567 bytes result sent to driver
20:10:44.451 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 15) in 91 ms on localhost (executor driver) (2/8)
20:10:44.453 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 11). 1567 bytes result sent to driver
20:10:44.454 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 11) in 96 ms on localhost (executor driver) (3/8)
20:10:44.455 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 13). 1567 bytes result sent to driver
20:10:44.456 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 13) in 98 ms on localhost (executor driver) (4/8)
20:10:44.460 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:1274 in memory (size: 2.2 KB, free: 1988.6 MB)
20:10:44.461 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 10). 1567 bytes result sent to driver
20:10:44.462 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 10) in 105 ms on localhost (executor driver) (5/8)
20:10:44.464 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 14). 1524 bytes result sent to driver
20:10:44.464 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 14) in 105 ms on localhost (executor driver) (6/8)
20:10:44.470 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 9). 1610 bytes result sent to driver
20:10:44.471 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 9) in 114 ms on localhost (executor driver) (7/8)
20:10:44.934 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 12). 1739 bytes result sent to driver
20:10:44.935 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 12) in 577 ms on localhost (executor driver) (8/8)
20:10:44.935 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
20:10:44.936 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:35) finished in 0.579 s
20:10:44.936 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:10:44.936 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:10:44.936 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
20:10:44.936 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:10:44.936 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35), which has no missing parents
20:10:44.977 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
20:10:44.980 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
20:10:44.981 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:1274 (size: 64.1 KB, free: 1988.6 MB)
20:10:44.981 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
20:10:44.982 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:10:44.982 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
20:10:44.983 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 17, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:10:44.984 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 18, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:10:44.984 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 19, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:10:44.985 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 20, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:10:44.985 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 19)
20:10:44.985 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 20)
20:10:44.985 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 17)
20:10:44.986 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 18)
20:10:45.026 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:45.026 INFO  [Executor task launch worker for task 19] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:10:45.026 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:45.026 INFO  [Executor task launch worker for task 20] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:10:45.026 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:45.026 INFO  [Executor task launch worker for task 18] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:10:45.030 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:45.030 INFO  [Executor task launch worker for task 17] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:10:45.048 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.884554 ms
20:10:45.069 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.961887 ms
20:10:45.111 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:45.111 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:45.111 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:45.111 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:45.112 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:45.112 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:45.112 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:45.113 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:45.123 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.752006 ms
20:10:45.166 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:1274 in memory (size: 9.6 KB, free: 1988.6 MB)
20:10:45.185 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 37.848524 ms
20:10:45.200 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.00475 ms
20:10:45.288 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7cf7c305
20:10:45.289 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6fcc851
20:10:45.289 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17d77a94
20:10:45.289 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@328d7255
20:10:45.301 INFO  [Executor task launch worker for task 18] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
20:10:45.308 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:45.308 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:45.308 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:45.308 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:45.308 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201045_0003_m_000001_0/bdp_day=20190923/part-00001-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
20:10:45.308 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201045_0003_m_000003_0/bdp_day=20190923/part-00003-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
20:10:45.308 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201045_0003_m_000000_0/bdp_day=20190923/part-00000-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
20:10:45.308 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201045_0003_m_000002_0/bdp_day=20190923/part-00002-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
20:10:45.311 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:45.311 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:45.311 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:45.311 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:45.312 INFO  [Executor task launch worker for task 19] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:45.312 INFO  [Executor task launch worker for task 20] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:45.312 INFO  [Executor task launch worker for task 17] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:45.312 INFO  [Executor task launch worker for task 18] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:45.313 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:45.313 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:45.313 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:45.313 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:45.313 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:45.313 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:45.314 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:45.314 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:45.314 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:45.314 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:45.314 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:45.314 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:45.314 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:45.314 INFO  [Executor task launch worker for task 18] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:45.314 INFO  [Executor task launch worker for task 20] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:45.314 INFO  [Executor task launch worker for task 17] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:45.314 INFO  [Executor task launch worker for task 19] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:45.491 INFO  [Executor task launch worker for task 17] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5eea0436
20:10:45.491 INFO  [Executor task launch worker for task 20] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@699727cb
20:10:45.492 INFO  [Executor task launch worker for task 19] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3773a989
20:10:45.492 INFO  [Executor task launch worker for task 18] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@9ffe365
20:10:45.625 INFO  [Executor task launch worker for task 18] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,756
20:10:45.628 INFO  [Executor task launch worker for task 19] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,684
20:10:45.629 INFO  [Executor task launch worker for task 20] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,692
20:10:45.633 INFO  [Executor task launch worker for task 17] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,830
20:10:45.715 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.715 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.715 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.716 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.716 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.716 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.716 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.720 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.721 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.723 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.724 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.725 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.725 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.725 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.726 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.726 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:45.726 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:45.727 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:45.727 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:45.727 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:45.728 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:45.728 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.728 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.728 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.729 INFO  [Executor task launch worker for task 20] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.729 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.729 INFO  [Executor task launch worker for task 19] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.729 INFO  [Executor task launch worker for task 17] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.729 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:45.730 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:45.731 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:45.731 INFO  [Executor task launch worker for task 18] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:45.958 INFO  [Executor task launch worker for task 17] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201045_0003_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/task_20190925201045_0003_m_000000
20:10:45.958 INFO  [Executor task launch worker for task 20] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201045_0003_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/task_20190925201045_0003_m_000003
20:10:45.958 INFO  [Executor task launch worker for task 19] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201045_0003_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/task_20190925201045_0003_m_000002
20:10:45.958 INFO  [Executor task launch worker for task 18] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201045_0003_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/_temporary/0/task_20190925201045_0003_m_000001
20:10:45.959 INFO  [Executor task launch worker for task 19] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201045_0003_m_000002_0: Committed
20:10:45.959 INFO  [Executor task launch worker for task 20] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201045_0003_m_000003_0: Committed
20:10:45.959 INFO  [Executor task launch worker for task 18] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201045_0003_m_000001_0: Committed
20:10:45.959 INFO  [Executor task launch worker for task 17] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201045_0003_m_000000_0: Committed
20:10:45.962 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 17). 2658 bytes result sent to driver
20:10:45.962 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 20). 2658 bytes result sent to driver
20:10:45.962 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 18). 2658 bytes result sent to driver
20:10:45.962 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 19). 2658 bytes result sent to driver
20:10:45.964 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 17) in 981 ms on localhost (executor driver) (1/4)
20:10:45.964 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 18) in 981 ms on localhost (executor driver) (2/4)
20:10:45.964 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 20) in 980 ms on localhost (executor driver) (3/4)
20:10:45.964 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 19) in 980 ms on localhost (executor driver) (4/4)
20:10:45.964 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
20:10:45.975 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:35) finished in 0.991 s
20:10:45.975 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 1.630295 s
20:10:46.022 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:10:46.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.047 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.067 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.069 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.069 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.069 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:46.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:46.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:46.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]
20:10:46.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]	
20:10:46.180 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
20:10:46.222 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/bdp_day=20190923/part-00000-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00000-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, Status:true
20:10:46.333 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/bdp_day=20190923/part-00001-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00001-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, Status:true
20:10:46.343 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/bdp_day=20190923/part-00002-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00002-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, Status:true
20:10:46.355 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/bdp_day=20190923/part-00003-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00003-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000, Status:true
20:10:46.359 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]
20:10:46.359 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]	
20:10:46.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_register_users[20190923]
20:10:46.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_register_users[20190923]	
20:10:46.392 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
20:10:46.395 WARN  [main] hive.log - Updated size to 872108
20:10:46.591 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-43_920_9063699991855473377-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
20:10:46.665 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
20:10:46.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:10:46.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:10:46.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.704 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.704 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.728 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.729 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.729 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.729 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.729 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.729 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.730 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.730 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.730 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:46.735 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_register_users (inference mode: INFER_AND_SAVE)
20:10:46.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:10:46.736 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:10:46.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:46.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:46.800 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.800 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.800 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.800 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:46.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:46.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_register_users
20:10:46.803 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_register_users	
20:10:46.904 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:10:46.905 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 1 output partitions
20:10:46.905 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (insertInto at SparkHelper.scala:35)
20:10:46.905 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
20:10:46.905 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
20:10:46.905 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:35), which has no missing parents
20:10:46.921 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 74.4 KB, free 1987.7 MB)
20:10:46.923 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1987.7 MB)
20:10:46.923 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:1274 (size: 26.8 KB, free: 1988.6 MB)
20:10:46.924 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
20:10:46.924 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
20:10:46.924 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
20:10:46.927 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 5055 bytes)
20:10:46.928 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 21)
20:10:47.525 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 21). 1721 bytes result sent to driver
20:10:47.525 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 21) in 600 ms on localhost (executor driver) (1/1)
20:10:47.526 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
20:10:47.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (insertInto at SparkHelper.scala:35) finished in 0.601 s
20:10:47.527 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.622817 s
20:10:47.532 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_register_users
20:10:47.532 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:10:47.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:10:47.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:47.539 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:47.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:47.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:47.581 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.581 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:47.586 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:47.586 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:47.606 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:47.606 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:47.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.627 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.627 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:47.627 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:47.632 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:47.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:47.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_register_users newtbl=dw_release_register_users
20:10:47.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_register_users newtbl=dw_release_register_users	
20:10:47.973 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
20:10:47.973 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:10:47.973 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:10:47.978 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:47.979 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:48.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:48.001 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:48.022 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.022 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.023 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:48.033 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
20:10:48.034 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
20:10:48.034 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
20:10:48.034 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
20:10:48.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
20:10:48.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
20:10:48.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
20:10:48.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
20:10:48.035 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
20:10:48.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
20:10:48.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
20:10:48.082 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:10:48.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:10:48.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:48.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:48.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:48.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:48.133 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:48.134 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:48.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:10:48.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:10:48.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:10:48.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:10:48.170 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#123),(bdp_day#123 = 20190924)
20:10:48.171 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#116),(release_status#116 = 06)
20:10:48.171 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:10:48.171 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
20:10:48.172 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:10:48.190 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
20:10:48.208 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.3 MB)
20:10:48.209 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:1274 (size: 26.3 KB, free: 1988.5 MB)
20:10:48.210 INFO  [main] org.apache.spark.SparkContext - Created broadcast 7 from show at Dw05ReleaseRegisterUsers.scala:66
20:10:48.210 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:10:48.220 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
20:10:48.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (show at Dw05ReleaseRegisterUsers.scala:66)
20:10:48.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
20:10:48.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (show at Dw05ReleaseRegisterUsers.scala:66)
20:10:48.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)
20:10:48.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)
20:10:48.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
20:10:48.228 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 21.7 KB, free 1987.3 MB)
20:10:48.229 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.3 MB)
20:10:48.230 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:1274 (size: 9.6 KB, free: 1988.5 MB)
20:10:48.231 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1004
20:10:48.231 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:10:48.232 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 8 tasks
20:10:48.233 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 22, localhost, executor driver, partition 0, ANY, 5391 bytes)
20:10:48.233 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 23, localhost, executor driver, partition 1, ANY, 5391 bytes)
20:10:48.233 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 24, localhost, executor driver, partition 2, ANY, 5391 bytes)
20:10:48.233 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 25, localhost, executor driver, partition 3, ANY, 5391 bytes)
20:10:48.234 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 5.0 (TID 26, localhost, executor driver, partition 4, ANY, 5391 bytes)
20:10:48.235 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 5.0 (TID 27, localhost, executor driver, partition 5, ANY, 5391 bytes)
20:10:48.235 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 5.0 (TID 28, localhost, executor driver, partition 6, ANY, 5391 bytes)
20:10:48.235 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 5.0 (TID 29, localhost, executor driver, partition 7, ANY, 5391 bytes)
20:10:48.236 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 22)
20:10:48.236 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 23)
20:10:48.236 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 25)
20:10:48.236 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 24)
20:10:48.236 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 4.0 in stage 5.0 (TID 26)
20:10:48.236 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 5.0 in stage 5.0 (TID 27)
20:10:48.236 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 6.0 in stage 5.0 (TID 28)
20:10:48.239 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 7.0 in stage 5.0 (TID 29)
20:10:48.242 INFO  [Executor task launch worker for task 24] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
20:10:48.244 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
20:10:48.245 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
20:10:48.246 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
20:10:48.247 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
20:10:48.247 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
20:10:48.248 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
20:10:48.249 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
20:10:48.291 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.292 INFO  [Executor task launch worker for task 24] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.293 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.293 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.295 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.295 INFO  [Executor task launch worker for task 26] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.301 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.306 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:48.310 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 24). 1524 bytes result sent to driver
20:10:48.311 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 24) in 78 ms on localhost (executor driver) (1/8)
20:10:48.314 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 7.0 in stage 5.0 (TID 29). 1524 bytes result sent to driver
20:10:48.315 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 5.0 (TID 29) in 80 ms on localhost (executor driver) (2/8)
20:10:48.318 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 5.0 in stage 5.0 (TID 27). 1481 bytes result sent to driver
20:10:48.318 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 5.0 (TID 27) in 83 ms on localhost (executor driver) (3/8)
20:10:48.321 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 23). 1481 bytes result sent to driver
20:10:48.322 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 23) in 89 ms on localhost (executor driver) (4/8)
20:10:48.324 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 4.0 in stage 5.0 (TID 26). 1524 bytes result sent to driver
20:10:48.325 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 5.0 (TID 26) in 91 ms on localhost (executor driver) (5/8)
20:10:48.328 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 6.0 in stage 5.0 (TID 28). 1524 bytes result sent to driver
20:10:48.329 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 5.0 (TID 28) in 94 ms on localhost (executor driver) (6/8)
20:10:48.359 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 22). 1567 bytes result sent to driver
20:10:48.359 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 22) in 127 ms on localhost (executor driver) (7/8)
20:10:48.956 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 83
20:10:48.957 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:1274 in memory (size: 26.3 KB, free: 1988.5 MB)
20:10:48.958 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 166
20:10:48.960 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:1274 in memory (size: 26.8 KB, free: 1988.6 MB)
20:10:48.964 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 1
20:10:48.964 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 88
20:10:48.964 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 87
20:10:48.964 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 92
20:10:48.964 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
20:10:48.964 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 89
20:10:48.964 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 82
20:10:48.966 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.32.1:1274 in memory (size: 26.3 KB, free: 1988.6 MB)
20:10:48.967 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 86
20:10:48.968 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
20:10:48.968 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
20:10:48.968 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
20:10:48.970 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:1274 in memory (size: 64.1 KB, free: 1988.7 MB)
20:10:48.971 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 93
20:10:48.972 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
20:10:48.972 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 85
20:10:48.975 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:1274 in memory (size: 9.6 KB, free: 1988.7 MB)
20:10:48.975 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 90
20:10:48.975 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 91
20:10:48.975 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
20:10:48.975 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
20:10:48.975 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 84
20:10:48.976 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
20:10:48.976 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
20:10:49.060 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 25). 1739 bytes result sent to driver
20:10:49.060 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 25) in 827 ms on localhost (executor driver) (8/8)
20:10:49.060 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
20:10:49.061 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.829 s
20:10:49.061 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:10:49.061 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:10:49.061 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 6)
20:10:49.061 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:10:49.062 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[24] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
20:10:49.063 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
20:10:49.064 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
20:10:49.065 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:1274 (size: 2.2 KB, free: 1988.7 MB)
20:10:49.065 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
20:10:49.066 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[24] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
20:10:49.066 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
20:10:49.066 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 30, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:10:49.067 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 30)
20:10:49.069 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:49.069 INFO  [Executor task launch worker for task 30] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:10:49.120 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 30). 5929 bytes result sent to driver
20:10:49.120 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 30) in 54 ms on localhost (executor driver) (1/1)
20:10:49.120 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
20:10:49.120 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.054 s
20:10:49.121 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.900125 s
20:10:49.132 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
20:10:49.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:49.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:49.155 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.156 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:49.161 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1
20:10:49.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
20:10:49.212 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
20:10:49.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:49.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:49.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
20:10:49.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
20:10:49.253 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.253 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:49.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:49.255 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
20:10:49.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
20:10:49.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
20:10:49.256 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
20:10:49.285 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#123),(bdp_day#123 = 20190924)
20:10:49.285 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#116),(release_status#116 = 06)
20:10:49.285 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
20:10:49.286 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
20:10:49.286 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
20:10:49.292 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:49.293 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:49.307 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
20:10:49.327 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
20:10:49.328 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:1274 (size: 26.3 KB, free: 1988.6 MB)
20:10:49.329 INFO  [main] org.apache.spark.SparkContext - Created broadcast 10 from insertInto at SparkHelper.scala:35
20:10:49.329 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
20:10:49.373 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
20:10:49.374 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 29 (insertInto at SparkHelper.scala:35)
20:10:49.374 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (insertInto at SparkHelper.scala:35) with 4 output partitions
20:10:49.374 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 (insertInto at SparkHelper.scala:35)
20:10:49.374 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)
20:10:49.374 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 7)
20:10:49.374 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 7 (MapPartitionsRDD[29] at insertInto at SparkHelper.scala:35), which has no missing parents
20:10:49.379 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 21.7 KB, free 1988.0 MB)
20:10:49.381 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.0 MB)
20:10:49.383 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:1274 (size: 9.6 KB, free: 1988.6 MB)
20:10:49.384 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
20:10:49.384 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[29] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
20:10:49.384 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 8 tasks
20:10:49.385 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 31, localhost, executor driver, partition 0, ANY, 5391 bytes)
20:10:49.385 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 32, localhost, executor driver, partition 1, ANY, 5391 bytes)
20:10:49.386 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 33, localhost, executor driver, partition 2, ANY, 5391 bytes)
20:10:49.386 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 34, localhost, executor driver, partition 3, ANY, 5391 bytes)
20:10:49.386 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 7.0 (TID 35, localhost, executor driver, partition 4, ANY, 5391 bytes)
20:10:49.386 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 7.0 (TID 36, localhost, executor driver, partition 5, ANY, 5391 bytes)
20:10:49.387 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 7.0 (TID 37, localhost, executor driver, partition 6, ANY, 5391 bytes)
20:10:49.387 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 7.0 (TID 38, localhost, executor driver, partition 7, ANY, 5391 bytes)
20:10:49.387 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 31)
20:10:49.387 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 32)
20:10:49.387 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 33)
20:10:49.387 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 4.0 in stage 7.0 (TID 35)
20:10:49.387 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 6.0 in stage 7.0 (TID 37)
20:10:49.387 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 3.0 in stage 7.0 (TID 34)
20:10:49.387 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 5.0 in stage 7.0 (TID 36)
20:10:49.387 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 7.0 in stage 7.0 (TID 38)
20:10:49.392 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
20:10:49.392 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
20:10:49.393 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
20:10:49.394 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
20:10:49.394 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
20:10:49.395 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
20:10:49.399 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
20:10:49.400 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
20:10:49.411 INFO  [Executor task launch worker for task 37] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.413 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.413 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.416 INFO  [Executor task launch worker for task 34] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.424 INFO  [Executor task launch worker for task 33] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.426 INFO  [Executor task launch worker for task 36] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.431 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 6.0 in stage 7.0 (TID 37). 1481 bytes result sent to driver
20:10:49.435 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 7.0 (TID 37) in 49 ms on localhost (executor driver) (1/8)
20:10:49.439 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 7.0 in stage 7.0 (TID 38). 1524 bytes result sent to driver
20:10:49.441 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 7.0 (TID 38) in 54 ms on localhost (executor driver) (2/8)
20:10:49.450 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.453 INFO  [Executor task launch worker for task 35] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
20:10:49.458 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 5.0 in stage 7.0 (TID 36). 1524 bytes result sent to driver
20:10:49.458 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 7.0 (TID 36) in 72 ms on localhost (executor driver) (3/8)
20:10:49.461 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 33). 1524 bytes result sent to driver
20:10:49.462 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 33) in 76 ms on localhost (executor driver) (4/8)
20:10:49.465 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 32). 1567 bytes result sent to driver
20:10:49.466 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 32) in 81 ms on localhost (executor driver) (5/8)
20:10:49.669 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 31). 1567 bytes result sent to driver
20:10:49.670 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 31) in 285 ms on localhost (executor driver) (6/8)
20:10:49.673 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 4.0 in stage 7.0 (TID 35). 1567 bytes result sent to driver
20:10:49.673 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 7.0 (TID 35) in 287 ms on localhost (executor driver) (7/8)
20:10:49.978 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 223
20:10:49.979 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:1274 in memory (size: 2.2 KB, free: 1988.6 MB)
20:10:50.069 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 3.0 in stage 7.0 (TID 34). 1739 bytes result sent to driver
20:10:50.070 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 7.0 (TID 34) in 684 ms on localhost (executor driver) (8/8)
20:10:50.070 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
20:10:50.070 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 7 (insertInto at SparkHelper.scala:35) finished in 0.685 s
20:10:50.070 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
20:10:50.070 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
20:10:50.070 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 8)
20:10:50.070 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
20:10:50.071 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 8 (MapPartitionsRDD[32] at insertInto at SparkHelper.scala:35), which has no missing parents
20:10:50.095 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
20:10:50.097 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
20:10:50.099 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:1274 (size: 64.1 KB, free: 1988.6 MB)
20:10:50.099 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
20:10:50.099 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 8 (MapPartitionsRDD[32] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
20:10:50.099 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 4 tasks
20:10:50.100 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 39, localhost, executor driver, partition 0, ANY, 4726 bytes)
20:10:50.100 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 40, localhost, executor driver, partition 1, ANY, 4726 bytes)
20:10:50.100 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 8.0 (TID 41, localhost, executor driver, partition 2, ANY, 4726 bytes)
20:10:50.100 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 8.0 (TID 42, localhost, executor driver, partition 3, ANY, 4726 bytes)
20:10:50.101 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 39)
20:10:50.101 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 40)
20:10:50.101 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 2.0 in stage 8.0 (TID 41)
20:10:50.101 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 3.0 in stage 8.0 (TID 42)
20:10:50.122 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:50.122 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:50.122 INFO  [Executor task launch worker for task 42] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:10:50.123 INFO  [Executor task launch worker for task 41] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
20:10:50.123 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:50.123 INFO  [Executor task launch worker for task 39] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:10:50.124 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
20:10:50.124 INFO  [Executor task launch worker for task 40] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
20:10:50.138 INFO  [Executor task launch worker for task 40] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:50.138 INFO  [Executor task launch worker for task 42] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:50.138 INFO  [Executor task launch worker for task 39] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:50.138 INFO  [Executor task launch worker for task 41] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
20:10:50.139 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:50.139 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:50.139 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:50.139 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
20:10:50.144 INFO  [Executor task launch worker for task 42] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a447802
20:10:50.144 INFO  [Executor task launch worker for task 41] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16da2810
20:10:50.144 INFO  [Executor task launch worker for task 40] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ae3aac1
20:10:50.144 INFO  [Executor task launch worker for task 39] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17e4671b
20:10:50.144 INFO  [Executor task launch worker for task 42] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:50.144 INFO  [Executor task launch worker for task 39] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:50.144 INFO  [Executor task launch worker for task 42] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201050_0008_m_000003_0/bdp_day=20190924/part-00003-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
20:10:50.144 INFO  [Executor task launch worker for task 40] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:50.144 INFO  [Executor task launch worker for task 39] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201050_0008_m_000000_0/bdp_day=20190924/part-00000-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
20:10:50.144 INFO  [Executor task launch worker for task 41] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
20:10:50.145 INFO  [Executor task launch worker for task 40] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201050_0008_m_000001_0/bdp_day=20190924/part-00001-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:50.145 INFO  [Executor task launch worker for task 41] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/_temporary/attempt_20190925201050_0008_m_000002_0/bdp_day=20190924/part-00002-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:50.145 INFO  [Executor task launch worker for task 39] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:50.145 INFO  [Executor task launch worker for task 41] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:50.145 INFO  [Executor task launch worker for task 39] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:50.145 INFO  [Executor task launch worker for task 40] parquet.hadoop.codec.CodecConfig - Compression set to false
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:50.145 INFO  [Executor task launch worker for task 41] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:50.145 INFO  [Executor task launch worker for task 39] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:50.145 INFO  [Executor task launch worker for task 40] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:50.145 INFO  [Executor task launch worker for task 41] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:50.145 INFO  [Executor task launch worker for task 39] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:50.145 INFO  [Executor task launch worker for task 40] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
20:10:50.145 INFO  [Executor task launch worker for task 42] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:50.145 INFO  [Executor task launch worker for task 41] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:50.145 INFO  [Executor task launch worker for task 39] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:50.145 INFO  [Executor task launch worker for task 40] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
20:10:50.145 INFO  [Executor task launch worker for task 41] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:50.145 INFO  [Executor task launch worker for task 39] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:50.145 INFO  [Executor task launch worker for task 40] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
20:10:50.146 INFO  [Executor task launch worker for task 41] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:50.146 INFO  [Executor task launch worker for task 39] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:50.146 INFO  [Executor task launch worker for task 40] parquet.hadoop.ParquetOutputFormat - Dictionary is on
20:10:50.146 INFO  [Executor task launch worker for task 41] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:50.146 INFO  [Executor task launch worker for task 39] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:50.146 INFO  [Executor task launch worker for task 40] parquet.hadoop.ParquetOutputFormat - Validation is off
20:10:50.146 INFO  [Executor task launch worker for task 41] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:50.146 INFO  [Executor task launch worker for task 40] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
20:10:50.193 INFO  [Executor task launch worker for task 42] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2f3542d7
20:10:50.193 INFO  [Executor task launch worker for task 40] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@370ba89f
20:10:50.194 INFO  [Executor task launch worker for task 39] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@75dd90eb
20:10:50.195 INFO  [Executor task launch worker for task 41] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@7581309c
20:10:50.236 INFO  [Executor task launch worker for task 42] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 323,318
20:10:50.237 INFO  [Executor task launch worker for task 39] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,890
20:10:50.239 INFO  [Executor task launch worker for task 40] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,616
20:10:50.243 INFO  [Executor task launch worker for task 41] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,880
20:10:50.244 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.248 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.249 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.249 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.249 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:50.249 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:50.249 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.250 INFO  [Executor task launch worker for task 42] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.251 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.252 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.252 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.253 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.253 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.253 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:50.253 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:50.254 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.254 INFO  [Executor task launch worker for task 39] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.255 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.255 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.257 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.257 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.257 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.258 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.258 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:50.259 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.259 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
20:10:50.259 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:50.259 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
20:10:50.262 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.262 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
20:10:50.263 INFO  [Executor task launch worker for task 41] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.263 INFO  [Executor task launch worker for task 40] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
20:10:50.311 INFO  [Executor task launch worker for task 42] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201050_0008_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/task_20190925201050_0008_m_000003
20:10:50.311 INFO  [Executor task launch worker for task 42] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201050_0008_m_000003_0: Committed
20:10:50.312 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 3.0 in stage 8.0 (TID 42). 2572 bytes result sent to driver
20:10:50.313 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 8.0 (TID 42) in 213 ms on localhost (executor driver) (1/4)
20:10:50.313 INFO  [Executor task launch worker for task 40] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201050_0008_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/task_20190925201050_0008_m_000001
20:10:50.314 INFO  [Executor task launch worker for task 40] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201050_0008_m_000001_0: Committed
20:10:50.314 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 1.0 in stage 8.0 (TID 40). 2572 bytes result sent to driver
20:10:50.315 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 8.0 (TID 40) in 215 ms on localhost (executor driver) (2/4)
20:10:50.317 INFO  [Executor task launch worker for task 39] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201050_0008_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/task_20190925201050_0008_m_000000
20:10:50.317 INFO  [Executor task launch worker for task 39] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201050_0008_m_000000_0: Committed
20:10:50.317 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 39). 2572 bytes result sent to driver
20:10:50.318 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 39) in 218 ms on localhost (executor driver) (3/4)
20:10:50.320 INFO  [Executor task launch worker for task 41] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925201050_0008_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/_temporary/0/task_20190925201050_0008_m_000002
20:10:50.320 INFO  [Executor task launch worker for task 41] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925201050_0008_m_000002_0: Committed
20:10:50.320 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 2.0 in stage 8.0 (TID 41). 2572 bytes result sent to driver
20:10:50.321 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 8.0 (TID 41) in 221 ms on localhost (executor driver) (4/4)
20:10:50.321 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
20:10:50.321 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 8 (insertInto at SparkHelper.scala:35) finished in 0.221 s
20:10:50.321 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: insertInto at SparkHelper.scala:35, took 0.947856 s
20:10:50.349 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
20:10:50.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:50.350 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:50.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:50.366 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:50.381 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.382 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:50.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:50.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:50.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:50.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:50.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:50.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:50.398 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:50.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:50.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
20:10:50.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
20:10:50.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:50.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:50.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
20:10:50.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
20:10:50.432 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/bdp_day=20190924/part-00000-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00000-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, Status:true
20:10:50.440 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/bdp_day=20190924/part-00001-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00001-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, Status:true
20:10:50.447 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/bdp_day=20190924/part-00002-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00002-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, Status:true
20:10:50.458 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/bdp_day=20190924/part-00003-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00003-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000, Status:true
20:10:50.462 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
20:10:50.462 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
20:10:50.470 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_register_users[20190924]
20:10:50.470 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_register_users[20190924]	
20:10:50.484 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
20:10:50.486 WARN  [main] hive.log - Updated size to 872108
20:10:50.577 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-25_20-10-49_161_5065846800403727843-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
20:10:50.578 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
20:10:50.579 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
20:10:50.579 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
20:10:50.583 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:50.583 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:50.599 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
20:10:50.599 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
20:10:50.617 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.617 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.617 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.618 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.618 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.618 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.618 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.618 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
20:10:50.618 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
20:10:50.627 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
20:10:50.633 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@46ce4881{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20:10:50.636 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
20:10:50.645 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
20:10:50.804 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
20:10:50.804 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
20:10:50.805 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
20:10:50.807 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
20:10:50.813 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
20:10:50.813 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
20:10:50.814 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-06e113bb-23f7-4732-9b35-9faeba82b464
