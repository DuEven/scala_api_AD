16:24:28.950 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:24:30.990 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:24:31.414 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:24:31.422 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:24:31.427 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:24:31.427 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:24:31.439 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:24:33.427 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3092.
16:24:33.526 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:24:33.658 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:24:33.677 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:24:33.678 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:24:33.692 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-8ac3abad-be6d-4180-b709-df411e332192
16:24:33.819 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:24:34.036 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:24:34.352 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @11156ms
16:24:34.604 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:24:34.656 INFO  [main] org.spark_project.jetty.server.Server - Started @11461ms
16:24:34.743 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:24:34.744 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:24:34.792 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
16:24:34.793 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
16:24:34.793 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
16:24:34.794 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
16:24:34.794 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
16:24:34.795 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
16:24:34.795 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
16:24:34.797 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
16:24:34.797 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
16:24:34.798 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
16:24:34.798 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
16:24:34.799 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
16:24:34.800 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
16:24:34.800 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
16:24:34.801 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
16:24:34.801 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
16:24:34.802 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
16:24:34.803 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
16:24:34.803 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
16:24:34.804 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:24:34.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
16:24:34.814 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
16:24:34.816 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
16:24:34.816 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
16:24:34.817 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
16:24:34.819 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:24:35.173 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:24:35.257 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3133.
16:24:35.283 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3133
16:24:35.294 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:24:35.388 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.411 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3133 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.417 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.417 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3133, None)
16:24:35.711 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
16:24:38.685 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:24:38.706 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:24:38.707 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:24:38.725 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL,null,AVAILABLE,@Spark}
16:24:38.726 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL/json,null,AVAILABLE,@Spark}
16:24:38.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution,null,AVAILABLE,@Spark}
16:24:38.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23468512{/SQL/execution/json,null,AVAILABLE,@Spark}
16:24:38.731 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53747c4a{/static/sql,null,AVAILABLE,@Spark}
16:24:40.021 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:24:41.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:24:41.257 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:24:43.479 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:24:44.776 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:24:44.779 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:24:45.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:24:45.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:24:45.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:24:45.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:24:45.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:24:45.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:24:45.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:24:45.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:24:45.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:24:45.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:24:45.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:24:45.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:24:45.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:24:51.288 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507
16:24:51.316 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507
16:24:51.318 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/607e9f8d-cbe9-4841-8a67-3df0165efd8b_resources
16:24:51.321 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/607e9f8d-cbe9-4841-8a67-3df0165efd8b
16:24:51.325 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/607e9f8d-cbe9-4841-8a67-3df0165efd8b
16:24:51.329 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/607e9f8d-cbe9-4841-8a67-3df0165efd8b/_tmp_space.db
16:24:51.345 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:24:51.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:24:51.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:24:51.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:24:51.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:24:51.504 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:24:51.981 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/3f18d854-f1d1-45e1-b03b-80e1b7586e92_resources
16:24:51.986 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3f18d854-f1d1-45e1-b03b-80e1b7586e92
16:24:51.990 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/3f18d854-f1d1-45e1-b03b-80e1b7586e92
16:24:51.995 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/3f18d854-f1d1-45e1-b03b-80e1b7586e92/_tmp_space.db
16:24:51.999 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:24:52.169 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:24:52.186 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:24:52.597 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:24:52.598 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:24:52.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:52.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:53.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.312 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.314 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.314 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.314 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:53.365 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:24:53.907 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table ods_release.ods_01_release_session (inference mode: INFER_AND_SAVE)
16:24:53.918 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:24:53.919 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:24:53.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:53.923 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.943 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:53.943 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:53.964 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.964 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.965 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.966 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.966 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:53.966 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:54.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
16:24:54.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
16:24:55.839 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
16:24:55.908 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
16:24:55.909 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
16:24:55.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:24:55.911 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:24:55.966 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
16:24:56.234 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 74.4 KB, free 1988.6 MB)
16:24:56.781 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.6 MB)
16:24:56.814 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:3133 (size: 26.8 KB, free: 1988.7 MB)
16:24:56.853 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
16:24:56.993 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
16:24:56.995 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
16:24:57.157 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5019 bytes)
16:24:57.237 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:24:58.889 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1781 bytes result sent to driver
16:24:58.905 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1772 ms on localhost (executor driver) (1/1)
16:24:58.908 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:24:58.926 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 1.815 s
16:24:58.930 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 3.090782 s
16:24:59.019 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table ods_release.ods_01_release_session
16:24:59.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:24:59.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:24:59.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.026 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:59.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.190 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:24:59.191 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:24:59.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:24:59.274 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:24:59.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session
16:24:59.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session	
16:25:00.214 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:25:00.226 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:25:00.227 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:25:00.227 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:25:00.228 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:25:00.228 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:25:00.229 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:25:00.251 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:25:00.279 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:25:00.282 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:25:00.283 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:25:00.284 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:25:00.284 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:25:00.285 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:25:00.286 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:25:00.287 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:25:00.288 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:25:00.288 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:25:00.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.316 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.367 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.382 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.391 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.392 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:25:00.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:25:00.594 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:25:00.625 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:25:00.628 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:25:00.695 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:25:00.768 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:25:00.769 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:25:00.785 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:25:00.792 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:25:00.805 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:25:00.809 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:25:00.810 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-d9b9e097-5052-446d-bde7-fba37b69038e
16:29:04.166 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:29:04.448 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:29:04.467 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:29:04.467 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:29:04.468 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:29:04.468 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:29:04.469 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:29:05.292 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3456.
16:29:05.312 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:29:05.329 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:29:05.332 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:29:05.332 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:29:05.342 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-199be220-6cc4-436a-ad2a-8ffe69acdae5
16:29:05.359 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:29:05.404 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:29:05.487 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3790ms
16:29:05.556 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:29:05.571 INFO  [main] org.spark_project.jetty.server.Server - Started @3875ms
16:29:05.596 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6de0a959{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:29:05.596 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:29:05.618 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6650813a{/jobs,null,AVAILABLE,@Spark}
16:29:05.619 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30272916{/jobs/json,null,AVAILABLE,@Spark}
16:29:05.619 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/job,null,AVAILABLE,@Spark}
16:29:05.620 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job/json,null,AVAILABLE,@Spark}
16:29:05.621 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/stages,null,AVAILABLE,@Spark}
16:29:05.622 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages/json,null,AVAILABLE,@Spark}
16:29:05.623 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/stage,null,AVAILABLE,@Spark}
16:29:05.624 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e8f862{/stages/stage/json,null,AVAILABLE,@Spark}
16:29:05.625 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/pool,null,AVAILABLE,@Spark}
16:29:05.626 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool/json,null,AVAILABLE,@Spark}
16:29:05.626 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/storage,null,AVAILABLE,@Spark}
16:29:05.627 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage/json,null,AVAILABLE,@Spark}
16:29:05.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/rdd,null,AVAILABLE,@Spark}
16:29:05.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd/json,null,AVAILABLE,@Spark}
16:29:05.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/environment,null,AVAILABLE,@Spark}
16:29:05.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment/json,null,AVAILABLE,@Spark}
16:29:05.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/executors,null,AVAILABLE,@Spark}
16:29:05.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors/json,null,AVAILABLE,@Spark}
16:29:05.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/threadDump,null,AVAILABLE,@Spark}
16:29:05.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:29:05.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/static,null,AVAILABLE,@Spark}
16:29:05.640 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@350b3a17{/,null,AVAILABLE,@Spark}
16:29:05.641 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/api,null,AVAILABLE,@Spark}
16:29:05.642 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/jobs/job/kill,null,AVAILABLE,@Spark}
16:29:05.643 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/stages/stage/kill,null,AVAILABLE,@Spark}
16:29:05.645 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:29:05.717 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:29:05.746 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3497.
16:29:05.746 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3497
16:29:05.747 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:29:05.781 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.784 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3497 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.787 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.788 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3497, None)
16:29:05.961 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@529cfee5{/metrics/json,null,AVAILABLE,@Spark}
16:29:07.335 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:29:07.362 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:29:07.363 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:29:07.370 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19489b27{/SQL,null,AVAILABLE,@Spark}
16:29:07.370 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL/json,null,AVAILABLE,@Spark}
16:29:07.371 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL/execution,null,AVAILABLE,@Spark}
16:29:07.371 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution/json,null,AVAILABLE,@Spark}
16:29:07.372 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69cd7630{/static/sql,null,AVAILABLE,@Spark}
16:29:07.855 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:29:08.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:29:08.550 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:29:09.926 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:29:11.511 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:29:11.514 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:29:11.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:29:11.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:29:11.894 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:29:11.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:29:11.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:29:12.002 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:29:12.003 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:29:12.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:29:12.371 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:29:12.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:29:12.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:29:12.379 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:29:12.379 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:29:12.882 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/f4365138-42ea-4731-be33-e2ada62905aa_resources
16:29:12.927 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f4365138-42ea-4731-be33-e2ada62905aa
16:29:12.932 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/f4365138-42ea-4731-be33-e2ada62905aa
16:29:12.937 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f4365138-42ea-4731-be33-e2ada62905aa/_tmp_space.db
16:29:12.942 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:29:12.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:12.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:12.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:29:12.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:29:12.979 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:29:13.178 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/ec3b6d36-39eb-4119-aa5f-f0ebd102590f_resources
16:29:13.204 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/ec3b6d36-39eb-4119-aa5f-f0ebd102590f
16:29:13.208 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/ec3b6d36-39eb-4119-aa5f-f0ebd102590f
16:29:13.213 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/ec3b6d36-39eb-4119-aa5f-f0ebd102590f/_tmp_space.db
16:29:13.215 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:29:13.238 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:29:13.243 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:29:13.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:29:13.441 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:29:13.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:29:13.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:29:13.553 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:29:13.554 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:29:13.588 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.603 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.603 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.604 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.604 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.604 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:29:13.605 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:29:13.622 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:29:14.010 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:29:14.022 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:29:14.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:29:14.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:29:14.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:29:14.024 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:29:14.024 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:29:14.039 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:29:14.064 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:29:14.067 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:29:14.067 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:29:14.068 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:29:14.069 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:29:14.070 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:29:14.071 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:29:14.072 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:29:14.073 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:29:14.074 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:29:14.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.105 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.135 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.141 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.142 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:29:14.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:29:14.285 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:29:14.293 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6de0a959{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:29:14.296 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:29:14.307 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:29:14.320 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:29:14.321 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:29:14.330 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:29:14.334 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:29:14.339 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:29:14.340 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:29:14.341 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-8178cb57-9c23-42b2-af3a-8a267a597b7e
16:34:43.542 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:34:43.851 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:34:43.871 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:34:43.872 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:34:43.872 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:34:43.873 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:34:43.873 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:34:44.741 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3593.
16:34:44.761 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:34:44.778 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:34:44.782 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:34:44.783 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:34:44.792 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-a267a7a4-5f70-4fa6-9d46-dbe4aacffe49
16:34:44.811 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:34:44.861 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:34:44.956 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4212ms
16:34:45.027 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:34:45.041 INFO  [main] org.spark_project.jetty.server.Server - Started @4300ms
16:34:45.070 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:34:45.071 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:34:45.097 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
16:34:45.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
16:34:45.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
16:34:45.099 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
16:34:45.100 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
16:34:45.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
16:34:45.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
16:34:45.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
16:34:45.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
16:34:45.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
16:34:45.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
16:34:45.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
16:34:45.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
16:34:45.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
16:34:45.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
16:34:45.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
16:34:45.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
16:34:45.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
16:34:45.109 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
16:34:45.109 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:34:45.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
16:34:45.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
16:34:45.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
16:34:45.120 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
16:34:45.121 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
16:34:45.123 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:34:45.205 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:34:45.235 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3634.
16:34:45.236 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3634
16:34:45.237 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:34:45.273 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.275 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3634 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.278 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.278 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3634, None)
16:34:45.451 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
16:34:46.984 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:34:47.011 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:34:47.011 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:34:47.019 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
16:34:47.019 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
16:34:47.020 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
16:34:47.021 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
16:34:47.023 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
16:34:47.505 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:34:48.197 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:34:48.225 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:34:49.483 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:34:50.798 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:34:50.801 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:34:51.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:34:51.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:34:51.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:34:51.233 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:34:51.234 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:34:51.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:34:51.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:34:51.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:34:51.337 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:34:51.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:34:51.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:34:51.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:34:51.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:34:51.879 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/09ba03b2-8a78-4f6a-8bee-a983da4e3493_resources
16:34:51.888 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/09ba03b2-8a78-4f6a-8bee-a983da4e3493
16:34:51.893 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/09ba03b2-8a78-4f6a-8bee-a983da4e3493
16:34:51.901 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/09ba03b2-8a78-4f6a-8bee-a983da4e3493/_tmp_space.db
16:34:51.906 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:34:51.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:51.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:51.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:34:51.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:34:51.943 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:34:52.124 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/b880db71-db9d-4954-9200-35833409f5f8_resources
16:34:52.201 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b880db71-db9d-4954-9200-35833409f5f8
16:34:52.205 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/b880db71-db9d-4954-9200-35833409f5f8
16:34:52.215 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b880db71-db9d-4954-9200-35833409f5f8/_tmp_space.db
16:34:52.219 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:34:52.243 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:34:52.249 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:34:52.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:34:52.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:34:52.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:34:52.457 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:34:52.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:34:52.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:34:52.580 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.592 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.593 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:34:52.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:34:52.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:34:52.952 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:34:52.967 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:34:52.968 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:34:52.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:34:52.983 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:34:53.012 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:34:53.015 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:34:53.017 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:34:53.018 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:34:53.019 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:34:53.020 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:34:53.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:34:53.022 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:34:53.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:34:53.023 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:34:53.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.093 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.112 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.116 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:34:53.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:34:53.242 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:34:53.249 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:34:53.252 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:34:53.264 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:34:53.278 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:34:53.278 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:34:53.286 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:34:53.290 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:34:53.296 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:34:53.296 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:34:53.297 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-d1d0e69e-4e01-4166-959b-32ffc76309b7
16:35:12.880 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:35:13.235 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:35:13.254 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:35:13.255 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:35:13.255 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:35:13.255 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:35:13.256 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:35:14.163 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3697.
16:35:14.181 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:35:14.199 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:35:14.204 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:35:14.204 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:35:14.214 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-5754cc95-f6f9-473d-adb0-6f78b45f0069
16:35:14.233 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:35:14.283 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:35:14.370 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4203ms
16:35:14.436 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:35:14.457 INFO  [main] org.spark_project.jetty.server.Server - Started @4291ms
16:35:14.480 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:35:14.480 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:35:14.506 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@450794b4{/jobs,null,AVAILABLE,@Spark}
16:35:14.507 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/json,null,AVAILABLE,@Spark}
16:35:14.508 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e7095ac{/jobs/job,null,AVAILABLE,@Spark}
16:35:14.509 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job/json,null,AVAILABLE,@Spark}
16:35:14.509 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages,null,AVAILABLE,@Spark}
16:35:14.510 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/json,null,AVAILABLE,@Spark}
16:35:14.510 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages/stage,null,AVAILABLE,@Spark}
16:35:14.511 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage/json,null,AVAILABLE,@Spark}
16:35:14.512 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool,null,AVAILABLE,@Spark}
16:35:14.513 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/pool/json,null,AVAILABLE,@Spark}
16:35:14.514 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage,null,AVAILABLE,@Spark}
16:35:14.514 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/json,null,AVAILABLE,@Spark}
16:35:14.515 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd,null,AVAILABLE,@Spark}
16:35:14.515 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/rdd/json,null,AVAILABLE,@Spark}
16:35:14.516 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment,null,AVAILABLE,@Spark}
16:35:14.517 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/environment/json,null,AVAILABLE,@Spark}
16:35:14.518 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors,null,AVAILABLE,@Spark}
16:35:14.519 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/json,null,AVAILABLE,@Spark}
16:35:14.519 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump,null,AVAILABLE,@Spark}
16:35:14.520 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:35:14.525 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/static,null,AVAILABLE,@Spark}
16:35:14.526 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/,null,AVAILABLE,@Spark}
16:35:14.527 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d52e3ef{/api,null,AVAILABLE,@Spark}
16:35:14.527 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/jobs/job/kill,null,AVAILABLE,@Spark}
16:35:14.528 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b50150{/stages/stage/kill,null,AVAILABLE,@Spark}
16:35:14.531 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:35:14.612 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:35:14.654 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3738.
16:35:14.655 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3738
16:35:14.657 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:35:14.691 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.694 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3738 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.697 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.697 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3738, None)
16:35:14.870 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@748fe51d{/metrics/json,null,AVAILABLE,@Spark}
16:35:16.397 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:35:16.420 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:35:16.421 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:35:16.426 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL,null,AVAILABLE,@Spark}
16:35:16.427 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@193bb809{/SQL/json,null,AVAILABLE,@Spark}
16:35:16.429 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution,null,AVAILABLE,@Spark}
16:35:16.429 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/execution/json,null,AVAILABLE,@Spark}
16:35:16.432 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3caafa67{/static/sql,null,AVAILABLE,@Spark}
16:35:16.927 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:35:17.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:35:17.668 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:35:18.836 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:35:20.258 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:35:20.261 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:35:20.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:35:20.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:35:20.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:35:20.638 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:35:20.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:35:20.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:35:20.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:35:20.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:35:20.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:35:20.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:35:20.708 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:35:20.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:35:20.714 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:35:21.315 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/74626c68-d377-4dc1-b79b-84710f6fa6f5_resources
16:35:21.349 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/74626c68-d377-4dc1-b79b-84710f6fa6f5
16:35:21.352 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/74626c68-d377-4dc1-b79b-84710f6fa6f5
16:35:21.356 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/74626c68-d377-4dc1-b79b-84710f6fa6f5/_tmp_space.db
16:35:21.360 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:35:21.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:21.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:21.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:35:21.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:35:21.393 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:35:21.580 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/776b2ee1-649e-4362-b2c9-1c85d68eecae_resources
16:35:21.584 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/776b2ee1-649e-4362-b2c9-1c85d68eecae
16:35:21.587 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/776b2ee1-649e-4362-b2c9-1c85d68eecae
16:35:21.595 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/776b2ee1-649e-4362-b2c9-1c85d68eecae/_tmp_space.db
16:35:21.599 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:35:21.642 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:35:21.651 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:35:21.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:35:21.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:35:21.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:21.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:21.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:21.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:22.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.036 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:22.049 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:35:22.386 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:35:22.399 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:35:22.399 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:35:22.400 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:35:22.400 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:35:22.401 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:35:22.402 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:35:22.417 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:35:22.445 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:35:22.448 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:35:22.449 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:35:22.449 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:35:22.451 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:35:22.453 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:35:22.454 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:35:22.455 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:35:22.455 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:35:22.456 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:35:22.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.461 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.475 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:35:22.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:35:22.708 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:35:22.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:22.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:22.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.778 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.779 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.780 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:22.781 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:23.566 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1
16:35:24.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:35:24.021 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:35:24.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:24.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:24.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:35:24.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:24.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:24.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:35:24.095 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:35:24.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:35:24.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:35:24.607 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
16:35:24.608 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:35:24.611 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:35:24.688 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:35:24.705 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:35:25.095 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:25.096 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:26.009 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 590.912862 ms
16:35:26.087 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 64.994816 ms
16:35:26.285 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
16:35:26.414 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
16:35:26.418 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:3738 (size: 26.3 KB, free: 1988.7 MB)
16:35:26.421 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from insertInto at SparkHelper.scala:35
16:35:26.532 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
16:35:26.823 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
16:35:26.837 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (insertInto at SparkHelper.scala:35)
16:35:26.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (insertInto at SparkHelper.scala:35) with 4 output partitions
16:35:26.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (insertInto at SparkHelper.scala:35)
16:35:26.840 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:35:26.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
16:35:26.877 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35), which has no missing parents
16:35:27.050 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1988.3 MB)
16:35:27.054 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.3 MB)
16:35:27.055 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:3738 (size: 10.4 KB, free: 1988.7 MB)
16:35:27.055 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:35:27.067 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
16:35:27.068 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
16:35:27.108 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
16:35:27.111 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
16:35:27.112 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
16:35:27.112 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
16:35:27.114 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
16:35:27.115 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
16:35:27.116 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
16:35:27.116 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
16:35:27.126 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
16:35:27.126 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
16:35:27.126 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:35:27.126 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
16:35:27.126 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
16:35:27.126 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
16:35:27.126 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
16:35:27.126 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
16:35:27.346 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:35:27.404 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.42008 ms
16:35:27.458 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
16:35:27.459 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
16:35:27.458 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
16:35:28.814 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.814 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:28.857 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:35:29.243 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1610 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1610 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1653 bytes result sent to driver
16:35:29.243 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1610 bytes result sent to driver
16:35:29.251 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 2134 ms on localhost (executor driver) (1/8)
16:35:29.253 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 2139 ms on localhost (executor driver) (2/8)
16:35:29.254 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 2139 ms on localhost (executor driver) (3/8)
16:35:29.254 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 2141 ms on localhost (executor driver) (4/8)
16:35:29.254 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 2143 ms on localhost (executor driver) (5/8)
16:35:29.254 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2155 ms on localhost (executor driver) (6/8)
16:35:29.254 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 2143 ms on localhost (executor driver) (7/8)
16:35:33.127 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1739 bytes result sent to driver
16:35:33.128 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 6016 ms on localhost (executor driver) (8/8)
16:35:33.130 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:35:33.131 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (insertInto at SparkHelper.scala:35) finished in 6.042 s
16:35:33.131 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:35:33.132 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:35:33.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
16:35:33.144 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:35:33.148 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35), which has no missing parents
16:35:33.215 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
16:35:33.217 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 66.1 KB, free 1988.1 MB)
16:35:33.218 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:3738 (size: 66.1 KB, free: 1988.6 MB)
16:35:33.219 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:35:33.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
16:35:33.221 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 4 tasks
16:35:33.224 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:35:33.224 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:35:33.224 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 10, localhost, executor driver, partition 2, ANY, 4726 bytes)
16:35:33.225 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 11, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:35:33.225 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 3.0 in stage 1.0 (TID 11)
16:35:33.225 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 9)
16:35:33.225 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
16:35:33.225 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 10)
16:35:33.320 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.320 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.320 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.320 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:35:33.323 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.323 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.323 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.323 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
16:35:33.381 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.908554 ms
16:35:33.419 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.436556 ms
16:35:33.678 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.678 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.679 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.679 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.680 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.680 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:35:33.681 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.681 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:35:33.705 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.9126 ms
16:35:33.763 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.693157 ms
16:35:33.783 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.539766 ms
16:35:34.093 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@79d6f404
16:35:34.094 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4bfae730
16:35:34.094 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3763a9fd
16:35:34.094 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a365b31
16:35:34.126 INFO  [Executor task launch worker for task 9] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16:35:34.156 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:35:34.156 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000003_0/bdp_day=20190924/part-00003-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.156 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000001_0/bdp_day=20190924/part-00001-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.156 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000000_0/bdp_day=20190924/part-00000-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.156 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163533_0001_m_000002_0/bdp_day=20190924/part-00002-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000
16:35:34.179 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.179 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.179 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.179 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression set to false
16:35:34.180 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.180 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.180 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.180 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:35:34.183 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:35:34.183 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.183 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.183 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:35:34.184 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.184 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.184 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.184 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:35:34.185 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Validation is off
16:35:34.185 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.185 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.186 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.186 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:35:34.626 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5d2b30af
16:35:34.626 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@294c5f99
16:35:34.627 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@363379ee
16:35:34.627 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@27127132
16:35:35.021 INFO  [Executor task launch worker for task 8] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,597
16:35:35.024 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,547
16:35:35.026 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,984
16:35:35.031 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,815
16:35:35.328 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.328 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.328 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.328 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.330 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.330 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.330 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.330 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.331 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.331 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.331 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.331 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.332 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,447 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:35:35.332 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.332 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:35:35.332 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.332 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.332 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:35:35.334 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
16:35:35.334 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:35:35.334 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,784B for [gender] BINARY: 21,446 values, 2,753B raw, 2,753B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,446 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,786B for [gender] BINARY: 21,447 values, 2,755B raw, 2,755B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,789B for [gender] BINARY: 21,446 values, 2,758B raw, 2,758B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,247B for [area_code] BINARY: 21,446 values, 3,206B raw, 3,206B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 3,213B for [area_code] BINARY: 21,446 values, 3,172B raw, 3,172B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,222B for [area_code] BINARY: 21,447 values, 3,181B raw, 3,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,227B for [area_code] BINARY: 21,446 values, 3,186B raw, 3,186B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:35:35.335 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:35:35.335 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:35:35.336 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.336 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.336 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.337 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.337 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.337 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.341 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.342 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:35:35.343 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.343 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.344 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.344 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:35:35.344 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:35.345 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:35.345 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:35.345 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:35:36.564 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000001
16:35:36.564 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000002
16:35:36.565 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000001_0: Committed
16:35:36.565 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000002_0: Committed
16:35:36.594 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 9). 2615 bytes result sent to driver
16:35:36.594 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 10). 2615 bytes result sent to driver
16:35:36.595 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 9) in 3371 ms on localhost (executor driver) (1/4)
16:35:36.596 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 10) in 3372 ms on localhost (executor driver) (2/4)
16:35:38.470 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000003
16:35:38.470 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000003_0: Committed
16:35:38.471 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 3.0 in stage 1.0 (TID 11). 2572 bytes result sent to driver
16:35:38.472 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163533_0001_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/_temporary/0/task_20190925163533_0001_m_000000
16:35:38.472 INFO  [Executor task launch worker for task 8] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163533_0001_m_000000_0: Committed
16:35:38.472 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 11) in 5247 ms on localhost (executor driver) (3/4)
16:35:38.474 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 2572 bytes result sent to driver
16:35:38.475 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 5253 ms on localhost (executor driver) (4/4)
16:35:38.475 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:35:38.475 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (insertInto at SparkHelper.scala:35) finished in 5.254 s
16:35:38.479 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: insertInto at SparkHelper.scala:35, took 11.655995 s
16:35:38.555 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
16:35:38.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.610 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.611 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:38.612 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:38.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:35:38.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:35:38.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:38.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:38.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:35:38.699 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:35:38.807 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
16:35:38.940 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00000-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00000-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.031 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00001-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00001-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.044 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00002-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00002-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.056 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924/part-00003-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00003-0a281146-86fd-4e19-9d42-d26b1b2b88b4.c000, Status:true
16:35:39.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:35:39.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:35:39.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
16:35:39.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
16:35:39.089 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
16:35:39.149 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
16:35:39.153 WARN  [main] hive.log - Updated size to 5782457
16:35:39.478 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-35-23_564_8616767954273283142-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
16:35:39.504 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
16:35:39.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:35:39.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:35:39.512 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.513 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.541 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.565 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:39.634 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dw_release.dw_release_customer (inference mode: INFER_AND_SAVE)
16:35:39.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:35:39.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:35:39.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:39.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:39.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:39.705 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dw_release tbl=dw_release_customer
16:35:39.705 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions : db=dw_release tbl=dw_release_customer	
16:35:39.821 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:35) with 1 output partitions
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (insertInto at SparkHelper.scala:35)
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:35:39.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35), which has no missing parents
16:35:39.836 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 74.4 KB, free 1988.0 MB)
16:35:39.838 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.8 KB, free 1988.0 MB)
16:35:39.838 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:3738 (size: 26.8 KB, free: 1988.6 MB)
16:35:39.839 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1004
16:35:39.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0))
16:35:39.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
16:35:39.842 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5050 bytes)
16:35:39.844 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 12)
16:35:40.469 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 12). 1978 bytes result sent to driver
16:35:40.469 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 12) in 629 ms on localhost (executor driver) (1/1)
16:35:40.470 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
16:35:40.470 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (insertInto at SparkHelper.scala:35) finished in 0.630 s
16:35:40.470 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:35, took 0.649859 s
16:35:40.478 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dw_release.dw_release_customer
16:35:40.479 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:35:40.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:35:40.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.539 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.540 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:40.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.546 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.596 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.597 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.598 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.599 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:35:40.600 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:35:40.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:35:40.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:35:40.695 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer
16:35:40.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_table: db=dw_release tbl=dw_release_customer newtbl=dw_release_customer	
16:35:40.921 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:35:40.929 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3b718392{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:35:40.931 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:35:40.965 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:35:41.049 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:35:41.050 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:35:41.050 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:35:41.054 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:35:41.059 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:35:41.060 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:35:41.061 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-053b050f-f2c3-41ac-9dfb-4c8a2fb0885a
16:38:52.196 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:38:52.483 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:38:52.501 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:38:52.501 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:38:52.503 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:38:52.503 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:38:52.503 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:38:53.300 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 3868.
16:38:53.324 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:38:53.341 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:38:53.344 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:38:53.344 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:38:53.354 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-67ef129d-28ad-4fb1-b121-00b5ee312650
16:38:53.372 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:38:53.415 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:38:53.497 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3789ms
16:38:53.574 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:38:53.589 INFO  [main] org.spark_project.jetty.server.Server - Started @3882ms
16:38:53.610 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@27e358a7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:38:53.611 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:38:53.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50cf5a23{/jobs,null,AVAILABLE,@Spark}
16:38:53.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/json,null,AVAILABLE,@Spark}
16:38:53.633 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/jobs/job,null,AVAILABLE,@Spark}
16:38:53.634 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/job/json,null,AVAILABLE,@Spark}
16:38:53.634 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages,null,AVAILABLE,@Spark}
16:38:53.635 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/json,null,AVAILABLE,@Spark}
16:38:53.636 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/stage,null,AVAILABLE,@Spark}
16:38:53.637 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/stage/json,null,AVAILABLE,@Spark}
16:38:53.637 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool,null,AVAILABLE,@Spark}
16:38:53.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/pool/json,null,AVAILABLE,@Spark}
16:38:53.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage,null,AVAILABLE,@Spark}
16:38:53.638 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/json,null,AVAILABLE,@Spark}
16:38:53.639 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd,null,AVAILABLE,@Spark}
16:38:53.639 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd/json,null,AVAILABLE,@Spark}
16:38:53.640 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment,null,AVAILABLE,@Spark}
16:38:53.640 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/environment/json,null,AVAILABLE,@Spark}
16:38:53.641 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors,null,AVAILABLE,@Spark}
16:38:53.642 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/json,null,AVAILABLE,@Spark}
16:38:53.643 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump,null,AVAILABLE,@Spark}
16:38:53.643 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:38:53.651 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/static,null,AVAILABLE,@Spark}
16:38:53.652 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/,null,AVAILABLE,@Spark}
16:38:53.654 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/api,null,AVAILABLE,@Spark}
16:38:53.655 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/jobs/job/kill,null,AVAILABLE,@Spark}
16:38:53.656 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/stages/stage/kill,null,AVAILABLE,@Spark}
16:38:53.658 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:38:53.730 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:38:53.757 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3909.
16:38:53.757 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:3909
16:38:53.758 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:38:53.791 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.794 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:3909 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.796 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.797 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 3909, None)
16:38:53.965 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319854f0{/metrics/json,null,AVAILABLE,@Spark}
16:38:55.362 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:38:55.386 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:38:55.386 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:38:55.392 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d5a1588{/SQL,null,AVAILABLE,@Spark}
16:38:55.392 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@125d47c4{/SQL/json,null,AVAILABLE,@Spark}
16:38:55.393 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/execution,null,AVAILABLE,@Spark}
16:38:55.393 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f2466c{/SQL/execution/json,null,AVAILABLE,@Spark}
16:38:55.395 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b53840a{/static/sql,null,AVAILABLE,@Spark}
16:38:55.852 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:38:56.502 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:38:56.533 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:38:57.630 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:38:58.867 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:38:58.869 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:38:59.088 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:38:59.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:38:59.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:38:59.246 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:38:59.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:38:59.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:38:59.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:38:59.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:38:59.301 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:38:59.305 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:38:59.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:38:59.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:38:59.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:38:59.809 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/bff0e9f9-04d5-4c09-a664-2295d277892e_resources
16:38:59.820 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/bff0e9f9-04d5-4c09-a664-2295d277892e
16:38:59.823 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/bff0e9f9-04d5-4c09-a664-2295d277892e
16:38:59.828 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/bff0e9f9-04d5-4c09-a664-2295d277892e/_tmp_space.db
16:38:59.832 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:38:59.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:38:59.856 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:38:59.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:38:59.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:38:59.867 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:39:00.025 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/8a95594f-8c29-4d1b-903a-273c1c0d79d8_resources
16:39:00.037 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8a95594f-8c29-4d1b-903a-273c1c0d79d8
16:39:00.040 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/8a95594f-8c29-4d1b-903a-273c1c0d79d8
16:39:00.100 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8a95594f-8c29-4d1b-903a-273c1c0d79d8/_tmp_space.db
16:39:00.103 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:39:00.127 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:39:00.132 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:39:00.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:39:00.323 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:39:00.330 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:00.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:00.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:00.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:00.449 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.461 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.462 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.462 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.462 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:00.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:00.477 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:39:00.764 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:39:00.775 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:39:00.776 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:39:00.777 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:39:00.777 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:39:00.778 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:39:00.778 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:39:00.792 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:39:00.816 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:39:00.819 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:39:00.820 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:39:00.821 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:39:00.822 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:39:00.823 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:39:00.823 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:39:00.824 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:39:00.825 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:39:00.825 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:39:00.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.838 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.843 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.844 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.878 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.879 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.883 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.893 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.899 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:00.904 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:39:00.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:39:01.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
16:39:01.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:01.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:01.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.103 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.104 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:01.408 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1
16:39:01.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:39:01.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:39:01.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:01.611 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:01.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:39:01.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:39:01.656 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.657 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:01.658 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:01.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:39:01.675 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:39:01.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:39:01.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:39:01.873 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
16:39:01.876 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:39:01.878 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:39:01.889 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:39:01.893 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:39:01.950 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:01.951 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:02.356 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 248.779258 ms
16:39:02.407 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 39.86255 ms
16:39:02.498 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
16:39:02.648 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
16:39:02.659 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:3909 (size: 26.3 KB, free: 1988.7 MB)
16:39:02.662 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from insertInto at SparkHelper.scala:35
16:39:02.669 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
16:39:02.841 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
16:39:02.854 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (insertInto at SparkHelper.scala:35)
16:39:02.856 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (insertInto at SparkHelper.scala:35) with 4 output partitions
16:39:02.856 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (insertInto at SparkHelper.scala:35)
16:39:02.857 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:39:02.859 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
16:39:02.863 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35), which has no missing parents
16:39:02.956 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1988.3 MB)
16:39:02.961 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.3 MB)
16:39:02.962 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:3909 (size: 10.4 KB, free: 1988.7 MB)
16:39:02.962 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:39:02.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
16:39:02.973 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
16:39:03.010 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
16:39:03.012 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
16:39:03.013 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
16:39:03.014 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
16:39:03.014 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
16:39:03.015 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
16:39:03.016 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
16:39:03.016 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
16:39:03.025 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
16:39:03.025 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:39:03.025 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
16:39:03.025 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
16:39:03.025 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
16:39:03.025 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
16:39:03.025 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
16:39:03.025 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
16:39:03.057 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:39:03.201 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 38.819191 ms
16:39:03.220 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
16:39:03.220 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
16:39:03.221 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
16:39:03.221 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
16:39:04.423 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.423 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.424 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.454 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.501 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:39:04.540 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1567 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1653 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1610 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1653 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1653 bytes result sent to driver
16:39:04.540 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1610 bytes result sent to driver
16:39:04.544 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1567 bytes result sent to driver
16:39:04.550 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1534 ms on localhost (executor driver) (1/8)
16:39:04.553 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1537 ms on localhost (executor driver) (2/8)
16:39:04.553 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1542 ms on localhost (executor driver) (3/8)
16:39:04.553 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1539 ms on localhost (executor driver) (4/8)
16:39:04.554 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1553 ms on localhost (executor driver) (5/8)
16:39:04.554 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1539 ms on localhost (executor driver) (6/8)
16:39:04.554 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1542 ms on localhost (executor driver) (7/8)
16:39:07.184 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1739 bytes result sent to driver
16:39:07.185 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 4172 ms on localhost (executor driver) (8/8)
16:39:07.187 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:39:07.187 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (insertInto at SparkHelper.scala:35) finished in 4.195 s
16:39:07.188 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:39:07.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:39:07.189 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
16:39:07.190 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:39:07.192 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35), which has no missing parents
16:39:07.252 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
16:39:07.254 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 66.0 KB, free 1988.1 MB)
16:39:07.254 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:3909 (size: 66.0 KB, free: 1988.6 MB)
16:39:07.255 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:39:07.256 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
16:39:07.256 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 4 tasks
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, ANY, 4726 bytes)
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 10, localhost, executor driver, partition 2, ANY, 4726 bytes)
16:39:07.259 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 11, localhost, executor driver, partition 3, ANY, 4726 bytes)
16:39:07.260 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 9)
16:39:07.260 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 3.0 in stage 1.0 (TID 11)
16:39:07.260 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 10)
16:39:07.260 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
16:39:07.313 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.313 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.313 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.313 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:39:07.316 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.316 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.316 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.316 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
16:39:07.346 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.099467 ms
16:39:07.368 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.961888 ms
16:39:07.553 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.553 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.553 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.553 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.553 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
16:39:07.554 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.554 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.554 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16:39:07.565 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.128104 ms
16:39:07.610 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.865596 ms
16:39:07.630 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.52505 ms
16:39:07.729 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b9d2c24
16:39:07.730 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d7ea8e5
16:39:07.730 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@523b7748
16:39:07.730 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ca1349a
16:39:07.743 INFO  [Executor task launch worker for task 9] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
16:39:07.748 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.748 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.748 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.748 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
16:39:07.749 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000000_0/bdp_day=20190924/part-00000-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.749 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000002_0/bdp_day=20190924/part-00002-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.749 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000001_0/bdp_day=20190924/part-00001-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.749 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/_temporary/attempt_20190925163907_0001_m_000003_0/bdp_day=20190924/part-00003-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000
16:39:07.751 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.751 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.751 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.751 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
16:39:07.752 INFO  [Executor task launch worker for task 8] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.752 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.752 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.752 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
16:39:07.753 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
16:39:07.753 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
16:39:07.754 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
16:39:07.754 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.754 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.754 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.754 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
16:39:07.755 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.755 INFO  [Executor task launch worker for task 8] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.755 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.755 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
16:39:07.975 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6605ac4d
16:39:07.976 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@68164763
16:39:07.976 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3d94f215
16:39:07.977 INFO  [Executor task launch worker for task 8] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@383cabdc
16:39:08.393 INFO  [Executor task launch worker for task 8] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,597
16:39:08.393 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,984
16:39:08.396 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,547
16:39:08.399 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,815
16:39:08.535 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.535 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.535 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.535 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.536 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.536 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.536 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.536 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.537 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.537 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.537 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.538 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.538 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,447 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 3 entries, 15B raw, 3B comp}
16:39:08.538 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.538 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.541 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.541 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.541 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.542 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.542 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 7 entries, 73B raw, 7B comp}
16:39:08.542 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1 entries, 6B raw, 1B comp}
16:39:08.543 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.543 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.543 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.543 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.543 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.544 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,446 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.544 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,786B for [gender] BINARY: 21,447 values, 2,755B raw, 2,755B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 3,213B for [area_code] BINARY: 21,446 values, 3,172B raw, 3,172B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,789B for [gender] BINARY: 21,446 values, 2,758B raw, 2,758B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,222B for [area_code] BINARY: 21,447 values, 3,181B raw, 3,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,227B for [area_code] BINARY: 21,446 values, 3,186B raw, 3,186B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.544 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
16:39:08.544 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.545 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.545 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 41 entries, 164B raw, 41B comp}
16:39:08.545 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.545 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.545 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.545 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,784B for [gender] BINARY: 21,446 values, 2,753B raw, 2,753B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 10B raw, 2B comp}
16:39:08.545 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.545 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.546 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,247B for [area_code] BINARY: 21,446 values, 3,206B raw, 3,206B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 2 entries, 20B raw, 2B comp}
16:39:08.546 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 479 entries, 7,244B raw, 479B comp}
16:39:08.546 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.546 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.546 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.547 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 339 entries, 5,248B raw, 339B comp}
16:39:08.547 INFO  [Executor task launch worker for task 8] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.547 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.547 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.547 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.547 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.548 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 71B raw, 10B comp}
16:39:08.548 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 10 entries, 61B raw, 10B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 5 entries, 40B raw, 5B comp}
16:39:08.548 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 1,357 entries, 10,856B raw, 1,357B comp}
16:39:08.827 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000002
16:39:08.827 INFO  [Executor task launch worker for task 8] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000000
16:39:08.827 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000003
16:39:08.827 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000003_0: Committed
16:39:08.827 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000002_0: Committed
16:39:08.827 INFO  [Executor task launch worker for task 8] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000000_0: Committed
16:39:08.831 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 3.0 in stage 1.0 (TID 11). 2658 bytes result sent to driver
16:39:08.831 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 2658 bytes result sent to driver
16:39:08.831 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 10). 2658 bytes result sent to driver
16:39:08.832 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 11) in 1573 ms on localhost (executor driver) (1/4)
16:39:08.832 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 10) in 1573 ms on localhost (executor driver) (2/4)
16:39:08.832 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 1575 ms on localhost (executor driver) (3/4)
16:39:08.839 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925163907_0001_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/_temporary/0/task_20190925163907_0001_m_000001
16:39:08.839 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925163907_0001_m_000001_0: Committed
16:39:08.840 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 9). 2615 bytes result sent to driver
16:39:08.841 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 9) in 1582 ms on localhost (executor driver) (4/4)
16:39:08.841 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:39:08.841 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (insertInto at SparkHelper.scala:35) finished in 1.584 s
16:39:08.846 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: insertInto at SparkHelper.scala:35, took 6.005189 s
16:39:08.905 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
16:39:08.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:08.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:08.934 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:08.935 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:08.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.956 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.957 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:08.958 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:08.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:08.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
16:39:08.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
16:39:09.005 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:09.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:09.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:39:09.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:39:09.114 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
16:39:09.127 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00000-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00000-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.142 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00001-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00001-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.201 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00002-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00002-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.262 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924/part-00003-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00003-29f6987f-1a8f-4c0c-9c7b-9e53e26fc841.c000, Status:true
16:39:09.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
16:39:09.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
16:39:09.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
16:39:09.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
16:39:09.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
16:39:09.343 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
16:39:09.346 WARN  [main] hive.log - Updated size to 5782457
16:39:09.637 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-25_16-39-01_406_512699875425031071-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
16:39:09.643 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
16:39:09.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
16:39:09.647 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
16:39:09.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:09.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:09.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
16:39:09.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
16:39:09.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.703 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.704 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:39:09.705 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:39:09.737 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:39:09.743 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@27e358a7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:39:09.745 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:39:09.755 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:39:09.837 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:39:09.838 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:39:09.842 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:39:09.845 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:39:09.851 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:39:09.851 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:39:09.852 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-cd48a837-f951-4d2e-af8b-b229f111fb1b
16:41:32.183 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
16:41:32.500 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
16:41:32.521 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
16:41:32.521 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
16:41:32.521 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
16:41:32.522 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
16:41:32.522 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
16:41:33.371 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 4013.
16:41:33.393 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
16:41:33.410 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
16:41:33.414 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
16:41:33.414 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
16:41:33.423 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-7fc9a955-241f-49a7-808e-75d51177f9ea
16:41:33.442 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
16:41:33.488 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
16:41:33.575 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @3997ms
16:41:33.648 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
16:41:33.664 INFO  [main] org.spark_project.jetty.server.Server - Started @4087ms
16:41:33.687 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:41:33.688 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
16:41:33.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/jobs,null,AVAILABLE,@Spark}
16:41:33.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bb3d42d{/jobs/json,null,AVAILABLE,@Spark}
16:41:33.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c1dc8e{/jobs/job,null,AVAILABLE,@Spark}
16:41:33.716 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job/json,null,AVAILABLE,@Spark}
16:41:33.717 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/stages,null,AVAILABLE,@Spark}
16:41:33.718 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15051a0{/stages/json,null,AVAILABLE,@Spark}
16:41:33.718 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/stages/stage,null,AVAILABLE,@Spark}
16:41:33.719 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/stage/json,null,AVAILABLE,@Spark}
16:41:33.719 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/pool,null,AVAILABLE,@Spark}
16:41:33.721 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c5204af{/stages/pool/json,null,AVAILABLE,@Spark}
16:41:33.721 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/storage,null,AVAILABLE,@Spark}
16:41:33.722 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/storage/json,null,AVAILABLE,@Spark}
16:41:33.723 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/storage/rdd,null,AVAILABLE,@Spark}
16:41:33.723 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage/rdd/json,null,AVAILABLE,@Spark}
16:41:33.724 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/environment,null,AVAILABLE,@Spark}
16:41:33.725 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/environment/json,null,AVAILABLE,@Spark}
16:41:33.726 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/executors,null,AVAILABLE,@Spark}
16:41:33.726 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/executors/json,null,AVAILABLE,@Spark}
16:41:33.727 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/executors/threadDump,null,AVAILABLE,@Spark}
16:41:33.728 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors/threadDump/json,null,AVAILABLE,@Spark}
16:41:33.733 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/static,null,AVAILABLE,@Spark}
16:41:33.734 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38600b{/,null,AVAILABLE,@Spark}
16:41:33.735 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@721eb7df{/api,null,AVAILABLE,@Spark}
16:41:33.736 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/jobs/job/kill,null,AVAILABLE,@Spark}
16:41:33.737 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76f10035{/stages/stage/kill,null,AVAILABLE,@Spark}
16:41:33.738 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
16:41:33.816 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
16:41:33.851 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4055.
16:41:33.853 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:4055
16:41:33.854 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
16:41:33.890 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:33.892 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:4055 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:33.894 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:33.895 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 4055, None)
16:41:34.074 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ca0863b{/metrics/json,null,AVAILABLE,@Spark}
16:41:35.599 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
16:41:35.622 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
16:41:35.623 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
16:41:35.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@626d2016{/SQL,null,AVAILABLE,@Spark}
16:41:35.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f116ca2{/SQL/json,null,AVAILABLE,@Spark}
16:41:35.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/execution,null,AVAILABLE,@Spark}
16:41:35.631 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL/execution/json,null,AVAILABLE,@Spark}
16:41:35.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/static/sql,null,AVAILABLE,@Spark}
16:41:36.127 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16:41:36.830 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16:41:36.859 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
16:41:38.127 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16:41:39.527 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
16:41:39.530 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
16:41:39.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
16:41:39.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
16:41:39.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
16:41:39.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
16:41:39.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
16:41:39.969 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
16:41:39.969 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16:41:40.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
16:41:40.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
16:41:40.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
16:41:40.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
16:41:40.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
16:41:40.025 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
16:41:40.554 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/a9fe9b95-c855-4f3a-a147-a33332dae926_resources
16:41:40.577 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a9fe9b95-c855-4f3a-a147-a33332dae926
16:41:40.581 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/a9fe9b95-c855-4f3a-a147-a33332dae926
16:41:40.618 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/a9fe9b95-c855-4f3a-a147-a33332dae926/_tmp_space.db
16:41:40.625 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:41:40.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:40.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:40.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
16:41:40.657 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
16:41:40.661 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
16:41:40.815 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/fe2a029c-51ee-4a43-a471-15d1af2c322e_resources
16:41:40.825 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/fe2a029c-51ee-4a43-a471-15d1af2c322e
16:41:40.830 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/fe2a029c-51ee-4a43-a471-15d1af2c322e
16:41:40.837 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/fe2a029c-51ee-4a43-a471-15d1af2c322e/_tmp_space.db
16:41:40.840 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
16:41:40.868 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
16:41:40.875 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
16:41:41.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:41:41.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:41:41.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:41.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:41.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:41.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:41.215 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.229 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.229 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.230 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.231 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.231 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:41.231 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:41:41.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
16:41:41.540 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
16:41:41.552 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
16:41:41.553 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
16:41:41.553 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
16:41:41.554 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
16:41:41.554 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
16:41:41.555 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
16:41:41.570 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
16:41:41.592 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
16:41:41.593 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
16:41:41.594 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
16:41:41.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
16:41:41.595 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
16:41:41.596 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
16:41:41.596 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
16:41:41.597 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
16:41:41.597 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
16:41:41.597 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
16:41:41.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.601 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.614 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.615 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.659 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.660 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.665 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.666 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.670 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:41.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
16:41:41.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
16:41:42.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
16:41:42.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
16:41:42.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:42.034 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:42.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
16:41:42.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
16:41:42.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
16:41:42.091 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
16:41:42.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
16:41:42.110 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
16:41:42.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
16:41:42.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
16:41:42.321 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
16:41:42.323 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
16:41:42.326 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
16:41:42.337 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
16:41:42.341 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
16:41:42.761 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 233.066061 ms
16:41:43.083 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 40.619488 ms
16:41:43.170 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
16:41:43.276 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
16:41:43.280 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:4055 (size: 26.3 KB, free: 1988.7 MB)
16:41:43.284 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DwReleaseCustomer.scala:59
16:41:43.294 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
16:41:43.408 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DwReleaseCustomer.scala:59
16:41:43.425 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DwReleaseCustomer.scala:59)
16:41:43.427 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DwReleaseCustomer.scala:59) with 1 output partitions
16:41:43.428 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DwReleaseCustomer.scala:59)
16:41:43.428 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
16:41:43.430 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
16:41:43.434 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseCustomer.scala:59), which has no missing parents
16:41:43.549 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 24.0 KB, free 1988.3 MB)
16:41:43.552 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1988.3 MB)
16:41:43.553 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:4055 (size: 10.4 KB, free: 1988.7 MB)
16:41:43.554 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
16:41:43.565 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DwReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
16:41:43.566 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 8 tasks
16:41:43.603 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5391 bytes)
16:41:43.605 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5391 bytes)
16:41:43.605 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5391 bytes)
16:41:43.606 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5391 bytes)
16:41:43.606 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 5391 bytes)
16:41:43.606 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 5391 bytes)
16:41:43.607 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 5391 bytes)
16:41:43.607 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 5391 bytes)
16:41:43.615 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
16:41:43.615 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
16:41:43.615 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
16:41:43.616 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
16:41:43.615 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
16:41:43.615 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
16:41:43.615 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
16:41:43.615 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
16:41:43.756 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 37.530104 ms
16:41:43.777 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
16:41:43.777 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
16:41:44.902 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 4] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.902 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.928 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
16:41:44.969 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
16:41:45.256 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1653 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1610 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1653 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1653 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1610 bytes result sent to driver
16:41:45.256 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1653 bytes result sent to driver
16:41:45.259 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1610 bytes result sent to driver
16:41:45.266 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1658 ms on localhost (executor driver) (1/8)
16:41:45.268 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1662 ms on localhost (executor driver) (2/8)
16:41:45.268 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 1663 ms on localhost (executor driver) (3/8)
16:41:45.269 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 1662 ms on localhost (executor driver) (4/8)
16:41:45.269 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 1662 ms on localhost (executor driver) (5/8)
16:41:45.269 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1664 ms on localhost (executor driver) (6/8)
16:41:45.269 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1675 ms on localhost (executor driver) (7/8)
16:41:48.110 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1782 bytes result sent to driver
16:41:48.111 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 4506 ms on localhost (executor driver) (8/8)
16:41:48.111 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
16:41:48.112 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DwReleaseCustomer.scala:59) finished in 4.527 s
16:41:48.112 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:41:48.113 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
16:41:48.113 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
16:41:48.114 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:41:48.116 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseCustomer.scala:59), which has no missing parents
16:41:48.122 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
16:41:48.123 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
16:41:48.125 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:4055 (size: 2.2 KB, free: 1988.7 MB)
16:41:48.125 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
16:41:48.126 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DwReleaseCustomer.scala:59) (first 15 tasks are for partitions Vector(0))
16:41:48.126 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
16:41:48.128 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, ANY, 4726 bytes)
16:41:48.129 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 8)
16:41:48.140 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
16:41:48.141 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
16:41:48.168 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 8). 11678 bytes result sent to driver
16:41:48.168 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 8) in 41 ms on localhost (executor driver) (1/1)
16:41:48.168 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:41:48.169 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DwReleaseCustomer.scala:59) finished in 0.042 s
16:41:48.173 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DwReleaseCustomer.scala:59, took 4.763907 s
16:41:48.268 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
16:41:48.275 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@4837595f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
16:41:48.278 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
16:41:48.288 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
16:41:48.381 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
16:41:48.381 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
16:41:48.382 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
16:41:48.384 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
16:41:48.390 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
16:41:48.391 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
16:41:48.391 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-afe93cb9-e97f-4492-9138-a6f3a0d639ec
