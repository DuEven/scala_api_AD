11:45:31.848 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:45:33.686 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:45:33.867 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
11:45:33.870 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
11:45:33.881 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:45:33.883 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:45:33.894 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
11:45:35.959 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6807.
11:45:36.181 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:45:36.367 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:45:36.386 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:45:36.387 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:45:36.400 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-1b801c09-e00c-48cf-96d6-5031ae6fd6f6
11:45:36.641 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
11:45:36.932 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:45:38.929 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @12542ms
11:45:40.226 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:45:40.355 INFO  [main] org.spark_project.jetty.server.Server - Started @13972ms
11:45:40.665 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@1df14fc1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:45:40.666 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
11:45:40.945 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50cf5a23{/jobs,null,AVAILABLE,@Spark}
11:45:40.946 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf61e67{/jobs/json,null,AVAILABLE,@Spark}
11:45:40.946 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/jobs/job,null,AVAILABLE,@Spark}
11:45:40.947 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/job/json,null,AVAILABLE,@Spark}
11:45:40.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/stages,null,AVAILABLE,@Spark}
11:45:40.948 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages/json,null,AVAILABLE,@Spark}
11:45:40.949 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/stage,null,AVAILABLE,@Spark}
11:45:40.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/stage/json,null,AVAILABLE,@Spark}
11:45:40.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/pool,null,AVAILABLE,@Spark}
11:45:40.950 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/pool/json,null,AVAILABLE,@Spark}
11:45:40.951 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/storage,null,AVAILABLE,@Spark}
11:45:40.951 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage/json,null,AVAILABLE,@Spark}
11:45:40.952 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/rdd,null,AVAILABLE,@Spark}
11:45:40.953 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd/json,null,AVAILABLE,@Spark}
11:45:40.953 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/environment,null,AVAILABLE,@Spark}
11:45:40.954 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/environment/json,null,AVAILABLE,@Spark}
11:45:40.955 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/executors,null,AVAILABLE,@Spark}
11:45:40.955 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors/json,null,AVAILABLE,@Spark}
11:45:40.956 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/threadDump,null,AVAILABLE,@Spark}
11:45:40.956 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:45:41.027 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/static,null,AVAILABLE,@Spark}
11:45:41.028 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669d2b1b{/,null,AVAILABLE,@Spark}
11:45:41.028 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/api,null,AVAILABLE,@Spark}
11:45:41.029 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ab9b447{/jobs/job/kill,null,AVAILABLE,@Spark}
11:45:41.030 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/stages/stage/kill,null,AVAILABLE,@Spark}
11:45:41.071 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4040
11:45:42.938 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:45:43.103 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6849.
11:45:43.126 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:6849
11:45:43.137 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:45:43.522 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 6849, None)
11:45:43.532 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:6849 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 6849, None)
11:45:43.538 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 6849, None)
11:45:43.538 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 6849, None)
11:45:43.887 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@319854f0{/metrics/json,null,AVAILABLE,@Spark}
11:45:48.306 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
11:45:48.347 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
11:45:48.348 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
11:45:48.380 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64b018f3{/SQL,null,AVAILABLE,@Spark}
11:45:48.381 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@581b1c08{/SQL/json,null,AVAILABLE,@Spark}
11:45:48.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64763e49{/SQL/execution,null,AVAILABLE,@Spark}
11:45:48.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@69cd7630{/SQL/execution/json,null,AVAILABLE,@Spark}
11:45:48.386 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64b0d1fa{/static/sql,null,AVAILABLE,@Spark}
11:45:51.634 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:45:53.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:45:53.177 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:45:57.611 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:46:00.284 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:46:00.287 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:46:00.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:46:00.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:46:00.980 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:46:01.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:46:01.181 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
11:46:01.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:46:01.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:46:01.424 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:46:01.424 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:46:01.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:46:01.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:46:01.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:46:01.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:46:06.839 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507
11:46:06.845 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/091d4eb0-7f9c-4b01-8b23-cdfd6d380c50_resources
11:46:06.887 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/091d4eb0-7f9c-4b01-8b23-cdfd6d380c50
11:46:06.890 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/091d4eb0-7f9c-4b01-8b23-cdfd6d380c50
11:46:06.894 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/091d4eb0-7f9c-4b01-8b23-cdfd6d380c50/_tmp_space.db
11:46:06.921 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:46:07.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:07.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:07.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:46:07.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:46:07.069 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:46:08.292 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/8c4be558-760f-4047-a20e-75abc180ddc1_resources
11:46:08.296 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8c4be558-760f-4047-a20e-75abc180ddc1
11:46:08.300 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/8c4be558-760f-4047-a20e-75abc180ddc1
11:46:08.305 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/8c4be558-760f-4047-a20e-75abc180ddc1/_tmp_space.db
11:46:08.307 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:46:09.134 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:46:09.150 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:09.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:09.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:09.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:09.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:10.220 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:10.220 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:10.251 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.347 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.349 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.349 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.350 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.350 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.350 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:10.350 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:10.442 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:46:11.357 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:46:11.440 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:11.441 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:11.442 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:11.443 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:11.443 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:11.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:11.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:11.444 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:11.454 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:12.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:12.099 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:12.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:12.104 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:12.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:12.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:12.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:12.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:12.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:12.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:12.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:12.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:12.830 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:46:12.831 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
11:46:12.833 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:12.916 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:12.939 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:13.880 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:46:14.042 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 398.738488 ms
11:46:14.510 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 69.372894 ms
11:46:15.044 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
11:46:15.815 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
11:46:15.822 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.7 MB)
11:46:15.830 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw05ReleaseRegisterUsers.scala:66
11:46:15.941 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:16.298 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:46:16.420 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:16.423 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:46:16.424 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:16.424 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:46:16.426 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:16.459 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:16.573 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:46:16.577 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:46:16.578 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:6849 (size: 2.2 KB, free: 1988.7 MB)
11:46:16.579 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:46:16.660 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:46:16.661 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:46:16.784 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:16.862 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:46:17.073 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:17.074 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 10 ms
11:46:17.199 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:46:17.215 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 456 ms on localhost (executor driver) (1/1)
11:46:17.218 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:46:17.281 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.503 s
11:46:17.299 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 1.000285 s
11:46:17.344 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:46:17.345 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw05ReleaseRegisterUsers.scala:66) with 3 output partitions
11:46:17.345 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:17.345 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:46:17.345 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:17.345 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:17.348 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:46:17.351 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:46:17.353 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:6849 (size: 2.2 KB, free: 1988.7 MB)
11:46:17.354 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:46:17.355 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:46:17.356 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:46:17.357 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:17.358 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:17.359 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:17.359 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:46:17.360 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:46:17.360 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:46:17.365 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:17.366 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:17.366 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:17.366 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:17.366 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:17.366 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:17.367 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
11:46:17.367 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
11:46:17.367 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
11:46:17.369 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 11 ms on localhost (executor driver) (1/3)
11:46:17.370 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 13 ms on localhost (executor driver) (2/3)
11:46:17.371 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 15 ms on localhost (executor driver) (3/3)
11:46:17.371 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:46:17.371 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.015 s
11:46:17.372 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.027625 s
11:46:17.435 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:46:17.463 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:17.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:17.506 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.507 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.507 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.507 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.507 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.507 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.508 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.508 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:17.508 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:17.733 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-17_731_6133519635708921771-1
11:46:18.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:18.170 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:18.175 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:18.175 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:18.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:18.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:18.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.213 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.213 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.213 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:18.213 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:18.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:18.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:18.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:18.216 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:18.256 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:46:18.257 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
11:46:18.257 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:18.257 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:18.257 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:18.304 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:18.305 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:18.386 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.312311 ms
11:46:18.400 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 312.0 KB, free 1988.1 MB)
11:46:18.429 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
11:46:18.430 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
11:46:18.441 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:46:18.465 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.6 MB)
11:46:18.466 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
11:46:18.466 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:18.497 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:6849 in memory (size: 2.2 KB, free: 1988.6 MB)
11:46:18.519 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
11:46:18.519 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
11:46:18.519 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
11:46:18.519 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
11:46:18.520 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.7 MB)
11:46:18.521 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
11:46:18.522 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
11:46:18.563 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
11:46:18.567 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:6849 in memory (size: 2.2 KB, free: 1988.7 MB)
11:46:18.569 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
11:46:18.688 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:46:18.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:35)
11:46:18.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:46:18.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
11:46:18.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
11:46:18.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:18.689 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:18.793 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 172.9 KB, free 1988.2 MB)
11:46:18.796 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 64.0 KB, free 1988.1 MB)
11:46:18.797 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:6849 (size: 64.0 KB, free: 1988.6 MB)
11:46:18.798 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
11:46:18.798 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:46:18.798 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
11:46:18.799 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:18.799 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:18.799 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:18.799 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:18.800 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
11:46:18.800 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
11:46:18.800 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
11:46:18.800 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
11:46:18.846 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:18.846 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:18.847 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:18.848 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:18.849 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:18.849 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:18.849 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:18.849 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:18.892 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.684949 ms
11:46:19.011 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 13.857196 ms
11:46:19.083 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:19.083 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:19.083 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:19.083 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:19.085 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:19.085 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:19.085 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:19.085 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:19.106 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.009687 ms
11:46:19.183 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.721598 ms
11:46:19.198 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.301145 ms
11:46:19.355 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114619_0005_m_000001_0
11:46:19.355 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114619_0005_m_000002_0
11:46:19.355 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114619_0005_m_000003_0
11:46:19.355 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114619_0005_m_000000_0
11:46:19.358 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2631 bytes result sent to driver
11:46:19.359 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2631 bytes result sent to driver
11:46:19.359 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2631 bytes result sent to driver
11:46:19.359 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2631 bytes result sent to driver
11:46:19.361 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 562 ms on localhost (executor driver) (1/4)
11:46:19.361 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 562 ms on localhost (executor driver) (2/4)
11:46:19.361 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 562 ms on localhost (executor driver) (3/4)
11:46:19.363 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 564 ms on localhost (executor driver) (4/4)
11:46:19.363 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
11:46:19.364 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.566 s
11:46:19.364 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.676267 s
11:46:19.777 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:46:19.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:19.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:19.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:19.810 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:19.834 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:19.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:19.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:19.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:19.887 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:46:19.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:19.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:19.970 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:46:19.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:46:19.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:46:19.980 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:19.981 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.006 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.007 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.033 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.034 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.035 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.078 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:20.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:20.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:20.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.085 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.130 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.131 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.131 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.131 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.131 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.131 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.131 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.131 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.132 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.132 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.152 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:46:20.153 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:20.153 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:20.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:20.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:20.154 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:20.155 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:20.155 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:20.155 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:20.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:20.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:20.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:20.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:20.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.250 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:20.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:20.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:20.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:20.281 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#123),(bdp_day#123 = 20190921)
11:46:20.282 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#116),(release_status#116 = 06)
11:46:20.282 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:20.282 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:20.283 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:20.302 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 312.0 KB, free 1987.8 MB)
11:46:20.318 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.8 MB)
11:46:20.320 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.6 MB)
11:46:20.321 INFO  [main] org.apache.spark.SparkContext - Created broadcast 5 from show at Dw05ReleaseRegisterUsers.scala:66
11:46:20.321 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:20.332 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:46:20.332 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 20 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:20.333 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:46:20.333 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:20.333 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
11:46:20.333 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:20.333 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:20.335 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1987.8 MB)
11:46:20.338 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.8 MB)
11:46:20.339 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:6849 (size: 2.2 KB, free: 1988.6 MB)
11:46:20.340 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
11:46:20.341 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:46:20.341 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
11:46:20.342 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:20.342 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 8)
11:46:20.345 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.345 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:20.347 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 8). 1182 bytes result sent to driver
11:46:20.347 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 8) in 5 ms on localhost (executor driver) (1/1)
11:46:20.347 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
11:46:20.348 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.006 s
11:46:20.348 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.016508 s
11:46:20.351 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:46:20.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at Dw05ReleaseRegisterUsers.scala:66) with 3 output partitions
11:46:20.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:20.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
11:46:20.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:20.352 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:20.354 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1987.8 MB)
11:46:20.356 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.8 MB)
11:46:20.357 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:6849 (size: 2.2 KB, free: 1988.6 MB)
11:46:20.358 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
11:46:20.358 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:46:20.358 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 3 tasks
11:46:20.359 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:20.359 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:20.359 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:20.359 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 9)
11:46:20.359 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 11)
11:46:20.359 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 10)
11:46:20.361 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.361 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.361 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.361 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:20.361 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:20.361 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:20.362 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 9). 1182 bytes result sent to driver
11:46:20.362 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 11). 1182 bytes result sent to driver
11:46:20.362 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 10). 1182 bytes result sent to driver
11:46:20.363 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 9) in 4 ms on localhost (executor driver) (1/3)
11:46:20.363 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 11) in 4 ms on localhost (executor driver) (2/3)
11:46:20.363 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 10) in 4 ms on localhost (executor driver) (3/3)
11:46:20.363 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
11:46:20.364 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.005 s
11:46:20.364 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.012922 s
11:46:20.369 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:46:20.370 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.370 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.390 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.390 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.390 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.394 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-20_394_2070201380513582992-1
11:46:20.442 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:20.442 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:20.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.447 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.490 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.491 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.492 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:20.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:20.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:20.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:20.517 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#123),(bdp_day#123 = 20190921)
11:46:20.517 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#116),(release_status#116 = 06)
11:46:20.518 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:20.518 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:20.518 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:20.525 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:20.525 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:20.537 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 312.0 KB, free 1987.5 MB)
11:46:20.552 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.5 MB)
11:46:20.553 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.6 MB)
11:46:20.554 INFO  [main] org.apache.spark.SparkContext - Created broadcast 8 from insertInto at SparkHelper.scala:35
11:46:20.554 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:20.597 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:46:20.598 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 27 (insertInto at SparkHelper.scala:35)
11:46:20.598 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:46:20.598 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (insertInto at SparkHelper.scala:35)
11:46:20.598 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
11:46:20.598 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:20.599 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:20.631 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 172.9 KB, free 1987.3 MB)
11:46:20.635 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 64.0 KB, free 1987.2 MB)
11:46:20.636 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:6849 (size: 64.0 KB, free: 1988.5 MB)
11:46:20.637 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
11:46:20.637 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 11 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:46:20.638 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 4 tasks
11:46:20.638 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:20.639 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:20.639 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:20.640 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:20.640 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 14)
11:46:20.640 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 15)
11:46:20.640 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 13)
11:46:20.640 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 12)
11:46:20.666 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.666 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:20.669 INFO  [Executor task launch worker for task 14] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:20.669 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:20.675 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.675 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.675 INFO  [Executor task launch worker for task 14] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114620_0011_m_000002_0
11:46:20.676 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:20.676 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:20.678 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 14). 2545 bytes result sent to driver
11:46:20.679 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:20.679 INFO  [Executor task launch worker for task 15] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:20.679 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:20.680 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:20.681 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:20.681 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:20.681 INFO  [Executor task launch worker for task 13] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:20.682 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:20.682 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 14) in 43 ms on localhost (executor driver) (1/4)
11:46:20.683 INFO  [Executor task launch worker for task 15] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114620_0011_m_000003_0
11:46:20.685 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 15). 2502 bytes result sent to driver
11:46:20.685 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114620_0011_m_000000_0
11:46:20.686 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 15) in 47 ms on localhost (executor driver) (2/4)
11:46:20.686 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 12). 2502 bytes result sent to driver
11:46:20.687 INFO  [Executor task launch worker for task 13] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114620_0011_m_000001_0
11:46:20.687 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 12) in 49 ms on localhost (executor driver) (3/4)
11:46:20.688 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 13). 2545 bytes result sent to driver
11:46:20.689 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 13) in 51 ms on localhost (executor driver) (4/4)
11:46:20.689 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
11:46:20.690 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (insertInto at SparkHelper.scala:35) finished in 0.052 s
11:46:20.690 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: insertInto at SparkHelper.scala:35, took 0.092265 s
11:46:20.701 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:46:20.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.726 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.746 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.747 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.747 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.747 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.747 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.747 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.772 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:46:20.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.794 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:46:20.794 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:46:20.794 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:46:20.799 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.799 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:20.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:20.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.854 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.855 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.855 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.855 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.869 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:20.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:20.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:20.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.876 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.922 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.924 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.925 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.925 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:20.925 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:20.931 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:46:20.931 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:20.932 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:20.932 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:20.932 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:20.932 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:20.932 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:20.932 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:20.932 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:20.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:20.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:20.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:20.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:20.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:20.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:20.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:21.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.017 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.017 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.017 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:21.017 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:21.018 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:21.018 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:21.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:21.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:21.228 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#227),(bdp_day#227 = 20190922)
11:46:21.228 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#220),(release_status#220 = 06)
11:46:21.228 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:21.228 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:21.228 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:46:21.247 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 312.0 KB, free 1986.9 MB)
11:46:21.267 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1986.9 MB)
11:46:21.270 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.5 MB)
11:46:21.270 INFO  [main] org.apache.spark.SparkContext - Created broadcast 10 from show at Dw05ReleaseRegisterUsers.scala:66
11:46:21.271 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:46:21.339 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:46:21.339 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 36 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:21.339 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:46:21.339 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:21.340 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
11:46:21.340 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
11:46:21.341 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[36] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:21.396 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 21.7 KB, free 1986.9 MB)
11:46:21.398 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1986.9 MB)
11:46:21.399 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:6849 (size: 9.6 KB, free: 1988.5 MB)
11:46:21.400 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
11:46:21.402 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[36] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:46:21.402 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 8 tasks
11:46:21.408 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:46:21.409 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 17, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:46:21.409 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 18, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:46:21.409 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 19, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:46:21.409 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 20, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:46:21.410 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 21, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:46:21.410 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 22, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:46:21.410 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 23, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:46:21.410 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)
11:46:21.410 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 18)
11:46:21.410 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 17)
11:46:21.410 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 19)
11:46:21.412 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 21)
11:46:21.412 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 20)
11:46:21.412 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 22)
11:46:21.412 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 23)
11:46:21.468 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.854924 ms
11:46:21.475 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:46:21.475 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:46:21.475 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:46:21.475 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:46:21.475 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:46:21.475 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:46:21.475 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:46:21.475 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:46:21.982 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 180
11:46:21.983 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.5 MB)
11:46:21.984 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 122
11:46:21.984 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 121
11:46:21.985 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:6849 in memory (size: 64.0 KB, free: 1988.5 MB)
11:46:21.985 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 123
11:46:21.986 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 118
11:46:21.987 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:6849 in memory (size: 2.2 KB, free: 1988.5 MB)
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 181
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 187
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 177
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 186
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 183
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 126
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 125
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 185
11:46:21.987 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 175
11:46:21.988 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 182
11:46:21.988 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 184
11:46:21.988 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 179
11:46:21.989 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:6849 in memory (size: 2.2 KB, free: 1988.6 MB)
11:46:21.989 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 120
11:46:21.989 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 178
11:46:21.990 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.6 MB)
11:46:21.991 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 119
11:46:21.992 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
11:46:21.992 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 212
11:46:21.992 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 176
11:46:21.992 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 124
11:46:23.466 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.466 INFO  [Executor task launch worker for task 17] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.466 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.466 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.466 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.466 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.466 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.515 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:23.661 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 22). 1610 bytes result sent to driver
11:46:23.661 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 1610 bytes result sent to driver
11:46:23.661 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 17). 1610 bytes result sent to driver
11:46:23.661 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 21). 1610 bytes result sent to driver
11:46:23.661 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 18). 1610 bytes result sent to driver
11:46:23.663 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 20). 1610 bytes result sent to driver
11:46:23.663 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 23). 1567 bytes result sent to driver
11:46:23.665 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 21) in 2256 ms on localhost (executor driver) (1/8)
11:46:23.665 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 2259 ms on localhost (executor driver) (2/8)
11:46:23.665 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 17) in 2257 ms on localhost (executor driver) (3/8)
11:46:23.665 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 18) in 2256 ms on localhost (executor driver) (4/8)
11:46:23.665 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 22) in 2255 ms on localhost (executor driver) (5/8)
11:46:23.665 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 20) in 2256 ms on localhost (executor driver) (6/8)
11:46:23.666 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 23) in 2256 ms on localhost (executor driver) (7/8)
11:46:31.234 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 84
11:46:31.234 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 89
11:46:31.235 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 1
11:46:31.235 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 82
11:46:31.239 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:6849 in memory (size: 64.0 KB, free: 1988.6 MB)
11:46:31.241 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 90
11:46:31.241 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 91
11:46:31.244 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.7 MB)
11:46:31.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 86
11:46:31.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 93
11:46:31.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 88
11:46:31.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 92
11:46:31.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 83
11:46:31.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 85
11:46:31.246 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 87
11:46:31.435 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 19). 1739 bytes result sent to driver
11:46:31.436 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 19) in 10027 ms on localhost (executor driver) (8/8)
11:46:31.436 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
11:46:31.437 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 10.031 s
11:46:31.437 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:46:31.438 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:46:31.466 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
11:46:31.467 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:46:31.471 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[38] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:31.475 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
11:46:31.477 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1988.3 MB)
11:46:31.479 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:6849 (size: 2.3 KB, free: 1988.7 MB)
11:46:31.480 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
11:46:31.481 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[38] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:46:31.481 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks
11:46:31.481 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 24, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:46:31.481 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 24)
11:46:31.506 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:31.507 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 21 ms
11:46:31.526 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 24). 5906 bytes result sent to driver
11:46:31.526 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 24) in 45 ms on localhost (executor driver) (1/1)
11:46:31.526 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
11:46:31.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.046 s
11:46:31.527 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 10.188328 s
11:46:31.543 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:46:31.544 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:31.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:31.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.570 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.570 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.570 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:31.575 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1
11:46:31.627 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:31.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:31.633 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:31.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:31.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:31.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:31.682 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.684 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.684 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:31.684 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:31.686 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:31.686 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:31.686 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:31.686 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:31.723 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#227),(bdp_day#227 = 20190922)
11:46:31.723 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#220),(release_status#220 = 06)
11:46:31.723 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:31.724 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:31.724 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:46:31.730 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:31.731 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:31.745 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
11:46:31.763 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:46:31.765 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.6 MB)
11:46:31.766 INFO  [main] org.apache.spark.SparkContext - Created broadcast 13 from insertInto at SparkHelper.scala:35
11:46:31.767 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:46:31.824 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:46:31.825 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 43 (insertInto at SparkHelper.scala:35)
11:46:31.826 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 7 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:46:31.826 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (insertInto at SparkHelper.scala:35)
11:46:31.826 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
11:46:31.826 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
11:46:31.826 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:31.831 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 21.7 KB, free 1988.0 MB)
11:46:31.834 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.0 MB)
11:46:31.834 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:6849 (size: 9.6 KB, free: 1988.6 MB)
11:46:31.835 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1004
11:46:31.836 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:46:31.836 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 8 tasks
11:46:31.840 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 25, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:46:31.841 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 26, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:46:31.841 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 27, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:46:31.841 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 28, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:46:31.842 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 29, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:46:31.844 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 30, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:46:31.844 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 31, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:46:31.845 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 32, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:46:31.845 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 25)
11:46:31.848 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 26)
11:46:31.850 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 27)
11:46:31.855 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:46:31.855 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 28)
11:46:31.857 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 29)
11:46:31.859 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:46:31.861 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 30)
11:46:31.861 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:46:31.861 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 31)
11:46:31.861 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:46:31.862 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:46:31.884 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:46:31.896 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 32)
11:46:31.901 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:46:31.902 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:46:31.925 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.948 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.953 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 31). 1567 bytes result sent to driver
11:46:31.955 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.955 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 31) in 111 ms on localhost (executor driver) (1/8)
11:46:31.964 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 25). 1524 bytes result sent to driver
11:46:31.966 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 25) in 130 ms on localhost (executor driver) (2/8)
11:46:31.966 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.968 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 29). 1524 bytes result sent to driver
11:46:31.969 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 29) in 127 ms on localhost (executor driver) (3/8)
11:46:31.970 INFO  [Executor task launch worker for task 26] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.975 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.994 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 26). 1524 bytes result sent to driver
11:46:31.997 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 26) in 156 ms on localhost (executor driver) (4/8)
11:46:31.998 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.999 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:31.999 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 30). 1524 bytes result sent to driver
11:46:32.000 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 30) in 158 ms on localhost (executor driver) (5/8)
11:46:32.033 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 27). 1524 bytes result sent to driver
11:46:32.035 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 27) in 194 ms on localhost (executor driver) (6/8)
11:46:32.039 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 32). 1524 bytes result sent to driver
11:46:32.039 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 32) in 195 ms on localhost (executor driver) (7/8)
11:46:32.099 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:46:32.497 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:46:32.524 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
11:46:32.525 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
11:46:32.525 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:46:32.526 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:46:32.526 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
11:46:33.371 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:6849 in memory (size: 2.3 KB, free: 1988.6 MB)
11:46:33.372 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 269
11:46:33.524 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 28). 1782 bytes result sent to driver
11:46:33.525 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 28) in 1684 ms on localhost (executor driver) (8/8)
11:46:33.525 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
11:46:33.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (insertInto at SparkHelper.scala:35) finished in 1.690 s
11:46:33.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:46:33.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:46:33.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
11:46:33.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:46:33.526 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:33.564 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
11:46:33.565 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6937.
11:46:33.566 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
11:46:33.567 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:6849 (size: 64.1 KB, free: 1988.6 MB)
11:46:33.567 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1004
11:46:33.568 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 15 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:46:33.568 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 4 tasks
11:46:33.569 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 33, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:46:33.570 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 34, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:46:33.570 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 15.0 (TID 35, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:46:33.570 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 15.0 (TID 36, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:46:33.570 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 33)
11:46:33.570 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 34)
11:46:33.570 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 3.0 in stage 15.0 (TID 36)
11:46:33.580 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 2.0 in stage 15.0 (TID 35)
11:46:33.595 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:33.595 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:33.609 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:33.609 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:33.613 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:33.613 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:33.615 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:33.615 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:33.634 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:46:33.658 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:46:33.661 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:46:33.662 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:46:33.673 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:33.673 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:33.673 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:33.673 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:33.673 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:33.673 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:33.674 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:33.674 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:33.684 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-556036e6-6bfc-436a-9ff8-123b1baa26a6
11:46:33.705 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
11:46:33.762 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:46:33.858 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @4950ms
11:46:33.927 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:46:33.941 INFO  [main] org.spark_project.jetty.server.Server - Started @5034ms
11:46:33.956 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
11:46:33.963 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6d2260db{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
11:46:33.964 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
11:46:33.985 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@273c947f{/jobs,null,AVAILABLE,@Spark}
11:46:33.986 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b273a59{/jobs/json,null,AVAILABLE,@Spark}
11:46:33.987 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/job,null,AVAILABLE,@Spark}
11:46:33.988 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/jobs/job/json,null,AVAILABLE,@Spark}
11:46:33.989 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/stages,null,AVAILABLE,@Spark}
11:46:33.989 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages/json,null,AVAILABLE,@Spark}
11:46:33.990 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e8f862{/stages/stage,null,AVAILABLE,@Spark}
11:46:33.992 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74518890{/stages/stage/json,null,AVAILABLE,@Spark}
11:46:33.993 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/pool,null,AVAILABLE,@Spark}
11:46:33.994 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/stages/pool/json,null,AVAILABLE,@Spark}
11:46:33.995 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/storage,null,AVAILABLE,@Spark}
11:46:33.996 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage/json,null,AVAILABLE,@Spark}
11:46:33.997 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/rdd,null,AVAILABLE,@Spark}
11:46:33.998 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/storage/rdd/json,null,AVAILABLE,@Spark}
11:46:33.999 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/environment,null,AVAILABLE,@Spark}
11:46:34.000 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/environment/json,null,AVAILABLE,@Spark}
11:46:34.000 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/executors,null,AVAILABLE,@Spark}
11:46:34.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors/json,null,AVAILABLE,@Spark}
11:46:34.002 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/threadDump,null,AVAILABLE,@Spark}
11:46:34.003 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:46:34.010 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/static,null,AVAILABLE,@Spark}
11:46:34.011 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ea9f009{/,null,AVAILABLE,@Spark}
11:46:34.012 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/api,null,AVAILABLE,@Spark}
11:46:34.013 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4f8caaf3{/jobs/job/kill,null,AVAILABLE,@Spark}
11:46:34.014 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/stages/stage/kill,null,AVAILABLE,@Spark}
11:46:34.016 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4041
11:46:34.050 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35602f4e
11:46:34.051 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23e6b5fb
11:46:34.051 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2fcc8708
11:46:34.051 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@77573860
11:46:34.096 INFO  [Executor task launch worker for task 33] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
11:46:34.103 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:46:34.135 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6979.
11:46:34.135 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:6979
11:46:34.137 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:46:34.174 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 6979, None)
11:46:34.176 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:6979 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 6979, None)
11:46:34.179 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 6979, None)
11:46:34.179 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 6979, None)
11:46:34.221 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:34.221 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:34.221 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:34.221 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:34.222 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114633_0015_m_000003_0/bdp_day=20190922/part-00003-03068620-8444-4ea9-853a-1de6907c559a.c000
11:46:34.222 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114633_0015_m_000001_0/bdp_day=20190922/part-00001-03068620-8444-4ea9-853a-1de6907c559a.c000
11:46:34.222 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114633_0015_m_000000_0/bdp_day=20190922/part-00000-03068620-8444-4ea9-853a-1de6907c559a.c000
11:46:34.222 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114633_0015_m_000002_0/bdp_day=20190922/part-00002-03068620-8444-4ea9-853a-1de6907c559a.c000
11:46:34.243 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:34.243 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:34.243 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:34.243 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:34.244 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:34.244 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:34.244 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:34.244 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:34.245 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:34.245 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:34.245 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:34.245 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:34.245 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:34.245 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:34.245 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:34.246 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:34.246 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:34.246 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:34.246 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:34.246 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:34.246 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:34.246 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:34.246 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:34.246 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:34.246 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:34.361 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@415156bf{/metrics/json,null,AVAILABLE,@Spark}
11:46:34.658 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6f0f6c3b
11:46:34.658 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3084dd77
11:46:34.659 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@7260f7d5
11:46:34.659 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@54cdeeac
11:46:34.796 INFO  [Executor task launch worker for task 36] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,634
11:46:34.803 INFO  [Executor task launch worker for task 34] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,854
11:46:34.822 INFO  [Executor task launch worker for task 35] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,810
11:46:34.823 INFO  [Executor task launch worker for task 33] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 323,058
11:46:34.921 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.921 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.921 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.921 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.922 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.922 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.925 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.926 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.926 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.926 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.926 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.927 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:34.927 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.927 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:34.927 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:34.927 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.928 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.928 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.929 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.929 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:34.929 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:34.929 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.930 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.932 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:34.932 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.932 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.933 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.934 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:34.935 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:34.936 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:34.937 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:34.937 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:35.057 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 192.168.32.1:6849 in memory (size: 9.6 KB, free: 1988.6 MB)
11:46:35.652 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114633_0015_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/task_20190926114633_0015_m_000002
11:46:35.652 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114633_0015_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/task_20190926114633_0015_m_000003
11:46:35.653 INFO  [Executor task launch worker for task 36] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114633_0015_m_000003_0: Committed
11:46:35.653 INFO  [Executor task launch worker for task 35] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114633_0015_m_000002_0: Committed
11:46:35.656 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 2.0 in stage 15.0 (TID 35). 2658 bytes result sent to driver
11:46:35.657 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 3.0 in stage 15.0 (TID 36). 2658 bytes result sent to driver
11:46:35.662 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 15.0 (TID 36) in 2092 ms on localhost (executor driver) (1/4)
11:46:35.664 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 15.0 (TID 35) in 2094 ms on localhost (executor driver) (2/4)
11:46:35.966 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114633_0015_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/task_20190926114633_0015_m_000000
11:46:35.966 INFO  [Executor task launch worker for task 33] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114633_0015_m_000000_0: Committed
11:46:35.966 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
11:46:35.967 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 33). 2658 bytes result sent to driver
11:46:35.969 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 33) in 2400 ms on localhost (executor driver) (3/4)
11:46:35.969 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114633_0015_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/_temporary/0/task_20190926114633_0015_m_000001
11:46:35.969 INFO  [Executor task launch worker for task 34] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114633_0015_m_000001_0: Committed
11:46:35.970 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 34). 2615 bytes result sent to driver
11:46:35.972 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 34) in 2403 ms on localhost (executor driver) (4/4)
11:46:35.972 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
11:46:35.972 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (insertInto at SparkHelper.scala:35) finished in 2.403 s
11:46:35.973 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 7 finished: insertInto at SparkHelper.scala:35, took 4.148306 s
11:46:35.996 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
11:46:35.997 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
11:46:36.002 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@125d47c4{/SQL,null,AVAILABLE,@Spark}
11:46:36.003 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64b018f3{/SQL/json,null,AVAILABLE,@Spark}
11:46:36.004 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@58f2466c{/SQL/execution,null,AVAILABLE,@Spark}
11:46:36.004 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64763e49{/SQL/execution/json,null,AVAILABLE,@Spark}
11:46:36.006 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e546734{/static/sql,null,AVAILABLE,@Spark}
11:46:36.060 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:46:36.061 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:36.062 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:36.080 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:36.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.102 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:36.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:36.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:36.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:36.122 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:36.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:36.139 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]
11:46:36.139 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]	
11:46:36.205 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
11:46:36.251 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/bdp_day=20190922/part-00000-03068620-8444-4ea9-853a-1de6907c559a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00000-03068620-8444-4ea9-853a-1de6907c559a.c000, Status:true
11:46:36.369 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/bdp_day=20190922/part-00001-03068620-8444-4ea9-853a-1de6907c559a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00001-03068620-8444-4ea9-853a-1de6907c559a.c000, Status:true
11:46:36.379 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/bdp_day=20190922/part-00002-03068620-8444-4ea9-853a-1de6907c559a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00002-03068620-8444-4ea9-853a-1de6907c559a.c000, Status:true
11:46:36.389 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/bdp_day=20190922/part-00003-03068620-8444-4ea9-853a-1de6907c559a.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00003-03068620-8444-4ea9-853a-1de6907c559a.c000, Status:true
11:46:36.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]
11:46:36.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]	
11:46:36.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_register_users[20190922]
11:46:36.400 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_register_users[20190922]	
11:46:36.460 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
11:46:36.462 WARN  [main] hive.log - Updated size to 872108
11:46:36.545 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:46:36.716 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-31_575_7562146406819666471-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
11:46:36.723 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:46:36.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:46:36.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:46:36.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:36.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:36.746 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:36.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:36.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:36.785 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:36.785 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:36.785 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:36.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:36.791 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:36.808 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:36.808 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.824 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:36.830 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:46:36.831 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:36.831 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:36.831 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:36.831 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:36.832 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:36.832 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:36.832 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:36.832 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:36.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:36.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:36.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:36.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:36.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:36.868 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:36.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:36.885 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:36.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:36.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:36.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:36.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:36.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:36.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:36.932 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#331),(bdp_day#331 = 20190923)
11:46:36.933 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#324),(release_status#324 = 06)
11:46:36.933 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:36.933 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:36.933 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:46:36.946 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 312.0 KB, free 1987.5 MB)
11:46:36.972 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
11:46:36.972 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.6 MB)
11:46:36.972 INFO  [main] org.apache.spark.SparkContext - Created broadcast 16 from show at Dw05ReleaseRegisterUsers.scala:66
11:46:36.973 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:46:36.984 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:46:36.984 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 52 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:36.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 8 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:46:36.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:36.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
11:46:36.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
11:46:36.985 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[52] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:36.991 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 21.7 KB, free 1987.4 MB)
11:46:36.993 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.4 MB)
11:46:36.994 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.32.1:6849 (size: 9.6 KB, free: 1988.5 MB)
11:46:36.994 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1004
11:46:36.994 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[52] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:46:36.994 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 8 tasks
11:46:36.995 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 37, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:46:36.995 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 38, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:46:36.995 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 39, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:46:36.996 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 40, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:46:36.996 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 41, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:46:36.996 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 42, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:46:36.996 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 43, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:46:36.997 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 16.0 (TID 44, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:46:36.997 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 38)
11:46:36.997 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 40)
11:46:36.997 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 37)
11:46:36.997 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 39)
11:46:36.997 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 42)
11:46:36.997 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 43)
11:46:36.997 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 7.0 in stage 16.0 (TID 44)
11:46:36.997 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 41)
11:46:37.001 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:46:37.002 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:46:37.002 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:46:37.003 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:46:37.003 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:46:37.004 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:46:37.005 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:46:37.008 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:46:37.187 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.187 INFO  [Executor task launch worker for task 39] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.193 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.199 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 39). 1481 bytes result sent to driver
11:46:37.200 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 39) in 205 ms on localhost (executor driver) (1/8)
11:46:37.204 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 38). 1524 bytes result sent to driver
11:46:37.205 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 38) in 210 ms on localhost (executor driver) (2/8)
11:46:37.213 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 41). 1567 bytes result sent to driver
11:46:37.214 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 41) in 218 ms on localhost (executor driver) (3/8)
11:46:37.245 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.245 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.246 INFO  [Executor task launch worker for task 37] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.261 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 7.0 in stage 16.0 (TID 44). 1567 bytes result sent to driver
11:46:37.265 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 16.0 (TID 44) in 269 ms on localhost (executor driver) (4/8)
11:46:37.267 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 37). 1524 bytes result sent to driver
11:46:37.267 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 37) in 272 ms on localhost (executor driver) (5/8)
11:46:37.294 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.295 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:37.316 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 43). 1524 bytes result sent to driver
11:46:37.320 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 43) in 324 ms on localhost (executor driver) (6/8)
11:46:37.322 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 42). 1567 bytes result sent to driver
11:46:37.322 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 42) in 326 ms on localhost (executor driver) (7/8)
11:46:37.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:46:37.520 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:46:39.850 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:46:42.496 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:46:42.498 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:46:43.104 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:46:43.134 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:46:43.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:46:43.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:46:43.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
11:46:43.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:46:43.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:46:43.782 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:46:43.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:46:43.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:46:43.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:46:43.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:46:43.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:46:44.379 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/b229565c-6e88-4b8d-97b3-a3d6fe7b7307_resources
11:46:44.397 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b229565c-6e88-4b8d-97b3-a3d6fe7b7307
11:46:44.421 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/b229565c-6e88-4b8d-97b3-a3d6fe7b7307
11:46:44.441 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/b229565c-6e88-4b8d-97b3-a3d6fe7b7307/_tmp_space.db
11:46:44.447 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:46:44.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:44.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:44.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:46:44.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:46:44.544 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:46:44.816 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/66e71a5a-2b4c-4e28-b23f-088d193a9817_resources
11:46:44.870 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/66e71a5a-2b4c-4e28-b23f-088d193a9817
11:46:44.906 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/66e71a5a-2b4c-4e28-b23f-088d193a9817
11:46:44.975 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/66e71a5a-2b4c-4e28-b23f-088d193a9817/_tmp_space.db
11:46:45.012 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:46:45.082 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:46:45.091 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:45.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:45.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:45.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:45.405 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:45.641 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:45.641 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:45.846 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.860 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.861 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:45.862 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:45.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:46:46.191 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:46.205 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:46.206 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:46.207 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:46.207 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:46.208 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:46.209 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:46:46.223 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:46:46.267 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:46:46.270 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:46:46.271 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:46:46.271 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:46:46.272 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:46:46.272 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:46:46.273 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:46:46.273 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:46:46.274 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:46.274 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:46.277 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.278 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.288 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.288 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.310 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.311 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.351 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.358 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.410 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.410 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.483 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.484 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.634 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:46.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:46.955 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:46.956 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:46.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:46.994 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:47.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:47.183 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:47.366 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.366 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.366 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.366 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.366 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:47.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:47.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:47.386 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:47.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:47.389 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:47.650 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:46:47.652 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:46:47.654 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:47.671 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:46:47.676 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:48.254 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 334.875919 ms
11:46:48.583 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 64.781878 ms
11:46:48.721 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
11:46:49.369 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
11:46:49.375 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.7 MB)
11:46:49.379 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw01ReleaseCustomer.scala:66
11:46:49.389 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:49.527 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:46:49.545 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at Dw01ReleaseCustomer.scala:66)
11:46:49.549 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw01ReleaseCustomer.scala:66) with 1 output partitions
11:46:49.550 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw01ReleaseCustomer.scala:66)
11:46:49.551 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:46:49.553 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:49.559 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:46:49.622 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:46:49.626 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:46:49.627 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.7 MB)
11:46:49.627 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:46:49.642 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0))
11:46:49.644 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:46:49.696 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:49.719 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:46:49.810 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:49.812 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
11:46:49.843 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:46:49.854 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 171 ms on localhost (executor driver) (1/1)
11:46:49.857 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:46:49.861 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw01ReleaseCustomer.scala:66) finished in 0.193 s
11:46:49.866 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw01ReleaseCustomer.scala:66, took 0.338939 s
11:46:49.875 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:46:49.876 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw01ReleaseCustomer.scala:66) with 3 output partitions
11:46:49.876 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw01ReleaseCustomer.scala:66)
11:46:49.876 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:46:49.876 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:49.877 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:46:49.880 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:46:49.886 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:46:49.889 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.7 MB)
11:46:49.890 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:46:49.894 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:46:49.894 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:46:49.895 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:49.896 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:49.897 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:49.897 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:46:49.905 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:49.905 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:49.907 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:46:49.907 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
11:46:49.908 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:46:49.972 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 77 ms on localhost (executor driver) (1/3)
11:46:49.973 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:49.974 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:49.975 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:49.975 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:49.975 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
11:46:49.976 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
11:46:49.977 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 81 ms on localhost (executor driver) (2/3)
11:46:49.978 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 82 ms on localhost (executor driver) (3/3)
11:46:49.978 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:46:49.979 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw01ReleaseCustomer.scala:66) finished in 0.084 s
11:46:49.979 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw01ReleaseCustomer.scala:66, took 0.104080 s
11:46:50.004 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:46:50.010 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:50.010 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:50.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.087 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:46:50.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.088 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.089 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.090 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:50.206 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-46-50_204_5165493191383364148-1
11:46:50.527 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:6979 in memory (size: 2.2 KB, free: 1988.7 MB)
11:46:50.531 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
11:46:50.532 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:46:50.532 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
11:46:50.533 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:6979 in memory (size: 2.2 KB, free: 1988.7 MB)
11:46:50.534 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
11:46:50.534 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
11:46:50.534 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
11:46:50.534 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
11:46:50.534 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
11:46:50.536 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.7 MB)
11:46:50.536 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
11:46:50.540 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
11:46:50.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:50.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:50.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:50.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:50.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:50.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:50.784 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.785 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.785 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.785 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.786 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:50.787 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:50.789 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:50.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:50.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:50.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:50.883 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:46:50.884 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
11:46:50.884 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:50.885 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:46:50.885 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:50.918 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:50.920 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:50.978 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.199728 ms
11:46:51.000 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
11:46:51.022 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
11:46:51.023 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.7 MB)
11:46:51.024 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
11:46:51.024 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:51.122 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:46:51.123 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:35)
11:46:51.124 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:46:51.124 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
11:46:51.124 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
11:46:51.124 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:51.125 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:51.217 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 180.1 KB, free 1988.2 MB)
11:46:51.220 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 66.0 KB, free 1988.1 MB)
11:46:51.222 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:6979 (size: 66.0 KB, free: 1988.6 MB)
11:46:51.223 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
11:46:51.224 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:46:51.224 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
11:46:51.225 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:51.226 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:51.226 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:51.226 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:51.226 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
11:46:51.226 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
11:46:51.226 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
11:46:51.234 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
11:46:51.290 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:46:51.296 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:51.298 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
11:46:51.306 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:51.307 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:51.312 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:51.312 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:51.320 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:51.320 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:51.333 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.495028 ms
11:46:51.371 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.233893 ms
11:46:51.516 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
11:46:51.524 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:51.524 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:51.524 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:51.524 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:51.525 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:51.526 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:51.526 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:51.526 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:51.550 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.22925 ms
11:46:51.614 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.497902 ms
11:46:51.639 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.181252 ms
11:46:51.652 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114651_0005_m_000000_0
11:46:51.655 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2631 bytes result sent to driver
11:46:51.657 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114651_0005_m_000003_0
11:46:51.658 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2545 bytes result sent to driver
11:46:51.662 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114651_0005_m_000001_0
11:46:51.663 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2588 bytes result sent to driver
11:46:51.667 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 442 ms on localhost (executor driver) (1/4)
11:46:51.667 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114651_0005_m_000002_0
11:46:51.668 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 443 ms on localhost (executor driver) (2/4)
11:46:51.668 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2588 bytes result sent to driver
11:46:51.668 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 442 ms on localhost (executor driver) (3/4)
11:46:51.670 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 444 ms on localhost (executor driver) (4/4)
11:46:51.670 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
11:46:51.671 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.445 s
11:46:51.672 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.549247 s
11:46:51.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 272
11:46:51.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 278
11:46:51.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 274
11:46:51.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 273
11:46:51.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 330
11:46:51.674 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 280
11:46:51.675 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 5
11:46:51.676 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.32.1:6849 in memory (size: 64.1 KB, free: 1988.6 MB)
11:46:51.679 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 276
11:46:51.680 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.6 MB)
11:46:51.681 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 271
11:46:51.681 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 275
11:46:51.681 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 277
11:46:51.681 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 281
11:46:51.681 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 270
11:46:51.681 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 279
11:46:51.777 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:46:51.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:51.778 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:51.798 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:46:51.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:51.805 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:51.821 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 40). 1782 bytes result sent to driver
11:46:51.821 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 40) in 14826 ms on localhost (executor driver) (8/8)
11:46:51.821 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
11:46:51.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 14.827 s
11:46:51.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:46:51.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:46:51.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
11:46:51.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:46:51.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[54] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:51.823 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:46:51.825 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1988.0 MB)
11:46:51.826 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.32.1:6849 (size: 2.3 KB, free: 1988.6 MB)
11:46:51.826 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1004
11:46:51.826 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[54] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:46:51.827 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks
11:46:51.827 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 45, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:46:51.828 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 45)
11:46:51.830 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:51.830 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:51.831 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
11:46:51.832 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
11:46:51.833 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.833 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:46:51.833 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.834 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.834 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.834 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:46:51.834 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.834 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.835 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
11:46:51.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:46:51.835 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 45). 5958 bytes result sent to driver
11:46:51.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.835 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.836 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 45) in 9 ms on localhost (executor driver) (1/1)
11:46:51.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.836 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
11:46:51.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.836 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.009 s
11:46:51.836 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.837 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:51.837 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 8 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 14.852470 s
11:46:51.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:51.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:51.850 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:46:51.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:51.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:51.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:51.880 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1
11:46:51.880 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:46:51.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:51.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:51.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:51.912 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:51.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:51.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:51.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:51.952 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:51.959 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:46:51.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:46:51.964 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:46:51.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:51.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:51.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:51.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:51.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:51.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:51.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:51.991 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:52.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:52.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:52.045 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#331),(bdp_day#331 = 20190923)
11:46:52.045 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#324),(release_status#324 = 06)
11:46:52.045 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:52.045 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:52.046 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:46:52.054 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:52.055 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:52.061 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.062 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:46:52.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.063 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.065 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.065 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:52.068 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 312.0 KB, free 1987.7 MB)
11:46:52.114 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.6 MB)
11:46:52.114 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.6 MB)
11:46:52.115 INFO  [main] org.apache.spark.SparkContext - Created broadcast 19 from insertInto at SparkHelper.scala:35
11:46:52.116 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:46:52.128 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:52.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:52.129 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:52.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:52.136 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:52.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:52.158 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:52.169 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:46:52.170 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 59 (insertInto at SparkHelper.scala:35)
11:46:52.170 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 9 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:46:52.170 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (insertInto at SparkHelper.scala:35)
11:46:52.170 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 18)
11:46:52.170 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 18)
11:46:52.171 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 18 (MapPartitionsRDD[59] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:52.179 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 21.7 KB, free 1987.6 MB)
11:46:52.192 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.192 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.193 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.193 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.193 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.193 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.193 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.194 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.194 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.194 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:52.205 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.6 MB)
11:46:52.206 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.32.1:6849 (size: 9.6 KB, free: 1988.6 MB)
11:46:52.206 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1004
11:46:52.207 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:52.207 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:52.207 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:52.207 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:52.207 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:52.208 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:52.208 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:46:52.208 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:46:52.210 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[59] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:46:52.210 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 8 tasks
11:46:52.210 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:46:52.211 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:46:52.212 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:46:52.213 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:46:52.213 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 46, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:46:52.214 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 47, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:46:52.214 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 18.0 (TID 48, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:46:52.214 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 18.0 (TID 49, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:46:52.214 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 18.0 (TID 50, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:46:52.215 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 18.0 (TID 51, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:46:52.215 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 18.0 (TID 52, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:46:52.215 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 18.0 (TID 53, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:46:52.216 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:46:52.217 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:46:52.217 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:46:52.217 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 46)
11:46:52.217 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:46:52.218 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:52.218 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:52.219 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.219 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.220 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 47)
11:46:52.222 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:46:52.222 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 18.0 (TID 49)
11:46:52.228 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.222 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 5.0 in stage 18.0 (TID 51)
11:46:52.222 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 7.0 in stage 18.0 (TID 53)
11:46:52.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.224 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:46:52.253 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.253 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.225 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 18.0 (TID 48)
11:46:52.225 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 4.0 in stage 18.0 (TID 50)
11:46:52.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.225 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 6.0 in stage 18.0 (TID 52)
11:46:52.232 INFO  [Executor task launch worker for task 46] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.236 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:46:52.238 INFO  [Executor task launch worker for task 51] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:46:52.238 INFO  [Executor task launch worker for task 53] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:46:52.313 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.314 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.260 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:46:52.317 INFO  [Executor task launch worker for task 47] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.319 INFO  [Executor task launch worker for task 52] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:46:52.271 INFO  [Executor task launch worker for task 50] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:46:52.342 INFO  [Executor task launch worker for task 53] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.351 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 46). 1524 bytes result sent to driver
11:46:52.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.356 INFO  [Executor task launch worker for task 49] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.357 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 46) in 144 ms on localhost (executor driver) (1/8)
11:46:52.359 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 47). 1524 bytes result sent to driver
11:46:52.361 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 47) in 147 ms on localhost (executor driver) (2/8)
11:46:52.364 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 7.0 in stage 18.0 (TID 53). 1524 bytes result sent to driver
11:46:52.366 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 18.0 (TID 53) in 151 ms on localhost (executor driver) (3/8)
11:46:52.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.457 INFO  [Executor task launch worker for task 50] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.460 INFO  [Executor task launch worker for task 48] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.463 INFO  [Executor task launch worker for task 52] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.466 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.467 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.469 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 4.0 in stage 18.0 (TID 50). 1567 bytes result sent to driver
11:46:52.469 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 18.0 (TID 50) in 255 ms on localhost (executor driver) (4/8)
11:46:52.473 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 18.0 (TID 48). 1481 bytes result sent to driver
11:46:52.473 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 18.0 (TID 48) in 259 ms on localhost (executor driver) (5/8)
11:46:52.475 INFO  [Executor task launch worker for task 51] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:52.476 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 6.0 in stage 18.0 (TID 52). 1567 bytes result sent to driver
11:46:52.477 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 18.0 (TID 52) in 262 ms on localhost (executor driver) (6/8)
11:46:52.478 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.478 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.485 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 5.0 in stage 18.0 (TID 51). 1567 bytes result sent to driver
11:46:52.485 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.485 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 18.0 (TID 51) in 271 ms on localhost (executor driver) (7/8)
11:46:52.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:52.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:52.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:52.581 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:52.589 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:52.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:52.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:52.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:52.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:52.642 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:52.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:52.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:52.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:52.674 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#212),(bdp_day#212 = 20190921)
11:46:52.674 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#205),(release_status#205 = 01)
11:46:52.675 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:52.675 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:46:52.676 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:52.702 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 312.0 KB, free 1987.8 MB)
11:46:52.719 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.8 MB)
11:46:52.721 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.6 MB)
11:46:52.722 INFO  [main] org.apache.spark.SparkContext - Created broadcast 5 from show at Dw01ReleaseCustomer.scala:66
11:46:52.723 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:52.736 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:46:52.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 20 (show at Dw01ReleaseCustomer.scala:66)
11:46:52.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw01ReleaseCustomer.scala:66) with 1 output partitions
11:46:52.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (show at Dw01ReleaseCustomer.scala:66)
11:46:52.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
11:46:52.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:52.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[22] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:46:52.739 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1987.8 MB)
11:46:52.743 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.8 MB)
11:46:52.745 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.6 MB)
11:46:52.745 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
11:46:52.746 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0))
11:46:52.746 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
11:46:52.747 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:52.747 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 8)
11:46:52.750 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:52.750 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:52.752 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 8). 1225 bytes result sent to driver
11:46:52.752 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 8) in 5 ms on localhost (executor driver) (1/1)
11:46:52.753 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
11:46:52.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (show at Dw01ReleaseCustomer.scala:66) finished in 0.006 s
11:46:52.753 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw01ReleaseCustomer.scala:66, took 0.017134 s
11:46:52.757 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:46:52.757 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at Dw01ReleaseCustomer.scala:66) with 3 output partitions
11:46:52.758 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at Dw01ReleaseCustomer.scala:66)
11:46:52.758 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
11:46:52.758 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:52.758 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[22] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:46:52.760 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1987.8 MB)
11:46:52.763 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.8 MB)
11:46:52.765 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.6 MB)
11:46:52.766 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
11:46:52.766 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:46:52.766 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 3 tasks
11:46:52.767 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:52.768 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:52.768 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:52.768 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 9)
11:46:52.768 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 10)
11:46:52.768 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 11)
11:46:52.770 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:52.770 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:52.770 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:52.770 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:52.770 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:52.770 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:52.772 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 10). 1182 bytes result sent to driver
11:46:52.772 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 11). 1182 bytes result sent to driver
11:46:52.772 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 9). 1182 bytes result sent to driver
11:46:52.773 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 10) in 6 ms on localhost (executor driver) (1/3)
11:46:52.773 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 11) in 5 ms on localhost (executor driver) (2/3)
11:46:52.773 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 9) in 6 ms on localhost (executor driver) (3/3)
11:46:52.774 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
11:46:52.774 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at Dw01ReleaseCustomer.scala:66) finished in 0.007 s
11:46:52.775 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at Dw01ReleaseCustomer.scala:66, took 0.017588 s
11:46:52.782 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:46:52.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:52.783 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:52.810 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.811 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:46:52.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.814 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:52.823 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-46-52_823_8339371488139502363-1
11:46:52.909 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:52.909 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:52.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:52.915 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:52.937 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:52.937 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:52.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.960 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.961 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.961 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.961 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.961 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:52.961 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:52.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:52.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:52.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:52.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:52.991 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#212),(bdp_day#212 = 20190921)
11:46:52.991 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#205),(release_status#205 = 01)
11:46:52.992 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:52.992 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:46:52.993 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:46:53.003 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:53.003 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:53.020 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 312.0 KB, free 1987.5 MB)
11:46:53.038 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.5 MB)
11:46:53.040 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.6 MB)
11:46:53.040 INFO  [main] org.apache.spark.SparkContext - Created broadcast 8 from insertInto at SparkHelper.scala:35
11:46:53.041 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:46:53.092 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:46:53.093 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 27 (insertInto at SparkHelper.scala:35)
11:46:53.093 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:46:53.093 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (insertInto at SparkHelper.scala:35)
11:46:53.093 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
11:46:53.093 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:46:53.093 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:53.140 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 180.1 KB, free 1987.3 MB)
11:46:53.144 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 66.0 KB, free 1987.2 MB)
11:46:53.146 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:6979 (size: 66.0 KB, free: 1988.5 MB)
11:46:53.146 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
11:46:53.147 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 11 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:46:53.147 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 4 tasks
11:46:53.148 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:46:53.148 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:46:53.149 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:46:53.149 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:46:53.149 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 13)
11:46:53.149 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 12)
11:46:53.149 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 14)
11:46:53.149 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 15)
11:46:53.171 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:53.172 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:53.172 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:53.172 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:53.174 INFO  [Executor task launch worker for task 14] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:53.174 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:53.174 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:53.175 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:53.175 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:53.175 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:53.177 INFO  [Executor task launch worker for task 13] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:53.178 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:53.180 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:46:53.180 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:53.182 INFO  [Executor task launch worker for task 14] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114653_0011_m_000002_0
11:46:53.183 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114653_0011_m_000000_0
11:46:53.183 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 14). 2502 bytes result sent to driver
11:46:53.183 INFO  [Executor task launch worker for task 15] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:53.183 INFO  [Executor task launch worker for task 13] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114653_0011_m_000001_0
11:46:53.183 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:53.183 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 12). 2502 bytes result sent to driver
11:46:53.184 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 13). 2502 bytes result sent to driver
11:46:53.184 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 14) in 35 ms on localhost (executor driver) (1/4)
11:46:53.184 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 13) in 36 ms on localhost (executor driver) (2/4)
11:46:53.185 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 12) in 37 ms on localhost (executor driver) (3/4)
11:46:53.187 INFO  [Executor task launch worker for task 15] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926114653_0011_m_000003_0
11:46:53.188 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 15). 2545 bytes result sent to driver
11:46:53.190 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 15) in 41 ms on localhost (executor driver) (4/4)
11:46:53.190 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
11:46:53.191 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (insertInto at SparkHelper.scala:35) finished in 0.044 s
11:46:53.191 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: insertInto at SparkHelper.scala:35, took 0.098632 s
11:46:53.213 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:46:53.214 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:53.214 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:53.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:53.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:53.273 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.273 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.273 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.274 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.275 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.275 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.275 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.275 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.275 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.275 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:53.276 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:53.277 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:53.307 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:46:53.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:53.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:53.341 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:46:53.341 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:46:53.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:46:53.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:53.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:53.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:46:53.373 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:46:53.396 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7060.
11:46:53.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.400 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.401 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.402 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.403 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:53.421 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:53.422 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:53.422 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:53.428 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:53.428 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:53.429 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:46:53.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:53.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:53.456 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:46:53.461 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:46:53.462 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:46:53.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:53.477 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-92d0f6d4-774a-48c3-b0fe-2bf494f402d8
11:46:53.480 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:53.480 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:53.480 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:53.480 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:53.480 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:53.481 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:53.481 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:46:53.481 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:46:53.482 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:46:53.483 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:46:53.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:46:53.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:46:53.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:46:53.485 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:46:53.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:46:53.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:46:53.487 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:53.487 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:53.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.493 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.499 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.505 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
11:46:53.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.516 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.517 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.524 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.552 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:53.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:53.574 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:46:53.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:53.672 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:53.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:53.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:53.716 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:53.716 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:53.741 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.741 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.741 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:53.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:53.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:53.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:53.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:53.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:53.754 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @5695ms
11:46:53.828 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#405),(bdp_day#405 = 20190922)
11:46:53.829 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#398),(release_status#398 = 01)
11:46:53.829 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:53.830 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:46:53.830 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:46:53.849 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:46:53.855 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 312.0 KB, free 1986.9 MB)
11:46:53.874 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1986.9 MB)
11:46:53.877 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.5 MB)
11:46:53.878 INFO  [main] org.apache.spark.SparkContext - Created broadcast 10 from show at Dw01ReleaseCustomer.scala:66
11:46:53.877 INFO  [main] org.spark_project.jetty.server.Server - Started @5820ms
11:46:53.879 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:46:53.913 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
11:46:53.914 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
11:46:53.926 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:46:53.927 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 36 (show at Dw01ReleaseCustomer.scala:66)
11:46:53.927 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (show at Dw01ReleaseCustomer.scala:66) with 1 output partitions
11:46:53.927 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (show at Dw01ReleaseCustomer.scala:66)
11:46:53.927 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
11:46:53.928 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
11:46:53.929 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[36] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:46:53.928 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@5df8a180{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
11:46:53.932 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4042.
11:46:53.958 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 24.0 KB, free 1986.9 MB)
11:46:53.961 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1986.9 MB)
11:46:53.963 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:6979 (size: 10.5 KB, free: 1988.5 MB)
11:46:53.964 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
11:46:53.967 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[36] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:46:53.968 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 8 tasks
11:46:53.970 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af1347d{/jobs,null,AVAILABLE,@Spark}
11:46:53.973 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@251ebf23{/jobs/json,null,AVAILABLE,@Spark}
11:46:53.974 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b70203f{/jobs/job,null,AVAILABLE,@Spark}
11:46:53.976 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1162410a{/jobs/job/json,null,AVAILABLE,@Spark}
11:46:53.977 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62df0ff3{/stages,null,AVAILABLE,@Spark}
11:46:53.978 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e8f862{/stages/json,null,AVAILABLE,@Spark}
11:46:53.979 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c49fab6{/stages/stage,null,AVAILABLE,@Spark}
11:46:53.981 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:46:53.981 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f3ddbd9{/stages/stage/json,null,AVAILABLE,@Spark}
11:46:53.982 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 17, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:46:53.982 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 18, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:46:53.982 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c2d4cc6{/stages/pool,null,AVAILABLE,@Spark}
11:46:53.983 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6134ac4a{/stages/pool/json,null,AVAILABLE,@Spark}
11:46:53.988 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71b1a49c{/storage,null,AVAILABLE,@Spark}
11:46:53.988 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 19, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:46:53.989 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 20, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:46:53.990 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 21, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:46:53.990 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 22, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:46:53.990 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 23, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:46:53.991 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)
11:46:53.992 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 18)
11:46:53.989 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3773862a{/storage/json,null,AVAILABLE,@Spark}
11:46:53.994 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@589b028e{/storage/rdd,null,AVAILABLE,@Spark}
11:46:53.995 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9fecdf1{/storage/rdd/json,null,AVAILABLE,@Spark}
11:46:53.996 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 21)
11:46:53.997 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b0f7d9d{/environment,null,AVAILABLE,@Spark}
11:46:53.997 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 20)
11:46:53.998 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c84624f{/environment/json,null,AVAILABLE,@Spark}
11:46:53.998 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 19)
11:46:53.999 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@232024b9{/executors,null,AVAILABLE,@Spark}
11:46:53.999 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 17)
11:46:54.000 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2a415aa9{/executors/json,null,AVAILABLE,@Spark}
11:46:54.001 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71ea1fda{/executors/threadDump,null,AVAILABLE,@Spark}
11:46:54.002 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@420745d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:46:54.010 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 23)
11:46:54.010 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 22)
11:46:54.033 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fa47fea{/static,null,AVAILABLE,@Spark}
11:46:54.035 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5298dead{/,null,AVAILABLE,@Spark}
11:46:54.037 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c7a078{/api,null,AVAILABLE,@Spark}
11:46:54.038 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@15b986cd{/jobs/job/kill,null,AVAILABLE,@Spark}
11:46:54.041 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41c62850{/stages/stage/kill,null,AVAILABLE,@Spark}
11:46:54.045 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4042
11:46:54.105 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 45.513518 ms
11:46:54.111 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:46:54.111 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:46:54.117 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:46:54.117 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:46:54.118 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:46:54.119 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:46:54.119 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:46:54.119 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:46:54.313 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:46:54.443 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7123.
11:46:54.445 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:7123
11:46:54.447 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:46:54.579 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 7123, None)
11:46:54.592 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:7123 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 7123, None)
11:46:54.598 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 7123, None)
11:46:54.599 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 7123, None)
11:46:55.286 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4af46df3{/metrics/json,null,AVAILABLE,@Spark}
11:46:55.365 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 336
11:46:55.373 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.32.1:6849 in memory (size: 2.3 KB, free: 1988.6 MB)
11:46:55.375 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 6
11:46:55.375 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 332
11:46:55.375 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 338
11:46:55.411 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.6 MB)
11:46:55.415 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 334
11:46:55.418 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.32.1:6849 in memory (size: 9.6 KB, free: 1988.6 MB)
11:46:55.421 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 331
11:46:55.421 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 337
11:46:55.421 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 335
11:46:55.421 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 333
11:46:55.422 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 387
11:46:55.582 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 18.0 (TID 49). 1739 bytes result sent to driver
11:46:55.583 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 18.0 (TID 49) in 3369 ms on localhost (executor driver) (8/8)
11:46:55.583 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
11:46:55.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 18 (insertInto at SparkHelper.scala:35) finished in 3.371 s
11:46:55.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:46:55.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:46:55.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 19)
11:46:55.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:46:55.584 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[62] at insertInto at SparkHelper.scala:35), which has no missing parents
11:46:55.633 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
11:46:55.636 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
11:46:55.636 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.32.1:6849 (size: 64.1 KB, free: 1988.6 MB)
11:46:55.637 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1004
11:46:55.638 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 19 (MapPartitionsRDD[62] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:46:55.638 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 4 tasks
11:46:55.639 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 54, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:46:55.639 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 55, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:46:55.639 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 56, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:46:55.640 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 57, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:46:55.640 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 54)
11:46:55.640 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 55)
11:46:55.640 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 56)
11:46:55.640 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 57)
11:46:55.655 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:55.655 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:55.655 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:55.655 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:46:55.664 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:55.664 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:55.665 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:46:55.665 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:46:55.668 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:55.669 INFO  [Executor task launch worker for task 57] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:55.670 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:55.671 INFO  [Executor task launch worker for task 56] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:55.679 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@68242b4
11:46:55.679 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:55.679 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:55.679 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114655_0019_m_000002_0/bdp_day=20190923/part-00002-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:55.680 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:55.680 INFO  [Executor task launch worker for task 54] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:55.681 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@74f0ee7d
11:46:55.681 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:55.681 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114655_0019_m_000003_0/bdp_day=20190923/part-00003-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:55.682 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:55.683 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:46:55.683 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@49dd48c4
11:46:55.684 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:55.684 INFO  [Executor task launch worker for task 55] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:46:55.684 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114655_0019_m_000000_0/bdp_day=20190923/part-00000-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:46:55.685 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:55.685 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:55.685 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:55.685 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:55.685 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:55.685 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:55.686 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:55.686 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:55.687 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2de8f090
11:46:55.687 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1faf47a1
11:46:55.687 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:46:55.688 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@63abaca4
11:46:55.689 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114655_0019_m_000001_0/bdp_day=20190923/part-00001-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression set to false
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Validation is off
11:46:55.691 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:46:55.763 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6f7048d3
11:46:55.764 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@46607df3
11:46:55.769 INFO  [Executor task launch worker for task 56] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,684
11:46:55.778 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.778 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.779 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.779 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.779 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:55.779 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:55.779 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.779 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.789 INFO  [Executor task launch worker for task 57] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,692
11:46:55.798 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.798 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.799 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.799 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.799 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:55.799 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:55.799 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.799 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.834 INFO  [Executor task launch worker for task 54] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,830
11:46:55.835 INFO  [Executor task launch worker for task 55] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,756
11:46:55.916 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.916 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.917 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.917 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.917 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:55.918 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:55.918 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.918 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.919 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.920 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.921 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.921 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:55.921 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:46:55.922 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:46:55.922 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:46:55.922 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:46:56.037 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114655_0019_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/task_20190926114655_0019_m_000003
11:46:56.037 INFO  [Executor task launch worker for task 57] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114655_0019_m_000003_0: Committed
11:46:56.037 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 57). 2529 bytes result sent to driver
11:46:56.039 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 57) in 399 ms on localhost (executor driver) (1/4)
11:46:56.045 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114655_0019_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/task_20190926114655_0019_m_000001
11:46:56.045 INFO  [Executor task launch worker for task 55] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114655_0019_m_000001_0: Committed
11:46:56.047 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 55). 2529 bytes result sent to driver
11:46:56.047 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114655_0019_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/task_20190926114655_0019_m_000002
11:46:56.047 INFO  [Executor task launch worker for task 56] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114655_0019_m_000002_0: Committed
11:46:56.047 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 55) in 408 ms on localhost (executor driver) (2/4)
11:46:56.048 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 56). 2529 bytes result sent to driver
11:46:56.049 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 56) in 410 ms on localhost (executor driver) (3/4)
11:46:56.343 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.343 INFO  [Executor task launch worker for task 17] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.361 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.371 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.422 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 17). 1567 bytes result sent to driver
11:46:56.423 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 23). 1610 bytes result sent to driver
11:46:56.429 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 20). 1524 bytes result sent to driver
11:46:56.435 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 17) in 2454 ms on localhost (executor driver) (1/8)
11:46:56.435 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 20) in 2447 ms on localhost (executor driver) (2/8)
11:46:56.435 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 23) in 2445 ms on localhost (executor driver) (3/8)
11:46:56.453 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114655_0019_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/_temporary/0/task_20190926114655_0019_m_000000
11:46:56.453 INFO  [Executor task launch worker for task 54] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114655_0019_m_000000_0: Committed
11:46:56.454 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 54). 2529 bytes result sent to driver
11:46:56.455 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 54) in 817 ms on localhost (executor driver) (4/4)
11:46:56.455 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
11:46:56.455 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 19 (insertInto at SparkHelper.scala:35) finished in 0.817 s
11:46:56.455 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 9 finished: insertInto at SparkHelper.scala:35, took 4.286149 s
11:46:56.463 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.475 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 1567 bytes result sent to driver
11:46:56.477 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 2502 ms on localhost (executor driver) (4/8)
11:46:56.516 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.530 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 22). 1610 bytes result sent to driver
11:46:56.531 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 22) in 2541 ms on localhost (executor driver) (5/8)
11:46:56.572 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.584 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:46:56.584 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 21). 1524 bytes result sent to driver
11:46:56.586 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 21) in 2597 ms on localhost (executor driver) (6/8)
11:46:56.755 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:46:56.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:56.756 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:56.786 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:56.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:56.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:56.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:56.818 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:56.818 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:56.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:56.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:56.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:56.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:56.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:56.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:56.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:46:56.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:46:56.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:56.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:56.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]
11:46:56.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]	
11:46:56.955 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00000-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
11:46:56.961 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:46:56.966 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00001-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
11:46:56.967 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:46:56.970 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00002-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
11:46:56.971 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:46:56.974 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00003-dd87c869-8f15-4d6a-9436-4d95dfadc97a.c000
11:46:56.976 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:46:56.992 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/bdp_day=20190923/part-00000-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00000-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, Status:true
11:46:57.009 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/bdp_day=20190923/part-00001-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00001-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, Status:true
11:46:57.015 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 18). 1524 bytes result sent to driver
11:46:57.015 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 18) in 3033 ms on localhost (executor driver) (7/8)
11:46:57.027 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/bdp_day=20190923/part-00002-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00002-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, Status:true
11:46:57.094 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/bdp_day=20190923/part-00003-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00003-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000, Status:true
11:46:57.101 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]
11:46:57.101 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]	
11:46:57.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_register_users
11:46:57.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_register_users	
11:46:57.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190923]
11:46:57.176 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
11:46:57.179 WARN  [main] hive.log - Updated size to 872108
11:46:57.861 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
11:46:57.894 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
11:46:57.895 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
11:46:57.905 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78b7f805{/SQL,null,AVAILABLE,@Spark}
11:46:57.906 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5809fa26{/SQL/json,null,AVAILABLE,@Spark}
11:46:57.906 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b53840a{/SQL/execution,null,AVAILABLE,@Spark}
11:46:57.907 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e546734{/SQL/execution/json,null,AVAILABLE,@Spark}
11:46:57.910 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e9474f{/static/sql,null,AVAILABLE,@Spark}
11:46:58.703 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:46:59.187 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-46-51_880_2223409124359066423-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
11:46:59.190 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:46:59.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:46:59.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:46:59.196 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:59.196 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:59.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:46:59.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:46:59.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.245 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.246 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.246 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.246 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.246 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.246 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:59.263 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:46:59.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:59.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:59.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:59.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:59.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:59.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:59.327 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.328 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.328 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.328 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.328 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:59.345 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:46:59.346 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:46:59.347 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:46:59.347 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:46:59.347 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:46:59.347 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:46:59.347 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:46:59.347 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:46:59.348 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:46:59.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:46:59.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:46:59.383 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:46:59.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:46:59.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:59.396 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:59.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:46:59.421 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:46:59.446 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.447 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.447 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:46:59.448 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:46:59.449 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:46:59.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:46:59.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:46:59.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:46:59.707 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#435),(bdp_day#435 = 20190924)
11:46:59.707 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#428),(release_status#428 = 06)
11:46:59.708 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:46:59.708 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:46:59.708 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:46:59.721 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
11:46:59.736 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
11:46:59.737 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.5 MB)
11:46:59.738 INFO  [main] org.apache.spark.SparkContext - Created broadcast 22 from show at Dw05ReleaseRegisterUsers.scala:66
11:46:59.738 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:46:59.752 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:46:59.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 68 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:59.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 10 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:46:59.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (show at Dw05ReleaseRegisterUsers.scala:66)
11:46:59.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 20)
11:46:59.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 20)
11:46:59.753 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 20 (MapPartitionsRDD[68] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:46:59.761 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 21.7 KB, free 1987.4 MB)
11:46:59.763 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.4 MB)
11:46:59.763 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.32.1:6849 (size: 9.6 KB, free: 1988.5 MB)
11:46:59.763 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1004
11:46:59.764 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[68] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:46:59.764 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 8 tasks
11:46:59.765 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 58, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:46:59.765 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 59, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:46:59.765 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 20.0 (TID 60, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:46:59.765 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 20.0 (TID 61, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:46:59.766 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 20.0 (TID 62, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:46:59.766 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 20.0 (TID 63, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:46:59.767 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 20.0 (TID 64, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:46:59.767 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 20.0 (TID 65, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:46:59.767 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 58)
11:46:59.769 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 59)
11:46:59.772 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Running task 3.0 in stage 20.0 (TID 61)
11:46:59.772 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Running task 4.0 in stage 20.0 (TID 62)
11:46:59.774 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Running task 7.0 in stage 20.0 (TID 65)
11:46:59.774 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Running task 6.0 in stage 20.0 (TID 64)
11:46:59.776 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 2.0 in stage 20.0 (TID 60)
11:46:59.776 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Running task 5.0 in stage 20.0 (TID 63)
11:46:59.778 INFO  [Executor task launch worker for task 65] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:46:59.778 INFO  [Executor task launch worker for task 58] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:46:59.778 INFO  [Executor task launch worker for task 59] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:46:59.790 INFO  [Executor task launch worker for task 64] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:46:59.790 INFO  [Executor task launch worker for task 60] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:46:59.791 INFO  [Executor task launch worker for task 61] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:46:59.794 INFO  [Executor task launch worker for task 63] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:46:59.803 INFO  [Executor task launch worker for task 62] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:46:59.833 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 87
11:46:59.841 INFO  [Executor task launch worker for task 59] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.851 INFO  [Executor task launch worker for task 60] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.857 INFO  [Executor task launch worker for task 62] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.860 INFO  [Executor task launch worker for task 63] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.861 INFO  [Executor task launch worker for task 64] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.865 INFO  [Executor task launch worker for task 58] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.865 INFO  [Executor task launch worker for task 65] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.867 INFO  [Executor task launch worker for task 61] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:46:59.868 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:6979 in memory (size: 66.0 KB, free: 1988.5 MB)
11:46:59.868 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 122
11:46:59.869 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 184
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 118
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 124
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 178
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 123
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 121
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 177
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 212
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 120
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 180
11:46:59.870 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 182
11:46:59.871 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
11:46:59.924 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:6979 in memory (size: 2.2 KB, free: 1988.5 MB)
11:46:59.926 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.5 MB)
11:46:59.927 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 186
11:46:59.929 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:6979 in memory (size: 66.0 KB, free: 1988.6 MB)
11:46:59.930 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 175
11:46:59.930 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 187
11:46:59.930 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 179
11:46:59.930 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 183
11:46:59.930 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 126
11:46:59.930 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 185
11:46:59.931 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 181
11:46:59.931 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 119
11:46:59.932 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:6979 in memory (size: 2.2 KB, free: 1988.6 MB)
11:46:59.933 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 176
11:46:59.933 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 125
11:46:59.935 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.6 MB)
11:46:59.938 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.7 MB)
11:46:59.940 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 86
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 93
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 90
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 92
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 83
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 85
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 91
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 89
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 84
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 82
11:46:59.941 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 88
11:46:59.942 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 1
11:47:00.412 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 59). 1567 bytes result sent to driver
11:47:00.413 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 59) in 648 ms on localhost (executor driver) (1/8)
11:47:00.425 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Finished task 4.0 in stage 20.0 (TID 62). 1524 bytes result sent to driver
11:47:00.429 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 20.0 (TID 62) in 663 ms on localhost (executor driver) (2/8)
11:47:00.641 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Finished task 6.0 in stage 20.0 (TID 64). 1524 bytes result sent to driver
11:47:00.641 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:47:00.642 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 20.0 (TID 64) in 876 ms on localhost (executor driver) (3/8)
11:47:00.656 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Finished task 5.0 in stage 20.0 (TID 63). 1567 bytes result sent to driver
11:47:00.658 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 20.0 (TID 63) in 892 ms on localhost (executor driver) (4/8)
11:47:00.686 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 2.0 in stage 20.0 (TID 60). 1481 bytes result sent to driver
11:47:00.688 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 20.0 (TID 60) in 923 ms on localhost (executor driver) (5/8)
11:47:00.702 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:47:00.887 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 58). 1524 bytes result sent to driver
11:47:00.925 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 58) in 1160 ms on localhost (executor driver) (6/8)
11:47:00.954 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Finished task 7.0 in stage 20.0 (TID 65). 1524 bytes result sent to driver
11:47:00.954 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 20.0 (TID 65) in 1187 ms on localhost (executor driver) (7/8)
11:47:02.955 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:47:03.330 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:47:03.360 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
11:47:03.360 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
11:47:03.361 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:47:03.362 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:47:03.362 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
11:47:03.385 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 19). 1739 bytes result sent to driver
11:47:03.387 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 19) in 9405 ms on localhost (executor driver) (8/8)
11:47:03.388 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
11:47:03.389 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (show at Dw01ReleaseCustomer.scala:66) finished in 9.417 s
11:47:03.390 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:47:03.392 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:47:03.393 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
11:47:03.393 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:47:03.397 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[38] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:47:03.401 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
11:47:03.403 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
11:47:03.405 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.7 MB)
11:47:03.405 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
11:47:03.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[38] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0))
11:47:03.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks
11:47:03.407 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 24, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:47:03.408 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 24)
11:47:03.414 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:47:03.414 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:47:03.449 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 24). 11772 bytes result sent to driver
11:47:03.451 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 24) in 44 ms on localhost (executor driver) (1/1)
11:47:03.451 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
11:47:03.451 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (show at Dw01ReleaseCustomer.scala:66) finished in 0.044 s
11:47:03.451 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: show at Dw01ReleaseCustomer.scala:66, took 9.525046 s
11:47:03.475 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:47:03.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:47:03.477 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:47:04.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:47:04.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:47:04.157 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1
11:47:04.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:47:04.223 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:47:04.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:47:04.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:47:04.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:47:04.348 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:47:04.351 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:47:04.451 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7183.
11:47:04.478 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.479 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:04.480 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:47:04.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:47:04.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:47:04.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:47:04.482 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:47:04.502 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:47:04.506 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:47:04.507 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:47:04.518 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-6bd49935-e26c-4543-af4b-7f2e078fb5d8
11:47:04.537 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
11:47:04.595 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:47:04.617 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#405),(bdp_day#405 = 20190922)
11:47:04.617 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#398),(release_status#398 = 01)
11:47:04.618 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:47:04.619 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:47:04.619 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:47:04.630 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:47:04.630 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:47:04.662 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
11:47:04.684 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:47:04.686 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.6 MB)
11:47:04.687 INFO  [main] org.apache.spark.SparkContext - Created broadcast 13 from insertInto at SparkHelper.scala:35
11:47:04.687 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:47:04.734 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:47:04.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 43 (insertInto at SparkHelper.scala:35)
11:47:04.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 7 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:47:04.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (insertInto at SparkHelper.scala:35)
11:47:04.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
11:47:04.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
11:47:04.740 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35), which has no missing parents
11:47:04.744 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 24.0 KB, free 1988.0 MB)
11:47:04.747 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1988.0 MB)
11:47:04.748 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:6979 (size: 10.5 KB, free: 1988.6 MB)
11:47:04.748 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1004
11:47:04.748 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:47:04.748 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 8 tasks
11:47:04.750 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 25, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:47:04.750 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 26, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:47:04.751 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 27, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:47:04.751 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 28, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:47:04.752 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 29, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:47:04.752 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 30, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:47:04.752 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 31, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:47:04.752 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 32, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:47:04.753 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 25)
11:47:04.753 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 29)
11:47:04.753 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 30)
11:47:04.753 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 26)
11:47:04.760 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @6284ms
11:47:04.753 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 27)
11:47:04.753 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 28)
11:47:04.753 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 31)
11:47:04.753 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 32)
11:47:04.760 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:47:04.762 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:47:04.764 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:47:04.771 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:47:04.773 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:47:04.779 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:47:04.779 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:47:04.779 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:47:04.862 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.864 INFO  [Executor task launch worker for task 26] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.871 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.875 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.879 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.892 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.926 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 30). 1524 bytes result sent to driver
11:47:04.928 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 26). 1567 bytes result sent to driver
11:47:04.932 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.939 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:47:04.946 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 30) in 194 ms on localhost (executor driver) (1/8)
11:47:04.950 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 27). 1610 bytes result sent to driver
11:47:04.955 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 32). 1524 bytes result sent to driver
11:47:04.957 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 26) in 207 ms on localhost (executor driver) (2/8)
11:47:04.958 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 27) in 208 ms on localhost (executor driver) (3/8)
11:47:04.959 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 32) in 207 ms on localhost (executor driver) (4/8)
11:47:04.959 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 31). 1524 bytes result sent to driver
11:47:04.963 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 31) in 211 ms on localhost (executor driver) (5/8)
11:47:04.967 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 29). 1524 bytes result sent to driver
11:47:04.968 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 29) in 217 ms on localhost (executor driver) (6/8)
11:47:04.970 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 25). 1524 bytes result sent to driver
11:47:04.971 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 25) in 221 ms on localhost (executor driver) (7/8)
11:47:04.996 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:47:05.020 INFO  [main] org.spark_project.jetty.server.Server - Started @6549ms
11:47:05.044 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
11:47:05.045 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
11:47:05.046 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
11:47:05.056 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@20282cb{HTTP/1.1,[http/1.1]}{0.0.0.0:4043}
11:47:05.057 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4043.
11:47:05.088 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632aa1a3{/jobs,null,AVAILABLE,@Spark}
11:47:05.089 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/json,null,AVAILABLE,@Spark}
11:47:05.090 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/jobs/job,null,AVAILABLE,@Spark}
11:47:05.091 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs/job/json,null,AVAILABLE,@Spark}
11:47:05.092 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages,null,AVAILABLE,@Spark}
11:47:05.093 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/json,null,AVAILABLE,@Spark}
11:47:05.094 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage,null,AVAILABLE,@Spark}
11:47:05.096 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/stage/json,null,AVAILABLE,@Spark}
11:47:05.097 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/stages/pool,null,AVAILABLE,@Spark}
11:47:05.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/stages/pool/json,null,AVAILABLE,@Spark}
11:47:05.098 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage,null,AVAILABLE,@Spark}
11:47:05.099 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/json,null,AVAILABLE,@Spark}
11:47:05.100 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/storage/rdd,null,AVAILABLE,@Spark}
11:47:05.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/storage/rdd/json,null,AVAILABLE,@Spark}
11:47:05.102 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/environment,null,AVAILABLE,@Spark}
11:47:05.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/environment/json,null,AVAILABLE,@Spark}
11:47:05.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors,null,AVAILABLE,@Spark}
11:47:05.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/json,null,AVAILABLE,@Spark}
11:47:05.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/executors/threadDump,null,AVAILABLE,@Spark}
11:47:05.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:47:05.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/static,null,AVAILABLE,@Spark}
11:47:05.115 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/,null,AVAILABLE,@Spark}
11:47:05.116 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/api,null,AVAILABLE,@Spark}
11:47:05.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/jobs/job/kill,null,AVAILABLE,@Spark}
11:47:05.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:47:05.120 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4043
11:47:05.228 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:47:05.273 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7247.
11:47:05.274 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:7247
11:47:05.276 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:47:05.321 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 7247, None)
11:47:05.325 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:7247 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 7247, None)
11:47:05.330 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 7247, None)
11:47:05.331 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 7247, None)
11:47:05.567 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4158debd{/metrics/json,null,AVAILABLE,@Spark}
11:47:07.484 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
11:47:07.523 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
11:47:07.523 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
11:47:07.533 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL,null,AVAILABLE,@Spark}
11:47:07.534 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/json,null,AVAILABLE,@Spark}
11:47:07.534 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23468512{/SQL/execution,null,AVAILABLE,@Spark}
11:47:07.535 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/SQL/execution/json,null,AVAILABLE,@Spark}
11:47:07.537 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f2276c9{/static/sql,null,AVAILABLE,@Spark}
11:47:08.096 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:47:08.812 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:47:08.842 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:47:11.886 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:47:11.889 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:47:12.926 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 216
11:47:12.926 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 214
11:47:12.926 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 218
11:47:12.926 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 219
11:47:12.926 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 220
11:47:12.926 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
11:47:12.926 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 217
11:47:12.928 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.7 MB)
11:47:13.026 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:6979 in memory (size: 2.2 KB, free: 1988.7 MB)
11:47:13.027 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 269
11:47:13.027 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 213
11:47:13.027 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 215
11:47:13.028 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:6979 in memory (size: 10.5 KB, free: 1988.7 MB)
11:47:14.805 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 28). 1739 bytes result sent to driver
11:47:14.806 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 28) in 10055 ms on localhost (executor driver) (8/8)
11:47:14.807 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
11:47:14.807 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (insertInto at SparkHelper.scala:35) finished in 10.058 s
11:47:14.807 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:47:14.807 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:47:14.807 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
11:47:14.807 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:47:14.807 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35), which has no missing parents
11:47:14.829 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
11:47:14.831 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 66.0 KB, free 1988.1 MB)
11:47:14.832 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:6979 (size: 66.0 KB, free: 1988.6 MB)
11:47:14.833 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1004
11:47:14.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 15 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:47:14.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 4 tasks
11:47:14.834 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 33, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:47:14.834 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 34, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:47:14.834 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 15.0 (TID 35, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:47:14.834 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 15.0 (TID 36, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:47:14.834 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 34)
11:47:14.834 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 33)
11:47:14.834 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 2.0 in stage 15.0 (TID 35)
11:47:14.834 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 3.0 in stage 15.0 (TID 36)
11:47:14.856 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:47:14.856 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:47:14.856 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:47:14.856 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:47:14.859 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:47:14.859 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:47:14.859 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:47:14.859 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:47:15.001 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:47:15.001 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:47:15.001 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:47:15.001 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:47:15.001 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:47:15.002 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:47:15.002 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:47:15.002 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:47:15.088 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@54ce3295
11:47:15.090 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7310df06
11:47:15.090 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3519488f
11:47:15.090 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a6d4d3e
11:47:15.100 INFO  [Executor task launch worker for task 36] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
11:47:15.105 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:47:15.105 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:47:15.891 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:47:15.891 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:47:15.892 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114715_0015_m_000002_0/bdp_day=20190922/part-00002-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000
11:47:15.892 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114715_0015_m_000001_0/bdp_day=20190922/part-00001-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000
11:47:15.892 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114715_0015_m_000000_0/bdp_day=20190922/part-00000-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000
11:47:15.892 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114715_0015_m_000003_0/bdp_day=20190922/part-00003-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000
11:47:15.894 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression set to false
11:47:15.894 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression set to false
11:47:15.894 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression set to false
11:47:15.894 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression set to false
11:47:15.895 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:47:15.895 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:47:15.895 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:47:15.895 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:47:15.896 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:47:15.896 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:47:15.896 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:47:15.896 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:47:15.896 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:47:15.896 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:47:15.896 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:47:15.896 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:47:15.896 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:47:15.896 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Validation is off
11:47:15.896 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Validation is off
11:47:15.896 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Validation is off
11:47:15.896 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Validation is off
11:47:15.897 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:47:15.897 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:47:15.897 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:47:15.897 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:47:17.077 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2b8c41ca
11:47:17.077 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3b01dda3
11:47:17.078 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@778f2f5f
11:47:17.078 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6ea7e41
11:47:17.891 INFO  [Executor task launch worker for task 33] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,753
11:47:17.936 INFO  [Executor task launch worker for task 35] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,987
11:47:17.968 INFO  [Executor task launch worker for task 34] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,847
11:47:17.990 INFO  [Executor task launch worker for task 36] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,892
11:47:20.020 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.020 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.020 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.020 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.022 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.022 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.022 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.022 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.023 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.023 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.023 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.023 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.024 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:47:20.024 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,447 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:47:20.024 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:47:20.024 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:47:20.024 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:47:20.025 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:47:20.025 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.025 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:47:20.025 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:47:20.025 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.025 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.025 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:47:20.026 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.026 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:47:20.027 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.027 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 2,804B for [gender] BINARY: 21,447 values, 2,773B raw, 2,773B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:47:20.027 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.027 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 3,205B for [area_code] BINARY: 21,447 values, 3,164B raw, 3,164B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:47:20.027 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:47:20.027 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 2,792B for [gender] BINARY: 21,446 values, 2,761B raw, 2,761B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:47:20.027 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:47:20.027 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:47:20.027 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:47:20.028 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 3,206B for [area_code] BINARY: 21,446 values, 3,165B raw, 3,165B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:47:20.028 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:47:20.028 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 2,780B for [gender] BINARY: 21,446 values, 2,749B raw, 2,749B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:47:20.028 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:47:20.029 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.029 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:47:20.029 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 3,223B for [area_code] BINARY: 21,446 values, 3,182B raw, 3,182B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:47:20.029 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:47:20.029 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:47:20.029 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 2,795B for [gender] BINARY: 21,446 values, 2,764B raw, 2,764B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:47:20.029 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.029 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 3,253B for [area_code] BINARY: 21,446 values, 3,212B raw, 3,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:47:20.030 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.030 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:47:20.030 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:47:20.030 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:47:20.030 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:47:20.030 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:47:20.030 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.030 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.031 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:47:20.031 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:47:20.031 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.031 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:47:20.031 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:47:20.032 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:47:20.032 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:47:20.032 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:47:20.033 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.033 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:47:20.033 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:47:20.034 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:47:20.034 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:47:20.770 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
11:47:21.089 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
11:47:21.109 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 16507
11:47:21.110 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 16507
11:47:21.110 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
11:47:21.111 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
11:47:21.111 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(16507); groups with view permissions: Set(); users  with modify permissions: Set(16507); groups with modify permissions: Set()
11:47:21.991 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 7302.
11:47:22.009 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
11:47:22.027 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
11:47:22.030 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
11:47:22.031 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
11:47:22.042 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\16507\AppData\Local\Temp\blockmgr-6047b2f4-c7d2-45c5-99e3-e599e40bd7b0
11:47:22.061 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1988.7 MB
11:47:22.106 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
11:47:22.210 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @6341ms
11:47:22.272 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
11:47:22.291 INFO  [main] org.spark_project.jetty.server.Server - Started @6423ms
11:47:22.340 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
11:47:22.342 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
11:47:22.343 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
11:47:22.343 WARN  [main] org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
11:47:22.352 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@7ee55e70{HTTP/1.1,[http/1.1]}{0.0.0.0:4044}
11:47:22.353 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4044.
11:47:22.375 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632aa1a3{/jobs,null,AVAILABLE,@Spark}
11:47:22.376 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29b732a2{/jobs/json,null,AVAILABLE,@Spark}
11:47:22.376 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51671b08{/jobs/job,null,AVAILABLE,@Spark}
11:47:22.377 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b09fac1{/jobs/job/json,null,AVAILABLE,@Spark}
11:47:22.377 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61019f59{/stages,null,AVAILABLE,@Spark}
11:47:22.377 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/stages/json,null,AVAILABLE,@Spark}
11:47:22.377 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@515f4131{/stages/stage,null,AVAILABLE,@Spark}
11:47:22.379 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14c053c6{/stages/stage/json,null,AVAILABLE,@Spark}
11:47:22.379 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30865a90{/stages/pool,null,AVAILABLE,@Spark}
11:47:22.380 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@777c9dc9{/stages/pool/json,null,AVAILABLE,@Spark}
11:47:22.380 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73e132e0{/storage,null,AVAILABLE,@Spark}
11:47:22.381 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2472c7d8{/storage/json,null,AVAILABLE,@Spark}
11:47:22.381 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@22175d4f{/storage/rdd,null,AVAILABLE,@Spark}
11:47:22.381 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b809711{/storage/rdd/json,null,AVAILABLE,@Spark}
11:47:22.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236ab296{/environment,null,AVAILABLE,@Spark}
11:47:22.382 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63034ed1{/environment/json,null,AVAILABLE,@Spark}
11:47:22.383 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55a8dc49{/executors,null,AVAILABLE,@Spark}
11:47:22.383 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53cdecf6{/executors/json,null,AVAILABLE,@Spark}
11:47:22.384 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b3df3a{/executors/threadDump,null,AVAILABLE,@Spark}
11:47:22.385 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e11ab3d{/executors/threadDump/json,null,AVAILABLE,@Spark}
11:47:22.392 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2392212b{/static,null,AVAILABLE,@Spark}
11:47:22.393 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@553f3b6e{/,null,AVAILABLE,@Spark}
11:47:22.394 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e406694{/api,null,AVAILABLE,@Spark}
11:47:22.396 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6bb7cce7{/jobs/job/kill,null,AVAILABLE,@Spark}
11:47:22.397 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b530eb9{/stages/stage/kill,null,AVAILABLE,@Spark}
11:47:22.409 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.32.1:4044
11:47:22.489 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
11:47:22.526 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7345.
11:47:22.527 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.32.1:7345
11:47:22.530 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
11:47:22.551 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.32.1, 7345, None)
11:47:22.555 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.32.1:7345 with 1988.7 MB RAM, BlockManagerId(driver, 192.168.32.1, 7345, None)
11:47:22.557 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.32.1, 7345, None)
11:47:22.558 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.32.1, 7345, None)
11:47:22.730 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4158debd{/metrics/json,null,AVAILABLE,@Spark}
11:47:24.091 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114715_0015_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/task_20190926114715_0015_m_000000
11:47:24.092 INFO  [Executor task launch worker for task 33] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114715_0015_m_000000_0: Committed
11:47:24.095 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 33). 2658 bytes result sent to driver
11:47:24.096 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 33) in 9262 ms on localhost (executor driver) (1/4)
11:47:24.203 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114715_0015_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/task_20190926114715_0015_m_000002
11:47:24.204 INFO  [Executor task launch worker for task 35] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114715_0015_m_000002_0: Committed
11:47:24.204 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 2.0 in stage 15.0 (TID 35). 2615 bytes result sent to driver
11:47:24.205 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 15.0 (TID 35) in 9371 ms on localhost (executor driver) (2/4)
11:47:24.206 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114715_0015_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/task_20190926114715_0015_m_000003
11:47:24.206 INFO  [Executor task launch worker for task 36] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114715_0015_m_000003_0: Committed
11:47:24.207 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 3.0 in stage 15.0 (TID 36). 2615 bytes result sent to driver
11:47:24.208 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 15.0 (TID 36) in 9373 ms on localhost (executor driver) (3/4)
11:47:24.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:47:24.220 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:47:24.418 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/D:/ideaProject/self/scala_api_AD/target/classes/hive-site.xml
11:47:24.447 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/root/hive/warehouse').
11:47:24.447 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is '/root/hive/warehouse'.
11:47:24.457 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20801cbb{/SQL,null,AVAILABLE,@Spark}
11:47:24.457 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c240cf2{/SQL/json,null,AVAILABLE,@Spark}
11:47:24.458 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@23468512{/SQL/execution,null,AVAILABLE,@Spark}
11:47:24.458 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7e8279e5{/SQL/execution/json,null,AVAILABLE,@Spark}
11:47:24.459 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f2276c9{/static/sql,null,AVAILABLE,@Spark}
11:47:24.960 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
11:47:25.656 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
11:47:25.685 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
11:47:32.576 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114715_0015_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/_temporary/0/task_20190926114715_0015_m_000001
11:47:32.576 INFO  [Executor task launch worker for task 34] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114715_0015_m_000001_0: Committed
11:47:32.577 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 34). 2572 bytes result sent to driver
11:47:32.577 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 34) in 17743 ms on localhost (executor driver) (4/4)
11:47:32.577 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
11:47:32.578 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (insertInto at SparkHelper.scala:35) finished in 17.745 s
11:47:32.578 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 7 finished: insertInto at SparkHelper.scala:35, took 27.843745 s
11:47:43.203 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:47:43.204 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:47:43.204 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:47:43.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:47:43.722 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:47:45.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.142 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:47:45.143 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:47:45.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:47:45.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:47:50.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:47:50.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:47:50.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:47:50.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:47:50.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:47:50.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:47:50.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:47:50.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:47:50.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:47:50.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:47:51.250 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]
11:47:51.250 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]	
11:47:51.893 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00000-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
11:47:51.966 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:47:52.060 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:47:52.168 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:47:52.193 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00001-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
11:47:52.194 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:47:52.292 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00002-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
11:47:52.450 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:47:53.125 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00003-e826cc9f-6804-4d4d-afc5-d0f8ba8b5e1a.c000
11:47:53.359 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:47:54.123 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
11:47:55.411 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
11:48:00.654 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/bdp_day=20190922/part-00000-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00000-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, Status:true
11:48:02.236 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/bdp_day=20190922/part-00001-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00001-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, Status:true
11:48:02.288 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/bdp_day=20190922/part-00002-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00002-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, Status:true
11:48:02.461 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/bdp_day=20190922/part-00003-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190922/part-00003-d41e8a93-1d36-4f19-a173-d3ce9a876760.c000, Status:true
11:48:02.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]
11:48:02.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190922]	
11:48:03.153 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
11:48:03.154 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
11:48:03.154 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190922]
11:48:04.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:48:04.606 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
11:48:05.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:48:05.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:48:07.286 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
11:48:07.505 WARN  [main] hive.log - Updated size to 5782454
11:48:11.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:48:11.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:48:11.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:48:11.487 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:48:11.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:48:11.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:48:11.928 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-47-04_157_4321584823497066098-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
11:48:11.960 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:48:11.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:48:11.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:48:12.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:48:12.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:48:12.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:48:12.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:48:12.496 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.496 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.496 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.496 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.497 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.498 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.498 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.498 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.498 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.498 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.498 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:48:12.522 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:48:12.522 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:48:12.522 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:48:12.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:12.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:12.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:12.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:12.713 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:48:12.737 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:48:12.776 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.776 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:12.777 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:48:12.783 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:48:12.783 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:48:12.783 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:48:12.783 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:48:12.783 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:48:12.783 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:48:12.784 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:48:12.784 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:48:12.785 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:48:12.785 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:48:12.786 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:48:12.786 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:48:12.786 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:48:12.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:48:12.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:48:12.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:48:12.787 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:48:12.788 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:48:12.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:12.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:12.897 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:12.897 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:12.958 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:12.958 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.093 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.093 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.187 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.231 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.231 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.385 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.509 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.509 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.673 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:13.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:13.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:48:13.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:48:13.809 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:13.809 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:14.367 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
11:48:14.370 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
11:48:16.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:16.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:17.219 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.219 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.219 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.220 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.220 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.220 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.220 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.220 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.220 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:17.220 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:48:17.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:48:17.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:48:17.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:48:17.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:48:28.813 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#598),(bdp_day#598 = 20190923)
11:48:28.813 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#591),(release_status#591 = 01)
11:48:28.813 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:48:28.813 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:48:28.814 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:48:28.828 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 312.0 KB, free 1987.8 MB)
11:48:29.535 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.8 MB)
11:48:29.536 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.6 MB)
11:48:29.537 INFO  [main] org.apache.spark.SparkContext - Created broadcast 16 from show at Dw01ReleaseCustomer.scala:66
11:48:29.537 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:48:32.618 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:48:32.619 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 52 (show at Dw01ReleaseCustomer.scala:66)
11:48:32.619 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 8 (show at Dw01ReleaseCustomer.scala:66) with 1 output partitions
11:48:32.619 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (show at Dw01ReleaseCustomer.scala:66)
11:48:32.619 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
11:48:32.619 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
11:48:32.619 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[52] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:48:32.631 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 24.0 KB, free 1987.7 MB)
11:48:32.633 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1987.7 MB)
11:48:32.633 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.32.1:6979 (size: 10.4 KB, free: 1988.6 MB)
11:48:32.634 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1004
11:48:32.634 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[52] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:48:32.634 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 8 tasks
11:48:32.635 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 37, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:48:32.635 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 38, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:48:32.635 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 39, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:48:32.636 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 40, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:48:32.636 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 41, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:48:32.636 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 42, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:48:32.636 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 43, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:48:32.636 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 16.0 (TID 44, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:48:32.637 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 37)
11:48:32.638 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 38)
11:48:32.638 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 40)
11:48:32.638 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 39)
11:48:32.639 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 41)
11:48:32.639 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 42)
11:48:32.640 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 43)
11:48:32.640 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 7.0 in stage 16.0 (TID 44)
11:48:32.646 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:48:32.646 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:48:32.647 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:48:32.649 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:48:32.649 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:48:32.649 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:48:32.650 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:48:32.652 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:48:36.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:48:36.157 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:48:36.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:48:36.558 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:48:36.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
11:48:36.607 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:48:36.607 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:48:36.615 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
11:48:36.622 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.631 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.635 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.635 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.637 INFO  [Executor task launch worker for task 39] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.637 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 42). 1524 bytes result sent to driver
11:48:36.639 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 42) in 4003 ms on localhost (executor driver) (1/8)
11:48:36.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
11:48:36.721 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.722 INFO  [Executor task launch worker for task 37] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.735 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:36.768 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 41). 1524 bytes result sent to driver
11:48:36.768 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 41) in 4132 ms on localhost (executor driver) (2/8)
11:48:36.771 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 38). 1524 bytes result sent to driver
11:48:36.771 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 38) in 4136 ms on localhost (executor driver) (3/8)
11:48:36.774 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 37). 1481 bytes result sent to driver
11:48:36.774 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 37) in 4139 ms on localhost (executor driver) (4/8)
11:48:36.791 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 7.0 in stage 16.0 (TID 44). 1481 bytes result sent to driver
11:48:36.792 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 16.0 (TID 44) in 4156 ms on localhost (executor driver) (5/8)
11:48:36.795 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 43). 1524 bytes result sent to driver
11:48:36.795 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 43) in 4159 ms on localhost (executor driver) (6/8)
11:48:36.798 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 39). 1524 bytes result sent to driver
11:48:36.798 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 39) in 4163 ms on localhost (executor driver) (7/8)
11:48:37.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
11:48:37.224 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
11:48:37.226 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_all_databases	
11:48:37.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
11:48:37.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
11:48:37.294 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:48:37.294 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:48:37.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:48:37.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:48:37.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:48:37.308 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:48:37.331 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
11:48:37.332 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
11:48:37.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dw_release pat=*
11:48:37.376 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=dw_release pat=*	
11:48:37.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
11:48:37.380 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
11:48:41.398 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 330
11:48:43.402 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 40). 1739 bytes result sent to driver
11:48:43.403 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 40) in 10767 ms on localhost (executor driver) (8/8)
11:48:43.403 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
11:48:43.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (show at Dw01ReleaseCustomer.scala:66) finished in 10.769 s
11:48:43.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:48:43.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:48:43.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
11:48:43.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:48:43.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[54] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:48:43.404 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 1987.7 MB)
11:48:43.405 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.7 MB)
11:48:43.406 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.6 MB)
11:48:43.406 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1004
11:48:43.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[54] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0))
11:48:43.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks
11:48:43.407 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 45, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:48:43.407 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 45)
11:48:43.409 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:48:43.409 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:48:43.420 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 45). 11823 bytes result sent to driver
11:48:43.421 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 45) in 13 ms on localhost (executor driver) (1/1)
11:48:43.421 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
11:48:43.421 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (show at Dw01ReleaseCustomer.scala:66) finished in 0.014 s
11:48:43.421 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 8 finished: show at Dw01ReleaseCustomer.scala:66, took 10.803172 s
11:48:43.434 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:48:43.434 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:48:43.434 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:48:45.624 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.625 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.625 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.625 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.626 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.627 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:48:45.627 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.627 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:45.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:48:45.633 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1
11:48:46.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:48:46.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:48:46.711 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:46.711 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:47.345 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:48:47.346 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:48:47.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.649 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:48:47.650 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:48:47.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:48:47.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:48:47.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:48:47.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:48:48.087 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#598),(bdp_day#598 = 20190923)
11:48:48.087 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#591),(release_status#591 = 01)
11:48:48.087 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:48:48.087 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:48:48.087 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:48:48.094 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:48:48.094 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:48:48.167 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
11:48:48.180 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
11:48:48.181 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.5 MB)
11:48:48.182 INFO  [main] org.apache.spark.SparkContext - Created broadcast 19 from insertInto at SparkHelper.scala:35
11:48:48.182 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:48:48.652 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:48:48.652 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 59 (insertInto at SparkHelper.scala:35)
11:48:48.653 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 9 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:48:48.653 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (insertInto at SparkHelper.scala:35)
11:48:48.653 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 18)
11:48:48.653 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 18)
11:48:48.653 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 18 (MapPartitionsRDD[59] at insertInto at SparkHelper.scala:35), which has no missing parents
11:48:48.657 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 24.0 KB, free 1987.4 MB)
11:48:48.659 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1987.4 MB)
11:48:48.660 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.32.1:6979 (size: 10.4 KB, free: 1988.5 MB)
11:48:48.660 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1004
11:48:48.661 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[59] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:48:48.661 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 8 tasks
11:48:48.662 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 46, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:48:48.662 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 47, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:48:48.662 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 18.0 (TID 48, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:48:48.663 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 18.0 (TID 49, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:48:48.663 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 18.0 (TID 50, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:48:48.663 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 18.0 (TID 51, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:48:48.663 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 18.0 (TID 52, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:48:48.663 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 18.0 (TID 53, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:48:48.663 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 18.0 (TID 49)
11:48:48.663 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 4.0 in stage 18.0 (TID 50)
11:48:48.663 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 18.0 (TID 48)
11:48:48.663 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 47)
11:48:48.663 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 46)
11:48:48.663 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 6.0 in stage 18.0 (TID 52)
11:48:48.663 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 5.0 in stage 18.0 (TID 51)
11:48:48.663 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 7.0 in stage 18.0 (TID 53)
11:48:48.668 INFO  [Executor task launch worker for task 52] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:48:48.670 INFO  [Executor task launch worker for task 53] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:48:48.670 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:48:48.670 INFO  [Executor task launch worker for task 50] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:48:48.671 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:48:48.672 INFO  [Executor task launch worker for task 51] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:48:48.673 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:48:48.674 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:48:49.610 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/2f32c381-40d0-46b7-b0b8-073ba4f9b091_resources
11:48:49.635 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/2d9dd682-3df4-4c35-8f5b-10d5d318cffb_resources
11:48:49.635 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/99c74971-9a08-4f0e-9127-a7511220c25d_resources
11:48:49.687 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/2f32c381-40d0-46b7-b0b8-073ba4f9b091
11:48:49.688 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/2d9dd682-3df4-4c35-8f5b-10d5d318cffb
11:48:49.688 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/99c74971-9a08-4f0e-9127-a7511220c25d
11:48:49.728 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/2f32c381-40d0-46b7-b0b8-073ba4f9b091
11:48:49.729 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/2d9dd682-3df4-4c35-8f5b-10d5d318cffb
11:48:49.729 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/99c74971-9a08-4f0e-9127-a7511220c25d
11:48:49.760 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/2f32c381-40d0-46b7-b0b8-073ba4f9b091/_tmp_space.db
11:48:49.761 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/99c74971-9a08-4f0e-9127-a7511220c25d/_tmp_space.db
11:48:49.761 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/2d9dd682-3df4-4c35-8f5b-10d5d318cffb/_tmp_space.db
11:48:49.780 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:48:49.780 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:48:49.781 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:48:50.033 INFO  [Executor task launch worker for task 52] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.033 INFO  [Executor task launch worker for task 46] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.034 INFO  [Executor task launch worker for task 47] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:48:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:48:50.272 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 6.0 in stage 18.0 (TID 52). 1481 bytes result sent to driver
11:48:50.273 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 18.0 (TID 52) in 1610 ms on localhost (executor driver) (1/8)
11:48:50.299 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 46). 1567 bytes result sent to driver
11:48:50.299 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 46) in 1637 ms on localhost (executor driver) (2/8)
11:48:50.303 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 47). 1481 bytes result sent to driver
11:48:50.303 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 47) in 1641 ms on localhost (executor driver) (3/8)
11:48:50.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:48:50.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:48:50.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
11:48:50.338 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:48:50.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:48:50.339 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: global_temp	
11:48:50.357 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:48:50.358 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:48:50.358 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
11:48:50.429 INFO  [Executor task launch worker for task 48] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.429 INFO  [Executor task launch worker for task 50] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.433 INFO  [Executor task launch worker for task 53] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.435 INFO  [Executor task launch worker for task 49] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.440 INFO  [Executor task launch worker for task 51] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:48:50.465 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 7.0 in stage 18.0 (TID 53). 1524 bytes result sent to driver
11:48:50.466 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 18.0 (TID 53) in 1803 ms on localhost (executor driver) (4/8)
11:48:50.469 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 5.0 in stage 18.0 (TID 51). 1524 bytes result sent to driver
11:48:50.469 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 18.0 (TID 51) in 1806 ms on localhost (executor driver) (5/8)
11:48:50.472 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 4.0 in stage 18.0 (TID 50). 1524 bytes result sent to driver
11:48:50.474 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 18.0 (TID 48). 1524 bytes result sent to driver
11:48:51.154 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 18.0 (TID 50) in 2491 ms on localhost (executor driver) (6/8)
11:48:51.154 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 18.0 (TID 48) in 2492 ms on localhost (executor driver) (7/8)
11:48:52.751 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/f533599d-5640-4660-9b41-38d10e9c4736_resources
11:48:52.766 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/1f08ef40-4fca-439f-aeab-8248d3d695bb_resources
11:48:52.847 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/1f08ef40-4fca-439f-aeab-8248d3d695bb
11:48:52.853 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/77fd0607-c19a-4eb0-8232-84182347434c_resources
11:48:52.906 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f533599d-5640-4660-9b41-38d10e9c4736
11:48:52.909 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/1f08ef40-4fca-439f-aeab-8248d3d695bb
11:48:52.947 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/77fd0607-c19a-4eb0-8232-84182347434c
11:48:52.948 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/1f08ef40-4fca-439f-aeab-8248d3d695bb/_tmp_space.db
11:48:52.949 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/f533599d-5640-4660-9b41-38d10e9c4736
11:48:52.951 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/16507/AppData/Local/Temp/16507/77fd0607-c19a-4eb0-8232-84182347434c
11:48:52.952 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:48:52.956 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/f533599d-5640-4660-9b41-38d10e9c4736/_tmp_space.db
11:48:52.958 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/16507/77fd0607-c19a-4eb0-8232-84182347434c/_tmp_space.db
11:48:52.959 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:48:52.961 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is /root/hive/warehouse
11:48:57.757 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 333
11:48:57.760 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 5
11:48:57.764 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.32.1:6979 in memory (size: 2.2 KB, free: 1988.5 MB)
11:48:57.767 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 387
11:48:57.767 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 331
11:48:57.767 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 336
11:48:57.770 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.6 MB)
11:48:57.773 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 272
11:48:57.773 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 335
11:48:57.773 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 274
11:48:57.773 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 278
11:48:57.773 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 337
11:48:57.774 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 192.168.32.1:6979 in memory (size: 10.5 KB, free: 1988.6 MB)
11:48:57.775 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 334
11:48:57.775 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 332
11:48:57.775 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 280
11:48:57.775 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 277
11:48:57.775 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 281
11:48:57.781 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.32.1:6979 in memory (size: 66.0 KB, free: 1988.6 MB)
11:48:57.782 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 270
11:48:57.782 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 6
11:48:57.782 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 276
11:48:57.784 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.7 MB)
11:48:57.786 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 271
11:48:57.786 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 338
11:48:57.786 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 275
11:48:57.786 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 279
11:48:57.787 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.32.1:6979 in memory (size: 10.4 KB, free: 1988.7 MB)
11:48:57.788 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 273
11:48:59.603 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 18.0 (TID 49). 1782 bytes result sent to driver
11:48:59.606 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 18.0 (TID 49) in 10944 ms on localhost (executor driver) (8/8)
11:48:59.606 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
11:48:59.607 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 18 (insertInto at SparkHelper.scala:35) finished in 10.944 s
11:48:59.607 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:48:59.607 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:48:59.607 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 19)
11:48:59.607 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:48:59.607 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[62] at insertInto at SparkHelper.scala:35), which has no missing parents
11:48:59.636 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 180.2 KB, free 1988.2 MB)
11:48:59.639 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 66.1 KB, free 1988.1 MB)
11:48:59.640 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.32.1:6979 (size: 66.1 KB, free: 1988.6 MB)
11:48:59.640 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1004
11:48:59.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 19 (MapPartitionsRDD[62] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:48:59.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 4 tasks
11:48:59.642 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 54, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:48:59.642 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 55, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:48:59.642 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 56, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:48:59.643 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 57, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:48:59.643 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 54)
11:48:59.643 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 55)
11:48:59.643 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 56)
11:48:59.643 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 57)
11:48:59.658 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:48:59.658 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:48:59.658 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:48:59.658 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:48:59.659 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:48:59.659 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:48:59.659 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:48:59.659 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:48:59.719 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:48:59.719 INFO  [Executor task launch worker for task 55] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:48:59.732 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:48:59.733 INFO  [Executor task launch worker for task 57] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:48:59.736 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:48:59.736 INFO  [Executor task launch worker for task 56] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:48:59.740 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:48:59.740 INFO  [Executor task launch worker for task 54] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:49:01.746 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@de229a7
11:49:01.746 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d6fbf66
11:49:01.746 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57644242
11:49:01.746 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@43900df1
11:49:01.746 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:49:01.746 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:49:01.746 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:49:01.746 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114859_0019_m_000001_0/bdp_day=20190923/part-00001-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000
11:49:01.746 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:49:01.746 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114859_0019_m_000003_0/bdp_day=20190923/part-00003-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000
11:49:01.746 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114859_0019_m_000000_0/bdp_day=20190923/part-00000-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000
11:49:01.746 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression set to false
11:49:01.747 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:49:01.747 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:49:01.747 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:49:01.747 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:49:01.747 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:49:01.747 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/_temporary/attempt_20190926114859_0019_m_000002_0/bdp_day=20190923/part-00002-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000
11:49:01.747 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression set to false
11:49:01.747 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression set to false
11:49:01.749 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Validation is off
11:49:01.749 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression set to false
11:49:01.749 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:49:01.749 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:49:01.749 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:49:01.749 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:49:01.749 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:49:01.749 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:49:01.750 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:49:01.750 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:49:01.750 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:49:01.750 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:49:01.750 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:49:01.750 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:49:01.750 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:49:01.750 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:49:01.750 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:49:01.750 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:49:01.750 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Validation is off
11:49:01.750 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Validation is off
11:49:01.750 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Validation is off
11:49:01.750 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:49:01.750 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:49:01.750 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:49:04.300 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@67dc57eb
11:49:04.558 INFO  [Executor task launch worker for task 57] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,668
11:49:04.583 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:04.584 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:04.584 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:04.584 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,447 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:49:04.584 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:49:04.585 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:04.586 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:04.586 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:49:04.586 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 2,782B for [gender] BINARY: 21,447 values, 2,751B raw, 2,751B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:49:04.587 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 3,225B for [area_code] BINARY: 21,447 values, 3,184B raw, 3,184B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:49:04.587 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:49:04.587 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:49:04.588 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.540 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@29ed5b55
11:49:05.541 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5ee04844
11:49:05.541 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@11e19038
11:49:05.556 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:49:05.556 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.557 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:49:05.557 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:49:05.742 INFO  [Executor task launch worker for task 54] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,987
11:49:05.751 INFO  [Executor task launch worker for task 56] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,947
11:49:05.761 INFO  [Executor task launch worker for task 55] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,857
11:49:05.765 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.765 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:05.766 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.766 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,446 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:49:05.766 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:49:05.766 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:05.767 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.767 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:49:05.767 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 2,800B for [gender] BINARY: 21,446 values, 2,769B raw, 2,769B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:49:05.768 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 3,224B for [area_code] BINARY: 21,446 values, 3,183B raw, 3,183B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:49:05.768 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:49:05.768 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:49:05.768 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.768 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:49:05.769 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.769 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:49:05.769 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:49:05.790 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.790 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:05.791 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.791 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,446 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:49:05.791 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:49:05.791 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:05.792 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.792 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:49:05.792 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 2,796B for [gender] BINARY: 21,446 values, 2,765B raw, 2,765B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:49:05.793 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 3,212B for [area_code] BINARY: 21,446 values, 3,171B raw, 3,171B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:49:05.793 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:49:05.793 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:49:05.793 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.793 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:49:05.796 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.796 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:49:05.796 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:49:05.806 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.807 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:05.807 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.807 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,446 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:49:05.807 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:49:05.808 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:49:05.808 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:49:05.809 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:49:05.809 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 2,801B for [gender] BINARY: 21,446 values, 2,770B raw, 2,770B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:49:05.809 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 3,233B for [area_code] BINARY: 21,446 values, 3,192B raw, 3,192B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:49:05.809 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:49:05.810 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:49:05.810 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.810 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:49:05.812 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:49:05.812 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:49:05.812 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:49:08.750 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:49:08.750 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:49:08.750 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
11:49:13.621 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:49:13.621 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:49:13.621 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:49:17.509 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114859_0019_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/task_20190926114859_0019_m_000000
11:49:17.509 INFO  [Executor task launch worker for task 54] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114859_0019_m_000000_0: Committed
11:49:17.510 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 54). 2615 bytes result sent to driver
11:49:17.511 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 54) in 17869 ms on localhost (executor driver) (1/4)
11:49:18.125 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114859_0019_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/task_20190926114859_0019_m_000001
11:49:18.125 INFO  [Executor task launch worker for task 55] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114859_0019_m_000001_0: Committed
11:49:18.127 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114859_0019_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/task_20190926114859_0019_m_000003
11:49:18.127 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 55). 2615 bytes result sent to driver
11:49:18.127 INFO  [Executor task launch worker for task 57] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114859_0019_m_000003_0: Committed
11:49:18.128 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 57). 2615 bytes result sent to driver
11:49:18.128 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 55) in 18486 ms on localhost (executor driver) (2/4)
11:49:18.130 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 57) in 18488 ms on localhost (executor driver) (3/4)
11:49:18.785 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926114859_0019_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/_temporary/0/task_20190926114859_0019_m_000002
11:49:18.785 INFO  [Executor task launch worker for task 56] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926114859_0019_m_000002_0: Committed
11:49:18.786 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 56). 2615 bytes result sent to driver
11:49:18.787 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 56) in 19145 ms on localhost (executor driver) (4/4)
11:49:18.787 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
11:49:18.787 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 19 (insertInto at SparkHelper.scala:35) finished in 19.146 s
11:49:18.787 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 9 finished: insertInto at SparkHelper.scala:35, took 30.135044 s
11:49:27.585 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:49:27.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:49:27.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:49:30.762 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:49:30.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:49:36.583 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:49:36.584 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:36.585 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:36.586 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:49:36.586 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:49:38.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:49:38.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:49:38.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:49:38.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:49:38.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:49:38.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:49:38.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:49:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:49:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:49:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:49:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:49:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:49:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:49:42.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:42.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:42.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:42.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:42.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:42.362 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:43.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:43.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:43.724 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:43.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:43.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:43.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:43.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:43.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:43.959 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.589 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.590 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.591 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:44.592 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:44.592 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:44.849 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:49:44.850 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:49:44.850 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
11:49:38.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:49:47.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:49:47.365 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:49:47.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]
11:49:47.682 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]	
11:49:47.792 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00000-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
11:49:47.793 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:49:47.795 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00001-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
11:49:47.797 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:49:47.840 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00002-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
11:49:47.843 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:49:47.847 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00003-72fbd67b-d450-47fd-993b-1a7e58bbafd6.c000
11:49:47.849 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:49:48.346 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/bdp_day=20190923/part-00000-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00000-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, Status:true
11:49:48.376 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/bdp_day=20190923/part-00001-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00001-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, Status:true
11:49:48.388 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/bdp_day=20190923/part-00002-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00002-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, Status:true
11:49:48.400 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/bdp_day=20190923/part-00003-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190923/part-00003-12cb8828-0328-4d10-bc98-9f2bb868cc91.c000, Status:true
11:49:48.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]
11:49:48.406 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190923]	
11:49:48.439 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
11:49:48.439 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
11:49:48.439 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190923]
11:49:48.580 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
11:49:48.583 WARN  [main] hive.log - Updated size to 5782475
11:49:49.533 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-48-45_633_9214547668343077476-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
11:49:49.537 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:49:49.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:49:49.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:49:49.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:49:49.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:49:49.887 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:49:49.888 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:49:50.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:50.041 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:49:50.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:49:50.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:49:50.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:50.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:50.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:50.193 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:50.329 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:50.330 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:50.337 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:49:50.337 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:49:50.337 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:49:50.338 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:49:50.338 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:49:50.338 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:49:50.338 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:49:50.339 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:49:50.340 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:49:50.341 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:49:50.341 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:49:50.342 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:49:50.342 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:49:50.342 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:49:50.342 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:49:50.343 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:49:50.343 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:49:50.343 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:49:50.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.343 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.374 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.375 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.401 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.402 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.495 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.495 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.528 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.593 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.618 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.713 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:50.745 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:49:50.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:49:50.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:49:50.872 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:50.873 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:51.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:49:51.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:49:51.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:49:51.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:49:51.206 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:49:51.206 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:49:51.206 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:49:51.207 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:49:51.468 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#791),(bdp_day#791 = 20190924)
11:49:51.468 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#784),(release_status#784 = 01)
11:49:51.468 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:49:51.470 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:49:51.470 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:49:51.481 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 312.0 KB, free 1987.8 MB)
11:49:51.492 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.8 MB)
11:49:51.493 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.6 MB)
11:49:51.494 INFO  [main] org.apache.spark.SparkContext - Created broadcast 22 from show at Dw01ReleaseCustomer.scala:66
11:49:51.494 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:49:51.507 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:49:51.508 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 68 (show at Dw01ReleaseCustomer.scala:66)
11:49:51.508 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 10 (show at Dw01ReleaseCustomer.scala:66) with 1 output partitions
11:49:51.508 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (show at Dw01ReleaseCustomer.scala:66)
11:49:51.508 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 20)
11:49:51.508 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 20)
11:49:51.508 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 20 (MapPartitionsRDD[68] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:49:51.517 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 24.0 KB, free 1987.7 MB)
11:49:51.519 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1987.7 MB)
11:49:51.520 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.32.1:6979 (size: 10.5 KB, free: 1988.6 MB)
11:49:51.520 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1004
11:49:51.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[68] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:49:51.521 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 8 tasks
11:49:51.521 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 58, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:49:51.522 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 59, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:49:51.522 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 20.0 (TID 60, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:49:51.522 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 20.0 (TID 61, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:49:51.522 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 20.0 (TID 62, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:49:51.522 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 20.0 (TID 63, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:49:51.522 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 20.0 (TID 64, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:49:51.523 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 20.0 (TID 65, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:49:51.523 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Running task 3.0 in stage 20.0 (TID 61)
11:49:51.523 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 58)
11:49:51.523 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 2.0 in stage 20.0 (TID 60)
11:49:51.523 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 59)
11:49:51.527 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Running task 4.0 in stage 20.0 (TID 62)
11:49:51.528 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Running task 5.0 in stage 20.0 (TID 63)
11:49:51.527 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Running task 6.0 in stage 20.0 (TID 64)
11:49:51.530 INFO  [Executor task launch worker for task 58] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:49:51.531 INFO  [Executor task launch worker for task 60] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:49:51.533 INFO  [Executor task launch worker for task 62] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:49:51.534 INFO  [Executor task launch worker for task 59] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:49:51.536 INFO  [Executor task launch worker for task 63] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:49:51.537 INFO  [Executor task launch worker for task 64] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:49:51.539 INFO  [Executor task launch worker for task 61] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:49:51.546 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Running task 7.0 in stage 20.0 (TID 65)
11:49:51.552 INFO  [Executor task launch worker for task 65] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:49:51.693 INFO  [Executor task launch worker for task 60] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.693 INFO  [Executor task launch worker for task 61] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.694 INFO  [Executor task launch worker for task 62] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.696 INFO  [Executor task launch worker for task 63] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.697 INFO  [Executor task launch worker for task 59] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.697 INFO  [Executor task launch worker for task 58] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.706 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 2.0 in stage 20.0 (TID 60). 1524 bytes result sent to driver
11:49:51.707 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 20.0 (TID 60) in 185 ms on localhost (executor driver) (1/8)
11:49:51.710 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Finished task 4.0 in stage 20.0 (TID 62). 1524 bytes result sent to driver
11:49:51.710 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 20.0 (TID 62) in 188 ms on localhost (executor driver) (2/8)
11:49:51.732 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Finished task 5.0 in stage 20.0 (TID 63). 1567 bytes result sent to driver
11:49:51.733 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 20.0 (TID 63) in 211 ms on localhost (executor driver) (3/8)
11:49:51.736 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 59). 1524 bytes result sent to driver
11:49:51.737 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 59) in 215 ms on localhost (executor driver) (4/8)
11:49:51.740 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 58). 1524 bytes result sent to driver
11:49:51.741 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 58) in 220 ms on localhost (executor driver) (5/8)
11:49:51.796 INFO  [Executor task launch worker for task 64] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.797 INFO  [Executor task launch worker for task 65] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:49:51.806 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Finished task 6.0 in stage 20.0 (TID 64). 1524 bytes result sent to driver
11:49:51.808 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 20.0 (TID 64) in 286 ms on localhost (executor driver) (6/8)
11:49:51.810 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Finished task 7.0 in stage 20.0 (TID 65). 1524 bytes result sent to driver
11:49:51.811 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 20.0 (TID 65) in 289 ms on localhost (executor driver) (7/8)
11:49:51.987 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:49:51.987 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:49:51.987 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:49:52.093 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:49:52.094 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:49:52.094 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:49:52.095 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:49:52.096 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:49:52.097 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:49:52.098 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:49:52.098 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:49:52.099 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:49:52.100 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:49:52.100 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:49:52.101 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:49:52.102 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:49:52.103 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:49:52.143 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:49:52.149 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:49:52.150 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:49:52.151 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:49:52.151 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:49:52.152 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:49:52.152 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:49:52.153 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:49:52.297 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:49:52.298 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:50:02.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:50:02.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:50:02.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:50:02.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:50:02.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:50:02.064 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:50:02.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:02.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:02.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:02.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:02.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:02.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:02.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:02.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:02.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:02.168 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:02.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:02.169 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:02.209 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.210 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.211 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:02.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.212 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:02.253 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.254 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.255 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:02.256 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:02.466 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:50:02.466 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:50:02.466 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:50:02.466 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:50:02.467 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:50:02.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:50:02.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:50:02.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:50:02.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:50:02.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:50:02.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:50:02.855 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:50:04.172 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:50:04.172 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:50:04.173 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:50:04.175 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 04)
11:50:04.175 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
11:50:04.175 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
11:50:04.178 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:50:04.178 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:50:04.178 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:50:04.360 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:50:04.360 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:50:04.360 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:50:04.392 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:50:04.391 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:50:04.391 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:50:06.541 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 218
11:50:07.131 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:6849 in memory (size: 9.6 KB, free: 1988.5 MB)
11:50:07.144 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 393
11:50:07.144 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 397
11:50:07.144 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 388
11:50:07.160 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.6 MB)
11:50:07.169 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 448
11:50:07.178 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.32.1:6849 in memory (size: 64.1 KB, free: 1988.6 MB)
11:50:07.182 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 395
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 7
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 399
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 396
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 394
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 398
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 390
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 392
11:50:07.185 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 391
11:50:07.193 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 192.168.32.1:6849 in memory (size: 9.6 KB, free: 1988.6 MB)
11:50:07.198 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 389
11:50:07.201 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
11:50:07.209 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 213
11:50:07.209 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 215
11:50:07.209 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 219
11:50:07.209 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 214
11:50:07.221 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:6849 in memory (size: 26.3 KB, free: 1988.7 MB)
11:50:07.230 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 220
11:50:07.231 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 216
11:50:07.231 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 217
11:50:08.638 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Finished task 3.0 in stage 20.0 (TID 61). 1782 bytes result sent to driver
11:50:08.639 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 20.0 (TID 61) in 188874 ms on localhost (executor driver) (8/8)
11:50:08.639 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
11:50:08.640 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 20 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 188.875 s
11:50:08.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:50:08.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:50:08.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 21)
11:50:08.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:50:08.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[70] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:50:08.643 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
11:50:08.646 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1988.3 MB)
11:50:08.646 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.32.1:6849 (size: 2.3 KB, free: 1988.7 MB)
11:50:08.647 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1004
11:50:08.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[70] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:50:08.649 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks
11:50:08.650 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 66, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:50:08.650 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 66)
11:50:08.652 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:50:08.652 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:08.655 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 66). 5886 bytes result sent to driver
11:50:08.656 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 66) in 7 ms on localhost (executor driver) (1/1)
11:50:08.656 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
11:50:08.656 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.007 s
11:50:08.656 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 10 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 188.904137 s
11:50:08.666 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:50:08.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:08.669 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:08.690 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.690 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.690 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.690 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:08.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:08.696 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1
11:50:09.480 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:50:09.481 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:50:09.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:09.486 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:09.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:09.501 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:09.516 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.516 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.516 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.516 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.516 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.517 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.517 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.517 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.517 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:09.517 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:09.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:50:09.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:50:09.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:50:09.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:50:09.548 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#435),(bdp_day#435 = 20190924)
11:50:09.548 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#428),(release_status#428 = 06)
11:50:09.549 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:50:09.549 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:50:09.549 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:50:09.554 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:09.554 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:09.703 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
11:50:10.864 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:50:10.865 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.6 MB)
11:50:10.867 INFO  [main] org.apache.spark.SparkContext - Created broadcast 25 from insertInto at SparkHelper.scala:35
11:50:10.868 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:50:12.681 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:50:12.681 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 75 (insertInto at SparkHelper.scala:35)
11:50:12.681 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 11 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:50:12.681 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (insertInto at SparkHelper.scala:35)
11:50:12.681 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 22)
11:50:12.681 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 22)
11:50:12.682 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 22 (MapPartitionsRDD[75] at insertInto at SparkHelper.scala:35), which has no missing parents
11:50:12.686 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 21.7 KB, free 1988.0 MB)
11:50:12.699 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.0 MB)
11:50:12.700 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 192.168.32.1:6849 (size: 9.6 KB, free: 1988.6 MB)
11:50:12.701 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1004
11:50:12.704 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[75] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:50:12.704 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 8 tasks
11:50:12.704 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 67, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:50:12.705 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 68, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:50:12.705 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 69, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:50:12.705 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 70, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:50:12.705 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 71, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:50:12.706 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 72, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:50:12.706 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 73, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:50:12.706 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 74, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:50:12.706 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 67)
11:50:12.710 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 68)
11:50:12.711 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 69)
11:50:12.711 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 71)
11:50:12.712 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 70)
11:50:12.715 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 72)
11:50:12.716 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 73)
11:50:12.719 INFO  [Executor task launch worker for task 68] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:50:12.720 INFO  [Executor task launch worker for task 67] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:50:12.722 INFO  [Executor task launch worker for task 71] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:50:12.725 INFO  [Executor task launch worker for task 72] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:50:12.728 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 74)
11:50:12.729 INFO  [Executor task launch worker for task 70] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:50:12.735 INFO  [Executor task launch worker for task 69] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:50:12.738 INFO  [Executor task launch worker for task 73] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:50:12.744 INFO  [Executor task launch worker for task 74] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:50:13.883 INFO  [Executor task launch worker for task 70] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.883 INFO  [Executor task launch worker for task 74] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.883 INFO  [Executor task launch worker for task 67] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.883 INFO  [Executor task launch worker for task 68] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.883 INFO  [Executor task launch worker for task 71] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.883 INFO  [Executor task launch worker for task 69] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.883 INFO  [Executor task launch worker for task 73] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.887 INFO  [Executor task launch worker for task 72] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:50:13.907 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 71). 1524 bytes result sent to driver
11:50:13.918 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 71) in 1213 ms on localhost (executor driver) (1/8)
11:50:13.928 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 68). 1567 bytes result sent to driver
11:50:13.930 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 72). 1567 bytes result sent to driver
11:50:13.938 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 74). 1610 bytes result sent to driver
11:50:13.939 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 67). 1524 bytes result sent to driver
11:50:13.952 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 72) in 1247 ms on localhost (executor driver) (2/8)
11:50:13.952 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 68) in 1247 ms on localhost (executor driver) (3/8)
11:50:13.952 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 74) in 1246 ms on localhost (executor driver) (4/8)
11:50:13.958 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 67) in 1254 ms on localhost (executor driver) (5/8)
11:50:13.979 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 69). 1567 bytes result sent to driver
11:50:13.979 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 69) in 1274 ms on localhost (executor driver) (6/8)
11:50:13.996 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 73). 1567 bytes result sent to driver
11:50:13.997 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 73) in 1291 ms on localhost (executor driver) (7/8)
11:50:15.017 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8688.716001 ms
11:50:15.017 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8688.645679 ms
11:50:15.017 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8688.670569 ms
11:50:17.089 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 192.168.32.1:6849 in memory (size: 2.3 KB, free: 1988.6 MB)
11:50:17.089 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 505
11:50:17.156 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 70). 1782 bytes result sent to driver
11:50:17.156 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 70) in 4451 ms on localhost (executor driver) (8/8)
11:50:17.156 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
11:50:17.157 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 22 (insertInto at SparkHelper.scala:35) finished in 4.453 s
11:50:17.157 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:50:17.157 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:50:17.157 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 23)
11:50:17.157 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:50:17.157 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[78] at insertInto at SparkHelper.scala:35), which has no missing parents
11:50:17.234 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
11:50:17.235 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
11:50:17.236 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 192.168.32.1:6849 (size: 64.1 KB, free: 1988.6 MB)
11:50:17.236 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1004
11:50:17.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[78] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:50:17.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 4 tasks
11:50:17.237 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 75, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:50:17.237 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 76, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:50:17.237 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 77, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:50:17.237 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 78, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:50:17.238 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 76)
11:50:17.238 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 78)
11:50:17.238 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 75)
11:50:17.238 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 77)
11:50:17.252 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:50:17.253 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:50:17.254 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:50:17.254 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:50:17.254 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:17.254 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:17.256 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:50:17.257 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:50:17.263 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:17.264 INFO  [Executor task launch worker for task 75] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:17.264 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:17.264 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:17.264 INFO  [Executor task launch worker for task 76] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:17.264 INFO  [Executor task launch worker for task 77] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:17.268 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:17.268 INFO  [Executor task launch worker for task 78] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:17.269 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d1169a8
11:50:17.269 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@63c20ee8
11:50:17.269 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7f8c7b5f
11:50:17.269 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:50:17.269 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115017_0023_m_000002_0/bdp_day=20190924/part-00002-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression set to false
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Validation is off
11:50:17.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:50:17.271 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:50:17.271 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115017_0023_m_000000_0/bdp_day=20190924/part-00000-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:50:17.271 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression set to false
11:50:17.271 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:50:17.272 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:50:17.273 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@526d0a1b
11:50:17.274 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:50:17.274 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115017_0023_m_000001_0/bdp_day=20190924/part-00001-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:50:17.274 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:50:17.274 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression set to false
11:50:17.274 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:50:17.274 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:50:17.274 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Validation is off
11:50:17.274 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115017_0023_m_000003_0/bdp_day=20190924/part-00003-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:50:17.274 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:50:17.274 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression set to false
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:50:17.274 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:50:17.274 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Validation is off
11:50:17.274 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:50:17.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:50:17.274 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:50:17.274 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:50:17.276 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Validation is off
11:50:17.276 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:50:17.742 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@13c20c56
11:50:17.743 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@6e5d5c5a
11:50:17.766 INFO  [Executor task launch worker for task 75] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,890
11:50:17.768 INFO  [Executor task launch worker for task 77] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,880
11:50:17.773 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:17.774 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:17.774 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:17.774 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:17.774 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:50:17.774 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:50:17.774 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:17.775 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:17.775 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:17.776 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:17.776 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:17.776 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:17.776 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:50:17.776 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:50:17.776 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:17.777 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.040 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@4b4651c4
11:50:18.040 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@55ff67a3
11:50:18.062 INFO  [Executor task launch worker for task 76] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,616
11:50:18.062 INFO  [Executor task launch worker for task 78] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 323,318
11:50:18.068 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.068 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.069 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:18.069 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.069 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:50:18.069 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:50:18.069 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:18.070 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.071 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.071 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.071 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:18.071 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.071 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:50:18.072 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:50:18.072 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:50:18.072 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
11:50:18.288 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115017_0023_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/task_20190926115017_0023_m_000000
11:50:18.288 INFO  [Executor task launch worker for task 75] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115017_0023_m_000000_0: Committed
11:50:18.289 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 75). 2615 bytes result sent to driver
11:50:18.289 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 75) in 1052 ms on localhost (executor driver) (1/4)
11:50:18.504 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115017_0023_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/task_20190926115017_0023_m_000002
11:50:18.504 INFO  [Executor task launch worker for task 77] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115017_0023_m_000002_0: Committed
11:50:18.505 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 77). 2572 bytes result sent to driver
11:50:18.506 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 77) in 1269 ms on localhost (executor driver) (2/4)
11:50:18.620 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115017_0023_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/task_20190926115017_0023_m_000001
11:50:18.620 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115017_0023_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/_temporary/0/task_20190926115017_0023_m_000003
11:50:18.620 INFO  [Executor task launch worker for task 76] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115017_0023_m_000001_0: Committed
11:50:18.620 INFO  [Executor task launch worker for task 78] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115017_0023_m_000003_0: Committed
11:50:18.621 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 76). 2572 bytes result sent to driver
11:50:18.621 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 78). 2572 bytes result sent to driver
11:50:18.621 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 76) in 1384 ms on localhost (executor driver) (3/4)
11:50:18.621 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 78) in 1384 ms on localhost (executor driver) (4/4)
11:50:18.621 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
11:50:18.622 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (insertInto at SparkHelper.scala:35) finished in 1.385 s
11:50:18.622 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 11 finished: insertInto at SparkHelper.scala:35, took 5.941283 s
11:50:18.639 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.32.1:6979 in memory (size: 66.1 KB, free: 1988.6 MB)
11:50:19.434 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:50:19.434 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:19.434 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:19.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:19.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:19.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.471 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:19.472 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:19.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:19.473 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:19.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:50:19.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:50:19.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:50:19.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:50:19.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:50:19.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:50:19.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:50:19.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:50:19.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:19.491 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:19.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
11:50:19.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
11:50:19.523 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00000-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
11:50:19.524 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:50:19.794 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00001-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
11:50:19.795 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:50:19.800 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00002-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
11:50:19.801 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:50:19.805 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00003-530cf1db-cd71-4399-bf9f-2755c1770ac7.c000
11:50:19.806 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:50:19.944 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/bdp_day=20190924/part-00000-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00000-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, Status:true
11:50:20.045 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/bdp_day=20190924/part-00001-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00001-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, Status:true
11:50:20.557 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/bdp_day=20190924/part-00002-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00002-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, Status:true
11:50:20.732 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/bdp_day=20190924/part-00003-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00003-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000, Status:true
11:50:20.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
11:50:20.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
11:50:20.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_register_users
11:50:20.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_register_users	
11:50:20.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
11:50:20.800 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
11:50:20.802 WARN  [main] hive.log - Updated size to 872108
11:50:22.097 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 448
11:50:27.479 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-08_696_7227064580017421566-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
11:50:27.643 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:50:27.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:50:27.643 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:50:27.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:27.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:27.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:27.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:27.682 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.682 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.682 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.682 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.683 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:27.697 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:50:27.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:50:27.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:50:27.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:27.702 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:27.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:27.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:27.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:27.743 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:50:27.743 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:50:27.743 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:50:27.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:50:27.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:50:27.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:50:27.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:50:27.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:50:27.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:50:27.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:50:27.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:50:27.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:50:27.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:50:27.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:27.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:27.800 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:27.800 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:27.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.812 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:27.813 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:27.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:50:27.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:50:27.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:50:27.814 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:50:27.828 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#539),(bdp_day#539 = 20190925)
11:50:27.828 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#532),(release_status#532 = 06)
11:50:27.829 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:50:27.829 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:50:27.829 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:50:27.838 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
11:50:27.982 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
11:50:27.982 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.5 MB)
11:50:27.984 INFO  [main] org.apache.spark.SparkContext - Created broadcast 28 from show at Dw05ReleaseRegisterUsers.scala:66
11:50:27.984 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:50:27.991 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:50:27.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 84 (show at Dw05ReleaseRegisterUsers.scala:66)
11:50:27.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 12 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:50:27.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (show at Dw05ReleaseRegisterUsers.scala:66)
11:50:27.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
11:50:27.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:50:27.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:50:27.992 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 1987.4 MB)
11:50:27.993 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.4 MB)
11:50:27.994 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 192.168.32.1:6849 (size: 2.2 KB, free: 1988.5 MB)
11:50:27.994 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1004
11:50:27.994 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:50:27.994 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks
11:50:27.995 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 79, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:50:27.995 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 79)
11:50:27.999 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:27.999 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.000 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 79). 1182 bytes result sent to driver
11:50:28.052 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 79) in 57 ms on localhost (executor driver) (1/1)
11:50:28.052 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
11:50:28.052 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.058 s
11:50:28.052 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 12 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.061389 s
11:50:28.055 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:50:28.055 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 13 (show at Dw05ReleaseRegisterUsers.scala:66) with 3 output partitions
11:50:28.055 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (show at Dw05ReleaseRegisterUsers.scala:66)
11:50:28.055 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)
11:50:28.055 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:50:28.055 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:50:28.056 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 1987.4 MB)
11:50:28.057 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.4 MB)
11:50:28.059 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 192.168.32.1:6849 (size: 2.2 KB, free: 1988.5 MB)
11:50:28.059 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1004
11:50:28.059 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:50:28.059 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 3 tasks
11:50:28.060 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 80, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:50:28.060 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 81, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:50:28.061 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 82, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:50:28.061 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 81)
11:50:28.061 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 80)
11:50:28.061 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 82)
11:50:28.062 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:28.062 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:28.062 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:28.062 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.062 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.062 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.063 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 80). 1139 bytes result sent to driver
11:50:28.063 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 81). 1139 bytes result sent to driver
11:50:28.063 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 82). 1139 bytes result sent to driver
11:50:28.063 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 80) in 3 ms on localhost (executor driver) (1/3)
11:50:28.063 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 81) in 3 ms on localhost (executor driver) (2/3)
11:50:28.064 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 82) in 4 ms on localhost (executor driver) (3/3)
11:50:28.064 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool 
11:50:28.064 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.004 s
11:50:28.064 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 13 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.009332 s
11:50:28.069 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:50:28.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:28.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:28.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.086 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:28.089 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-50-28_089_1805083981121378790-1
11:50:28.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:50:28.113 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:50:28.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:28.118 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:28.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:50:28.133 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:28.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:28.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:50:28.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:50:28.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:50:28.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:50:28.163 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#539),(bdp_day#539 = 20190925)
11:50:28.163 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#532),(release_status#532 = 06)
11:50:28.164 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:50:28.164 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:50:28.164 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:50:28.169 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:28.169 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:28.183 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 312.0 KB, free 1987.1 MB)
11:50:28.204 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.1 MB)
11:50:28.205 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 192.168.32.1:6849 (size: 26.3 KB, free: 1988.5 MB)
11:50:28.208 INFO  [main] org.apache.spark.SparkContext - Created broadcast 31 from insertInto at SparkHelper.scala:35
11:50:28.208 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:50:28.248 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:50:28.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 91 (insertInto at SparkHelper.scala:35)
11:50:28.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 14 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:50:28.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (insertInto at SparkHelper.scala:35)
11:50:28.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 28)
11:50:28.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:50:28.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[94] at insertInto at SparkHelper.scala:35), which has no missing parents
11:50:28.276 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 172.9 KB, free 1986.9 MB)
11:50:28.277 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 64.0 KB, free 1986.8 MB)
11:50:28.278 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 192.168.32.1:6849 (size: 64.0 KB, free: 1988.4 MB)
11:50:28.278 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1004
11:50:28.278 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[94] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:50:28.278 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 4 tasks
11:50:28.279 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 83, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:50:28.279 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 29.0 (TID 84, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:50:28.279 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 29.0 (TID 85, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:50:28.279 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 29.0 (TID 86, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:50:28.279 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Running task 1.0 in stage 29.0 (TID 84)
11:50:28.279 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 83)
11:50:28.279 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Running task 3.0 in stage 29.0 (TID 86)
11:50:28.279 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Running task 2.0 in stage 29.0 (TID 85)
11:50:28.290 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:28.290 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.292 INFO  [Executor task launch worker for task 85] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:28.292 INFO  [Executor task launch worker for task 85] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:28.292 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:28.292 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.293 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:28.293 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.293 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:50:28.293 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:50:28.294 INFO  [Executor task launch worker for task 84] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:28.294 INFO  [Executor task launch worker for task 86] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:28.294 INFO  [Executor task launch worker for task 84] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:28.294 INFO  [Executor task launch worker for task 86] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:28.294 INFO  [Executor task launch worker for task 83] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:50:28.294 INFO  [Executor task launch worker for task 83] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:50:28.294 INFO  [Executor task launch worker for task 85] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115028_0029_m_000002_0
11:50:28.295 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Finished task 2.0 in stage 29.0 (TID 85). 2459 bytes result sent to driver
11:50:28.295 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 29.0 (TID 85) in 16 ms on localhost (executor driver) (1/4)
11:50:28.296 INFO  [Executor task launch worker for task 84] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115028_0029_m_000001_0
11:50:28.296 INFO  [Executor task launch worker for task 86] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115028_0029_m_000003_0
11:50:28.296 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Finished task 1.0 in stage 29.0 (TID 84). 2502 bytes result sent to driver
11:50:28.296 INFO  [Executor task launch worker for task 83] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115028_0029_m_000000_0
11:50:28.297 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Finished task 3.0 in stage 29.0 (TID 86). 2502 bytes result sent to driver
11:50:28.297 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 29.0 (TID 84) in 18 ms on localhost (executor driver) (2/4)
11:50:28.297 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 83). 2502 bytes result sent to driver
11:50:28.297 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 29.0 (TID 86) in 18 ms on localhost (executor driver) (3/4)
11:50:28.297 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 83) in 18 ms on localhost (executor driver) (4/4)
11:50:28.298 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool 
11:50:28.298 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (insertInto at SparkHelper.scala:35) finished in 0.020 s
11:50:28.298 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 14 finished: insertInto at SparkHelper.scala:35, took 0.050822 s
11:50:29.650 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:50:29.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:29.651 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:29.663 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:29.664 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:29.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:29.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:29.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:29.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:29.694 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:50:29.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:29.694 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:29.978 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:50:29.979 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:50:29.979 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:50:29.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:29.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:29.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:50:29.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:50:30.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:50:30.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:50:32.543 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:50:41.512 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@1df14fc1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
11:50:41.604 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1220.340717 ms
11:50:41.605 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4040
11:50:41.606 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 2277.089405 ms
11:50:41.609 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1226.433759 ms
11:50:50.163 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:50:53.325 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:50:53.338 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:50:53.342 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:50:55.239 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:50:55.319 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:50:55.440 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:50:55.511 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-d6aab326-e26c-4f77-804b-7e95d07077ed
11:50:59.810 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 398
11:50:59.810 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 397
11:50:59.810 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 388
11:50:59.810 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 389
11:51:00.862 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 192.168.32.1:6979 in memory (size: 10.4 KB, free: 1988.6 MB)
11:51:00.862 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 399
11:51:00.862 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 390
11:51:00.862 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 394
11:51:00.862 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 395
11:51:00.863 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 7
11:51:00.864 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 192.168.32.1:6979 in memory (size: 26.3 KB, free: 1988.7 MB)
11:51:00.911 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 392
11:51:00.911 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 393
11:51:00.911 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 396
11:51:00.911 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 391
11:51:01.437 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Finished task 3.0 in stage 20.0 (TID 61). 1739 bytes result sent to driver
11:51:01.595 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 20.0 (TID 61) in 70073 ms on localhost (executor driver) (8/8)
11:51:01.596 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
11:51:01.734 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 20 (show at Dw01ReleaseCustomer.scala:66) finished in 70.076 s
11:51:01.734 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:01.734 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:01.734 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 21)
11:51:01.734 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:01.734 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[70] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:51:01.860 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
11:51:01.924 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.3 MB)
11:51:01.925 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.7 MB)
11:51:01.949 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1004
11:51:02.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[70] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:02.167 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks
11:51:02.167 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 66, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:02.168 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 66)
11:51:02.235 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:02.235 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:02.292 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 66). 11721 bytes result sent to driver
11:51:02.293 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 66) in 125 ms on localhost (executor driver) (1/1)
11:51:02.293 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
11:51:02.293 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (show at Dw01ReleaseCustomer.scala:66) finished in 0.126 s
11:51:02.293 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 10 finished: show at Dw01ReleaseCustomer.scala:66, took 70.785705 s
11:51:03.040 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:51:03.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:03.450 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:03.702 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:03.770 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:04.600 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1
11:51:05.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:05.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:05.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:05.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:05.579 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:05.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:05.594 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:05.628 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:05.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:05.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:05.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:06.404 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#791),(bdp_day#791 = 20190924)
11:51:06.404 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#784),(release_status#784 = 01)
11:51:06.405 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:06.405 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:51:06.405 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:06.433 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:06.434 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:06.524 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
11:51:06.538 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:51:06.538 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.6 MB)
11:51:06.539 INFO  [main] org.apache.spark.SparkContext - Created broadcast 25 from insertInto at SparkHelper.scala:35
11:51:06.593 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:07.057 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:07.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 75 (insertInto at SparkHelper.scala:35)
11:51:07.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 11 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:07.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (insertInto at SparkHelper.scala:35)
11:51:07.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 22)
11:51:07.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 22)
11:51:07.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 22 (MapPartitionsRDD[75] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:07.063 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 24.0 KB, free 1988.0 MB)
11:51:07.064 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1988.0 MB)
11:51:07.065 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 192.168.32.1:6979 (size: 10.5 KB, free: 1988.6 MB)
11:51:07.065 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1004
11:51:07.065 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[75] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:07.066 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 8 tasks
11:51:07.066 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 67, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:51:07.066 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 68, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:51:07.066 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 69, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:51:07.067 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 70, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:51:07.067 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 71, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:51:07.067 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 72, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:51:07.067 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 73, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:51:07.067 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 74, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:51:07.067 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 67)
11:51:07.159 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 68)
11:51:07.160 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 70)
11:51:07.160 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 69)
11:51:07.162 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 71)
11:51:07.163 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 72)
11:51:07.239 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 73)
11:51:07.247 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 74)
11:51:07.366 INFO  [Executor task launch worker for task 71] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:51:07.366 INFO  [Executor task launch worker for task 72] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:51:07.366 INFO  [Executor task launch worker for task 73] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:51:07.366 INFO  [Executor task launch worker for task 68] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:51:07.366 INFO  [Executor task launch worker for task 70] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:51:07.366 INFO  [Executor task launch worker for task 69] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:51:07.366 INFO  [Executor task launch worker for task 74] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:51:07.366 INFO  [Executor task launch worker for task 67] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:51:07.844 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 312.0 KB, free 1988.4 MB)
11:51:07.844 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
11:51:07.844 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 311.6 KB, free 1988.4 MB)
11:51:07.956 INFO  [Executor task launch worker for task 67] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:07.956 INFO  [Executor task launch worker for task 68] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:07.956 INFO  [Executor task launch worker for task 70] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:07.956 INFO  [Executor task launch worker for task 72] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:07.956 INFO  [Executor task launch worker for task 71] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:07.992 INFO  [Executor task launch worker for task 69] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:08.021 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 72). 1567 bytes result sent to driver
11:51:08.021 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 72) in 954 ms on localhost (executor driver) (1/8)
11:51:08.023 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 69). 1524 bytes result sent to driver
11:51:08.024 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 69) in 958 ms on localhost (executor driver) (2/8)
11:51:08.028 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 71). 1567 bytes result sent to driver
11:51:08.029 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 71) in 962 ms on localhost (executor driver) (3/8)
11:51:08.049 INFO  [Executor task launch worker for task 73] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:08.049 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 68). 1567 bytes result sent to driver
11:51:08.050 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 68) in 984 ms on localhost (executor driver) (4/8)
11:51:08.059 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 73). 1524 bytes result sent to driver
11:51:08.059 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 73) in 992 ms on localhost (executor driver) (5/8)
11:51:08.080 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 67). 1524 bytes result sent to driver
11:51:08.081 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 67) in 1015 ms on localhost (executor driver) (6/8)
11:51:08.085 INFO  [Executor task launch worker for task 74] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
11:51:08.169 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 74). 1481 bytes result sent to driver
11:51:08.216 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 74) in 1149 ms on localhost (executor driver) (7/8)
11:51:10.233 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 192.168.32.1:6979 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:10.233 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 505
11:51:11.292 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
11:51:11.292 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.4 MB)
11:51:11.292 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
11:51:11.296 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.7 MB)
11:51:11.296 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.7 MB)
11:51:11.297 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.7 MB)
11:51:11.384 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw04ReleaseClick.scala:66
11:51:11.384 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw03ReleaseExposure.scala:65
11:51:11.384 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at Dw05ReleaseRegisterUsers.scala:66
11:51:11.530 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:11.530 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:11.530 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:11.728 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 70). 1782 bytes result sent to driver
11:51:11.729 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 70) in 4662 ms on localhost (executor driver) (8/8)
11:51:11.729 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
11:51:11.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 22 (insertInto at SparkHelper.scala:35) finished in 4.663 s
11:51:11.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:11.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:11.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 23)
11:51:11.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:11.729 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[78] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:11.816 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 180.2 KB, free 1987.8 MB)
11:51:11.818 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 66.1 KB, free 1987.7 MB)
11:51:11.819 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 192.168.32.1:6979 (size: 66.1 KB, free: 1988.6 MB)
11:51:11.819 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1004
11:51:11.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[78] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:11.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 4 tasks
11:51:11.820 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 75, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:11.820 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 76, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:11.821 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 77, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:11.821 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 78, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:11.821 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 75)
11:51:11.821 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 76)
11:51:11.821 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 78)
11:51:11.821 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 77)
11:51:11.902 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:11.902 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:11.902 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:11.902 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:11.903 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:11.903 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:11.905 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:11.905 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:11.967 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:11.967 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:11.967 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:11.967 INFO  [Executor task launch worker for task 75] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:11.967 INFO  [Executor task launch worker for task 77] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:11.967 INFO  [Executor task launch worker for task 76] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:11.969 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a54af6
11:51:11.969 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a91d883
11:51:11.970 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@243daf5e
11:51:11.970 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:11.970 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:11.970 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115111_0023_m_000000_0/bdp_day=20190924/part-00000-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000
11:51:11.970 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:11.970 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115111_0023_m_000001_0/bdp_day=20190924/part-00001-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000
11:51:11.970 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115111_0023_m_000002_0/bdp_day=20190924/part-00002-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000
11:51:11.970 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:11.970 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:11.970 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:11.970 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:11.970 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:11.970 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:11.970 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:11.970 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:11.970 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:11.970 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:11.970 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:11.971 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:11.971 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:11.971 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:11.971 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:11.971 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:11.972 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:11.972 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:11.972 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:11.972 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:11.972 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:11.972 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:11.972 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:11.972 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:12.032 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@70832e81
11:51:12.033 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@676c1dee
11:51:12.033 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@de4bec7
11:51:12.033 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:12.034 INFO  [Executor task launch worker for task 78] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:12.035 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@658db0ee
11:51:12.036 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:12.036 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115112_0023_m_000003_0/bdp_day=20190924/part-00003-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:12.036 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:12.065 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@30678064
11:51:12.247 INFO  [Executor task launch worker for task 76] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,547
11:51:12.249 INFO  [Executor task launch worker for task 77] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,815
11:51:12.261 INFO  [Executor task launch worker for task 75] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,597
11:51:12.267 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.267 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.267 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.268 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:12.268 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:12.268 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.269 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.269 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.269 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.269 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:51:12.269 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 2,789B for [gender] BINARY: 21,446 values, 2,758B raw, 2,758B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:51:12.269 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 3,227B for [area_code] BINARY: 21,446 values, 3,186B raw, 3,186B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:51:12.269 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:51:12.270 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:12.270 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:51:12.271 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:12.271 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.271 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.271 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:51:12.272 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.272 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:51:12.272 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:12.272 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.272 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:51:12.272 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 2,784B for [gender] BINARY: 21,446 values, 2,753B raw, 2,753B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:51:12.273 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 3,247B for [area_code] BINARY: 21,446 values, 3,206B raw, 3,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:51:12.273 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:51:12.273 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:51:12.273 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.273 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:51:12.273 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.273 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:51:12.274 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:12.416 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.417 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.419 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.420 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:12.421 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:12.421 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.425 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.428 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,446 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:51:12.429 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,446 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:51:12.430 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 3,213B for [area_code] BINARY: 21,446 values, 3,172B raw, 3,172B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:51:12.431 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:51:12.433 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:51:12.436 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.436 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:51:12.463 INFO  [Executor task launch worker for task 78] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,739,984
11:51:12.481 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.481 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.482 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.482 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 5,447B for [device_type] BINARY: 21,447 values, 5,416B raw, 5,416B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:12.482 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:12.483 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:12.483 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:12.484 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 16,177B for [age] INT32: 21,447 values, 16,138B raw, 16,138B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 41 entries, 164B raw, 41B comp}
11:51:12.484 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 2,786B for [gender] BINARY: 21,447 values, 2,755B raw, 2,755B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
11:51:12.484 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 3,222B for [area_code] BINARY: 21,447 values, 3,181B raw, 3,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
11:51:12.484 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 24,230B for [longitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 479 entries, 7,244B raw, 479B comp}
11:51:12.484 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
11:51:12.485 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.485 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
11:51:12.486 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.486 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:51:12.486 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:12.437 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
11:51:12.504 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
11:51:12.505 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:12.526 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115111_0023_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/task_20190926115111_0023_m_000000
11:51:12.526 INFO  [Executor task launch worker for task 75] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115111_0023_m_000000_0: Committed
11:51:12.527 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 75). 2615 bytes result sent to driver
11:51:12.527 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 75) in 707 ms on localhost (executor driver) (1/4)
11:51:12.528 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115111_0023_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/task_20190926115111_0023_m_000001
11:51:12.528 INFO  [Executor task launch worker for task 76] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115111_0023_m_000001_0: Committed
11:51:12.528 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 76). 2572 bytes result sent to driver
11:51:12.529 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 76) in 709 ms on localhost (executor driver) (2/4)
11:51:12.539 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115112_0023_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/task_20190926115112_0023_m_000003
11:51:12.539 INFO  [Executor task launch worker for task 78] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115112_0023_m_000003_0: Committed
11:51:12.540 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 78). 2572 bytes result sent to driver
11:51:12.540 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 78) in 719 ms on localhost (executor driver) (3/4)
11:51:13.031 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115111_0023_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/_temporary/0/task_20190926115111_0023_m_000002
11:51:13.031 INFO  [Executor task launch worker for task 77] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115111_0023_m_000002_0: Committed
11:51:13.031 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 77). 2572 bytes result sent to driver
11:51:13.032 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 77) in 1211 ms on localhost (executor driver) (4/4)
11:51:13.032 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
11:51:13.032 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (insertInto at SparkHelper.scala:35) finished in 1.212 s
11:51:13.032 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 11 finished: insertInto at SparkHelper.scala:35, took 5.975443 s
11:51:13.587 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:13.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:13.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:13.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:13.792 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:13.807 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:13.808 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:13.808 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:13.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:13.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:13.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:13.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:13.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:13.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:13.971 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:13.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:13.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:13.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:13.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
11:51:13.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
11:51:14.173 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/bdp_day=20190924/part-00000-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00000-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, Status:true
11:51:14.182 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/bdp_day=20190924/part-00001-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00001-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, Status:true
11:51:14.192 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/bdp_day=20190924/part-00002-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00002-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, Status:true
11:51:14.200 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/bdp_day=20190924/part-00003-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_customer/bdp_day=20190924/part-00003-61948ccb-d105-40bf-a131-6f7d5774ac2b.c000, Status:true
11:51:14.205 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]
11:51:14.205 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_customer[20190924]	
11:51:14.216 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:14.216 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:51:14.216 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:14.224 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_customer
11:51:14.225 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_customer	
11:51:14.225 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
11:51:14.257 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
11:51:14.259 WARN  [main] hive.log - Updated size to 5782457
11:51:14.698 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at Dw04ReleaseClick.scala:66)
11:51:14.698 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:14.698 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at Dw03ReleaseExposure.scala:65)
11:51:14.769 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
11:51:14.769 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:51:14.769 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
11:51:14.770 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:14.770 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw04ReleaseClick.scala:66)
11:51:14.770 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at Dw03ReleaseExposure.scala:65)
11:51:14.770 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:51:14.770 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:51:14.770 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
11:51:14.823 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:14.823 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:14.823 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:15.027 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:15.027 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:15.027 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:15.091 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-04_600_5392001693696724851-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
11:51:15.093 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:51:15.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:15.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:15.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:15.098 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:15.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:15.109 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.139 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:15.140 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:15.741 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:51:15.741 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:51:15.742 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:51:15.746 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:51:15.746 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:51:15.746 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:51:15.747 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:7345 (size: 2.2 KB, free: 1988.7 MB)
11:51:15.747 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:7247 (size: 2.2 KB, free: 1988.7 MB)
11:51:15.747 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:51:15.749 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.32.1:7123 (size: 2.2 KB, free: 1988.7 MB)
11:51:15.749 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:51:15.749 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
11:51:15.887 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:15.887 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
11:51:15.887 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:15.942 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:51:15.942 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:51:15.942 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
11:51:16.245 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:16.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:16.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:16.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:16.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:16.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:16.465 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.478 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:16.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:16.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:16.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:16.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:16.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:16.484 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:16.485 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.idcard') as idcard
11:51:16.485 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),'yyyy')as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int))as age
11:51:16.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
11:51:16.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.area_code') as area_code
11:51:16.486 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.longitude') as longitude
11:51:16.487 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.latitude') as latitude
11:51:16.487 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.matter_id') as matter_id
11:51:16.487 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_code') as model_code
11:51:16.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.model_version') as model_version
11:51:16.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.aid') as aid
11:51:16.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:16.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:16.488 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.496 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.500 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.500 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.504 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.505 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.518 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.519 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.522 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.522 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.525 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.527 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.527 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.531 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:16.534 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:16.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:16.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:16.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:16.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:16.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:16.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:16.662 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.662 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.739 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:16.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:16.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:16.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:16.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:16.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:17.059 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#984),(bdp_day#984 = 20190925)
11:51:17.059 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#977),(release_status#977 = 01)
11:51:17.060 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:17.060 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:51:17.060 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:17.087 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
11:51:17.578 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
11:51:17.579 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.5 MB)
11:51:17.580 INFO  [main] org.apache.spark.SparkContext - Created broadcast 28 from show at Dw01ReleaseCustomer.scala:66
11:51:17.580 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:17.609 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:51:17.610 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 84 (show at Dw01ReleaseCustomer.scala:66)
11:51:17.610 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 12 (show at Dw01ReleaseCustomer.scala:66) with 1 output partitions
11:51:17.610 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (show at Dw01ReleaseCustomer.scala:66)
11:51:17.610 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
11:51:17.610 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:17.610 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[86] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:51:17.611 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 1987.4 MB)
11:51:17.612 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.4 MB)
11:51:17.612 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.5 MB)
11:51:17.613 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1004
11:51:17.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[86] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:17.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks
11:51:17.613 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 79, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:17.614 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 79)
11:51:17.632 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.632 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:17.633 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 79). 1225 bytes result sent to driver
11:51:17.633 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 79) in 20 ms on localhost (executor driver) (1/1)
11:51:17.634 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
11:51:17.634 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (show at Dw01ReleaseCustomer.scala:66) finished in 0.021 s
11:51:17.634 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 12 finished: show at Dw01ReleaseCustomer.scala:66, took 0.024969 s
11:51:17.651 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw01ReleaseCustomer.scala:66
11:51:17.651 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 13 (show at Dw01ReleaseCustomer.scala:66) with 3 output partitions
11:51:17.651 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (show at Dw01ReleaseCustomer.scala:66)
11:51:17.651 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)
11:51:17.651 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:17.651 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[86] at show at Dw01ReleaseCustomer.scala:66), which has no missing parents
11:51:17.652 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 1987.4 MB)
11:51:17.653 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.4 MB)
11:51:17.655 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 192.168.32.1:6979 (size: 2.2 KB, free: 1988.5 MB)
11:51:17.655 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1004
11:51:17.655 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[86] at show at Dw01ReleaseCustomer.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:17.656 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 3 tasks
11:51:17.656 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 80, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:17.656 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 81, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:17.657 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 82, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:17.657 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 80)
11:51:17.657 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 82)
11:51:17.657 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 81)
11:51:17.658 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.658 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.658 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.658 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:17.658 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:17.658 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:17.658 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 81). 1139 bytes result sent to driver
11:51:17.658 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 82). 1139 bytes result sent to driver
11:51:17.659 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 81) in 3 ms on localhost (executor driver) (1/3)
11:51:17.659 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 82) in 3 ms on localhost (executor driver) (2/3)
11:51:17.671 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 80). 1139 bytes result sent to driver
11:51:17.671 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 80) in 15 ms on localhost (executor driver) (3/3)
11:51:17.671 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool 
11:51:17.672 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (show at Dw01ReleaseCustomer.scala:66) finished in 0.016 s
11:51:17.672 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 13 finished: show at Dw01ReleaseCustomer.scala:66, took 0.021547 s
11:51:17.695 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_customer
11:51:17.695 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:17.695 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:17.714 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_customer/.hive-staging_hive_2019-09-26_11-51-17_714_4070695588429976848-1
11:51:17.740 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:17.740 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:17.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:17.744 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:17.760 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:17.760 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.774 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:17.775 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:17.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:17.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:17.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:17.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:17.795 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#984),(bdp_day#984 = 20190925)
11:51:17.795 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#977),(release_status#977 = 01)
11:51:17.795 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:17.795 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
11:51:17.795 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:17.818 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:17.818 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:17.827 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 312.0 KB, free 1987.1 MB)
11:51:17.838 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.1 MB)
11:51:17.839 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 192.168.32.1:6979 (size: 26.3 KB, free: 1988.5 MB)
11:51:17.839 INFO  [main] org.apache.spark.SparkContext - Created broadcast 31 from insertInto at SparkHelper.scala:35
11:51:17.839 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:17.867 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:17.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 91 (insertInto at SparkHelper.scala:35)
11:51:17.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 14 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:17.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (insertInto at SparkHelper.scala:35)
11:51:17.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 28)
11:51:17.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:17.868 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[94] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:17.898 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 180.1 KB, free 1986.9 MB)
11:51:17.900 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 66.0 KB, free 1986.8 MB)
11:51:17.901 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 192.168.32.1:6979 (size: 66.0 KB, free: 1988.4 MB)
11:51:17.901 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1004
11:51:17.901 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[94] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:17.901 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 4 tasks
11:51:17.902 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 83, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:17.902 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 29.0 (TID 84, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:17.902 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 29.0 (TID 85, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:17.903 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 29.0 (TID 86, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:17.903 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 83)
11:51:17.903 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Running task 1.0 in stage 29.0 (TID 84)
11:51:17.903 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Running task 2.0 in stage 29.0 (TID 85)
11:51:17.903 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Running task 3.0 in stage 29.0 (TID 86)
11:51:17.903 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:17.903 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:17.904 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:17.960 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.960 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.960 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:17.960 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:17.964 INFO  [Executor task launch worker for task 84] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:17.965 INFO  [Executor task launch worker for task 84] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:17.966 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.966 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:17.966 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:17.967 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:17.970 INFO  [Executor task launch worker for task 86] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:17.970 INFO  [Executor task launch worker for task 83] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:17.971 INFO  [Executor task launch worker for task 83] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:17.971 INFO  [Executor task launch worker for task 86] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:17.972 INFO  [Executor task launch worker for task 84] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115117_0029_m_000001_0
11:51:17.973 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Finished task 1.0 in stage 29.0 (TID 84). 2545 bytes result sent to driver
11:51:17.975 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 29.0 (TID 84) in 73 ms on localhost (executor driver) (1/4)
11:51:17.979 INFO  [Executor task launch worker for task 86] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115117_0029_m_000003_0
11:51:17.980 INFO  [Executor task launch worker for task 83] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115117_0029_m_000000_0
11:51:17.981 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Finished task 3.0 in stage 29.0 (TID 86). 2502 bytes result sent to driver
11:51:17.981 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 83). 2502 bytes result sent to driver
11:51:17.982 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 29.0 (TID 86) in 80 ms on localhost (executor driver) (2/4)
11:51:17.984 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 83) in 81 ms on localhost (executor driver) (3/4)
11:51:18.025 INFO  [Executor task launch worker for task 85] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:18.025 INFO  [Executor task launch worker for task 85] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:18.070 INFO  [Executor task launch worker for task 85] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115118_0029_m_000002_0
11:51:18.071 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Finished task 2.0 in stage 29.0 (TID 85). 2502 bytes result sent to driver
11:51:18.072 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 29.0 (TID 85) in 170 ms on localhost (executor driver) (4/4)
11:51:18.072 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool 
11:51:18.073 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (insertInto at SparkHelper.scala:35) finished in 0.171 s
11:51:18.074 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 14 finished: insertInto at SparkHelper.scala:35, took 0.207118 s
11:51:18.093 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:18.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:18.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:18.120 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:51:18.120 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:51:18.120 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
11:51:18.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:18.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.141 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:18.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.147 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.148 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:18.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:18.148 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:18.163 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:18.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:18.163 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:18.177 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_customer`
11:51:18.177 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:18.177 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:18.180 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:18.180 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:18.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_customer
11:51:18.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_customer	
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.204 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:18.205 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:18.712 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:51:19.015 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:19.015 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:19.015 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:19.060 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 160 ms
11:51:19.060 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 160 ms
11:51:19.060 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 160 ms
11:51:19.178 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6d2260db{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
11:51:19.360 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4041
11:51:19.493 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:51:19.889 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:51:19.921 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:51:19.921 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:51:19.921 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1268 bytes result sent to driver
11:51:19.962 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:51:19.963 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:51:19.980 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 2186 ms on localhost (executor driver) (1/1)
11:51:19.980 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 2186 ms on localhost (executor driver) (1/1)
11:51:19.980 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 2186 ms on localhost (executor driver) (1/1)
11:51:20.016 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:51:20.021 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:51:20.021 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:51:20.022 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-d156a42a-8b52-4458-8fe2-43fbcf1ec946
11:51:20.250 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:51:20.250 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:51:20.250 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
11:51:20.324 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw03ReleaseExposure.scala:65) finished in 3.761 s
11:51:20.324 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw04ReleaseClick.scala:66) finished in 3.762 s
11:51:20.324 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 3.761 s
11:51:20.389 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw04ReleaseClick.scala:66, took 6.172698 s
11:51:20.389 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw03ReleaseExposure.scala:65, took 6.172701 s
11:51:20.389 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 6.172718 s
11:51:20.455 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:51:20.455 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:20.455 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:20.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw03ReleaseExposure.scala:65) with 3 output partitions
11:51:20.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw03ReleaseExposure.scala:65)
11:51:20.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:51:20.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:20.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw04ReleaseClick.scala:66) with 3 output partitions
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw04ReleaseClick.scala:66)
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at Dw05ReleaseRegisterUsers.scala:66) with 3 output partitions
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:20.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:20.459 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:20.461 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:51:20.464 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:51:20.466 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1988.4 MB)
11:51:20.471 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:51:20.476 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:51:20.476 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.4 MB)
11:51:20.481 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:7123 (size: 2.2 KB, free: 1988.7 MB)
11:51:20.482 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:51:20.482 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:7345 (size: 2.2 KB, free: 1988.7 MB)
11:51:20.483 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:51:20.483 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[5] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:20.484 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:51:20.485 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[5] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:20.485 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:51:20.485 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:20.486 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:20.486 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:20.486 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:20.487 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:20.488 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:20.488 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:51:20.489 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:51:20.490 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.32.1:7247 (size: 2.2 KB, free: 1988.7 MB)
11:51:20.493 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
11:51:20.494 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.494 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:20.495 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:20.495 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
11:51:20.496 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
11:51:20.501 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:20.502 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:20.502 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:20.503 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
11:51:20.503 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:51:20.503 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:51:20.506 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.506 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:20.509 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:51:20.509 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
11:51:20.511 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 27 ms on localhost (executor driver) (1/3)
11:51:20.509 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.512 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
11:51:20.511 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:51:20.512 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.513 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:20.513 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
11:51:20.515 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1225 bytes result sent to driver
11:51:20.515 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.516 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
11:51:20.516 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:20.517 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.518 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:20.519 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.519 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1225 bytes result sent to driver
11:51:20.519 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:20.520 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 33 ms on localhost (executor driver) (1/3)
11:51:20.520 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1225 bytes result sent to driver
11:51:20.521 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1182 bytes result sent to driver
11:51:20.522 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.522 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:20.521 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1225 bytes result sent to driver
11:51:20.525 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1182 bytes result sent to driver
11:51:20.526 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 40 ms on localhost (executor driver) (2/3)
11:51:20.527 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 26 ms on localhost (executor driver) (1/3)
11:51:20.527 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:20.528 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:20.528 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 30 ms on localhost (executor driver) (2/3)
11:51:20.529 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 43 ms on localhost (executor driver) (2/3)
11:51:20.529 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1268 bytes result sent to driver
11:51:20.530 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 45 ms on localhost (executor driver) (3/3)
11:51:20.530 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:51:20.531 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw03ReleaseExposure.scala:65) finished in 0.047 s
11:51:20.532 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw03ReleaseExposure.scala:65, took 0.076433 s
11:51:20.529 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 42 ms on localhost (executor driver) (3/3)
11:51:20.533 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:51:20.534 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw04ReleaseClick.scala:66) finished in 0.049 s
11:51:20.535 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw04ReleaseClick.scala:66, took 0.078937 s
11:51:20.535 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 33 ms on localhost (executor driver) (3/3)
11:51:20.536 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
11:51:20.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.040 s
11:51:20.537 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.081769 s
11:51:20.827 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
11:51:20.827 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
11:51:20.828 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:51:20.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:20.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:20.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:20.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:20.854 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:20.854 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:20.927 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.927 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.928 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.928 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.928 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.928 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.929 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.929 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.929 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.929 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:20.929 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.930 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.930 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.931 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.931 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.931 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.931 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:20.931 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.932 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.932 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.933 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.933 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.933 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.933 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:20.934 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:21.858 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-21_824_4998451571219713950-1
11:51:21.858 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-21_776_8369996263539037934-1
11:51:21.858 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-21_776_9030726570109996182-1
11:51:22.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:22.902 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:22.903 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:22.903 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:22.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:22.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:22.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:22.911 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:22.930 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:22.931 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:22.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:22.954 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:22.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:22.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:22.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:22.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:23.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.009 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.010 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:23.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:23.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:23.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:23.016 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:23.024 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
11:51:23.029 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.029 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.030 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.030 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.031 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.031 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.031 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.031 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.032 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.033 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:23.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:23.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:23.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:23.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:23.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:23.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:23.055 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:51:23.056 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 06)
11:51:23.057 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:23.057 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:23.058 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:23.064 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.065 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.066 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.066 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.066 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.067 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.067 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.067 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.067 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:23.068 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:23.069 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:51:23.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:23.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:23.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:23.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:23.072 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 03)
11:51:23.072 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:23.072 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:23.073 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:23.087 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:23.089 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:23.093 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:23.095 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:23.109 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190920)
11:51:23.109 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 04)
11:51:23.110 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:23.110 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:23.110 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:23.130 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 311.6 KB, free 1988.1 MB)
11:51:23.130 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:23.151 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
11:51:23.152 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.6 MB)
11:51:23.154 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
11:51:23.154 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.251967 ms
11:51:23.155 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:23.161 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:23.182 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 312.0 KB, free 1988.1 MB)
11:51:23.276 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:51:23.278 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:51:23.279 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
11:51:23.280 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:23.336 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 311.6 KB, free 1988.1 MB)
11:51:23.337 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:7345 in memory (size: 2.2 KB, free: 1988.7 MB)
11:51:23.354 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
11:51:23.355 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.7 MB)
11:51:23.356 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:51:23.359 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.4 MB)
11:51:23.360 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.7 MB)
11:51:23.361 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:35
11:51:23.362 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:23.434 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:23.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:35)
11:51:23.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:23.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
11:51:23.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
11:51:23.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:23.436 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:23.455 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:23.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:35)
11:51:23.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:23.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
11:51:23.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
11:51:23.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:23.456 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:23.465 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
11:51:23.466 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
11:51:23.466 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
11:51:23.466 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
11:51:23.466 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
11:51:23.469 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:7345 in memory (size: 2.2 KB, free: 1988.7 MB)
11:51:23.469 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
11:51:23.470 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
11:51:23.540 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:23.541 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 9 (insertInto at SparkHelper.scala:35)
11:51:23.541 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:23.541 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:35)
11:51:23.541 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
11:51:23.541 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:23.541 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:23.617 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 167.4 KB, free 1987.9 MB)
11:51:23.619 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 167.3 KB, free 1988.2 MB)
11:51:23.620 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.8 MB)
11:51:23.622 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:7123 (size: 62.1 KB, free: 1988.6 MB)
11:51:23.622 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 61.9 KB, free 1988.1 MB)
11:51:23.622 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
11:51:23.623 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 172.9 KB, free 1987.9 MB)
11:51:23.623 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:23.623 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
11:51:23.624 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:7345 (size: 61.9 KB, free: 1988.6 MB)
11:51:23.624 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:23.624 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
11:51:23.624 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:23.625 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:23.625 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:23.625 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:23.625 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
11:51:23.626 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
11:51:23.626 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
11:51:23.626 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
11:51:23.627 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
11:51:23.627 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:23.628 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:23.628 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:23.628 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:23.629 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
11:51:23.629 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
11:51:23.631 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
11:51:23.631 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 64.0 KB, free 1987.8 MB)
11:51:23.633 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.32.1:7247 (size: 64.0 KB, free: 1988.6 MB)
11:51:23.640 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
11:51:23.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:23.641 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
11:51:23.657 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:23.657 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:23.658 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:23.658 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:23.664 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
11:51:23.669 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
11:51:23.670 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
11:51:23.718 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.718 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.721 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.721 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.741 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.741 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.745 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.745 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.749 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
11:51:23.753 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
11:51:23.759 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.759 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.759 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.759 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.803 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.804 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:23.806 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.806 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:23.820 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.820 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:23.845 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.845 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.854 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.854 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.882 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:23.882 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:23.907 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 110.723643 ms
11:51:23.908 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 111.636632 ms
11:51:23.910 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 73.230676 ms
11:51:24.051 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.663028 ms
11:51:24.053 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.282583 ms
11:51:24.053 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.66372 ms
11:51:24.164 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.164 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.164 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.164 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.166 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.164 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.166 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.166 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.167 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.166 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.167 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.167 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.167 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.168 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.166 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.166 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:24.170 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.170 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.171 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.177 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.175 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.177 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.178 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.179 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:24.202 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.467169 ms
11:51:24.202 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.79428 ms
11:51:24.203 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.341441 ms
11:51:24.297 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.662443 ms
11:51:24.299 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 31.983433 ms
11:51:24.327 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.457106 ms
11:51:24.361 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 93.662889 ms
11:51:24.389 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 89.957602 ms
11:51:24.401 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 37.935437 ms
11:51:24.413 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000001_0
11:51:24.414 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000003_0
11:51:24.415 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000002_0
11:51:24.418 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2588 bytes result sent to driver
11:51:24.420 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2498 bytes result sent to driver
11:51:24.419 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2588 bytes result sent to driver
11:51:24.421 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 764 ms on localhost (executor driver) (1/4)
11:51:24.422 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 794 ms on localhost (executor driver) (1/4)
11:51:24.424 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000000_0
11:51:24.425 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 767 ms on localhost (executor driver) (2/4)
11:51:24.427 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000001_0
11:51:24.427 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000001_0
11:51:24.427 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000002_0
11:51:24.428 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2498 bytes result sent to driver
11:51:24.429 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2455 bytes result sent to driver
11:51:24.429 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2455 bytes result sent to driver
11:51:24.430 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2455 bytes result sent to driver
11:51:24.430 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000000_0
11:51:24.431 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 804 ms on localhost (executor driver) (2/4)
11:51:24.431 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000003_0
11:51:24.432 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000000_0
11:51:24.432 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2545 bytes result sent to driver
11:51:24.433 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2455 bytes result sent to driver
11:51:24.433 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000002_0
11:51:24.435 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115124_0005_m_000003_0
11:51:24.433 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2455 bytes result sent to driver
11:51:24.437 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2455 bytes result sent to driver
11:51:24.439 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2545 bytes result sent to driver
11:51:24.440 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 816 ms on localhost (executor driver) (1/4)
11:51:24.441 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 816 ms on localhost (executor driver) (2/4)
11:51:24.441 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 783 ms on localhost (executor driver) (3/4)
11:51:24.441 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 816 ms on localhost (executor driver) (3/4)
11:51:24.442 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 786 ms on localhost (executor driver) (4/4)
11:51:24.442 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
11:51:24.442 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 814 ms on localhost (executor driver) (3/4)
11:51:24.443 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 819 ms on localhost (executor driver) (4/4)
11:51:24.443 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 817 ms on localhost (executor driver) (4/4)
11:51:24.444 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
11:51:24.444 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
11:51:24.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.818 s
11:51:24.445 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.803 s
11:51:24.446 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 1.012259 s
11:51:24.446 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:35) finished in 0.822 s
11:51:24.447 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.991935 s
11:51:24.448 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:35, took 0.908002 s
11:51:24.804 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
11:51:24.804 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
11:51:24.812 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:7247 in memory (size: 64.0 KB, free: 1988.6 MB)
11:51:24.816 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:7247 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:24.816 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
11:51:24.816 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
11:51:24.816 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
11:51:24.819 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
11:51:24.819 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:51:24.819 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
11:51:24.820 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.7 MB)
11:51:24.821 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
11:51:24.821 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
11:51:24.822 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:7247 in memory (size: 2.2 KB, free: 1988.7 MB)
11:51:24.822 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
11:51:25.220 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
11:51:25.223 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:7345 in memory (size: 61.9 KB, free: 1988.7 MB)
11:51:25.321 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:25.321 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:25.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:25.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:25.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:25.322 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:25.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:25.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:25.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:25.347 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:25.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.367 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.368 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:25.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.369 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:25.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:25.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:25.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:25.372 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:25.384 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
11:51:25.385 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
11:51:25.385 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
11:51:25.385 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
11:51:25.385 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
11:51:25.401 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.32.1:7123 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:25.405 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.32.1:7123 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:25.405 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
11:51:25.405 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
11:51:25.406 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.32.1:7123 in memory (size: 62.1 KB, free: 1988.7 MB)
11:51:25.406 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
11:51:25.407 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
11:51:25.407 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
11:51:25.407 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.32.1:7123 in memory (size: 2.2 KB, free: 1988.7 MB)
11:51:25.411 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
11:51:25.520 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:25.521 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:25.521 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:25.541 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:25.542 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:25.544 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:25.544 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:25.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:25.566 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:25.566 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:25.567 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:25.567 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:25.572 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:25.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:25.604 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:25.620 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:25.621 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:25.704 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
11:51:25.704 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:51:25.704 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
11:51:25.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:25.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:25.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:25.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:25.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:25.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:25.776 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:25.796 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:25.818 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:25.818 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:25.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:25.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:25.837 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:25.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:25.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.875 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.876 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.877 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:25.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:25.951 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:25.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.984 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.984 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.984 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.984 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.984 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.985 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.985 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:25.985 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:26.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:26.051 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:26.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.052 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.068 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:26.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.076 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.083 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.084 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.084 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.084 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.102 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.144 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.169 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:26.170 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.170 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.180 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.181 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.182 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.182 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.182 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.182 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.199 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.213 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.213 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.236 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.237 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.237 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.237 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.237 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.237 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.238 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.238 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.238 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.238 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.252 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:26.252 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:51:26.252 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:26.253 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:26.255 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:26.255 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:26.256 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:26.257 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:26.257 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:26.258 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:26.258 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:26.258 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:26.310 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:26.311 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:26.312 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:26.313 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:26.314 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:26.316 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:26.316 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:26.316 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:26.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.363 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.399 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.407 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.408 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.425 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.459 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.459 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.462 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.463 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.464 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.464 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.465 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.465 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.466 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:26.468 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:26.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:26.469 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:26.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.492 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.499 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.499 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.500 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.500 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.500 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.502 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.502 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.502 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.502 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.503 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:26.506 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:26.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:26.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:26.511 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
11:51:26.512 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 04)
11:51:26.512 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:26.513 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:26.513 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:26.543 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 311.6 KB, free 1988.1 MB)
11:51:26.559 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.559 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:26.565 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:26.565 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:26.565 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:26.569 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
11:51:26.572 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.6 MB)
11:51:26.573 INFO  [main] org.apache.spark.SparkContext - Created broadcast 5 from show at Dw04ReleaseClick.scala:66
11:51:26.573 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:26.587 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#123),(bdp_day#123 = 20190921)
11:51:26.587 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#116),(release_status#116 = 06)
11:51:26.589 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:26.589 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:26.590 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:26.594 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:26.594 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (show at Dw04ReleaseClick.scala:66)
11:51:26.595 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
11:51:26.595 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (show at Dw04ReleaseClick.scala:66)
11:51:26.595 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
11:51:26.595 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:26.595 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[18] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:26.601 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:26.605 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:51:26.614 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:7345 (size: 2.2 KB, free: 1988.6 MB)
11:51:26.615 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
11:51:26.615 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:26.616 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
11:51:26.617 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:26.617 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 8)
11:51:26.619 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.619 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:26.620 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 8). 1139 bytes result sent to driver
11:51:26.626 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 8) in 10 ms on localhost (executor driver) (1/1)
11:51:26.626 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
11:51:26.631 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
11:51:26.632 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 03)
11:51:26.632 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:26.633 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:26.634 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (show at Dw04ReleaseClick.scala:66) finished in 0.013 s
11:51:26.635 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw04ReleaseClick.scala:66, took 0.041860 s
11:51:26.638 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:26.639 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at Dw04ReleaseClick.scala:66) with 3 output partitions
11:51:26.639 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at Dw04ReleaseClick.scala:66)
11:51:26.639 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
11:51:26.640 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:26.639 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 312.0 KB, free 1988.1 MB)
11:51:26.640 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[18] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:26.641 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:26.651 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:26.654 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:51:26.657 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:7345 (size: 2.2 KB, free: 1988.6 MB)
11:51:26.658 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
11:51:26.659 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[18] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:26.659 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 3 tasks
11:51:26.660 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:26.660 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:26.660 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:26.661 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 9)
11:51:26.661 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 10)
11:51:26.661 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 11)
11:51:26.663 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.663 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:26.663 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.663 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.663 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:26.664 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 9). 1182 bytes result sent to driver
11:51:26.669 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 311.6 KB, free 1988.1 MB)
11:51:26.666 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
11:51:26.667 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 10). 1182 bytes result sent to driver
11:51:26.672 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 9) in 12 ms on localhost (executor driver) (1/3)
11:51:26.672 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 11). 1182 bytes result sent to driver
11:51:26.676 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 11) in 16 ms on localhost (executor driver) (2/3)
11:51:26.677 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 10) in 17 ms on localhost (executor driver) (3/3)
11:51:26.677 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
11:51:26.679 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at Dw04ReleaseClick.scala:66) finished in 0.019 s
11:51:26.679 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at Dw04ReleaseClick.scala:66, took 0.040860 s
11:51:26.687 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
11:51:26.687 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:26.688 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:26.699 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:51:26.706 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:51:26.707 INFO  [main] org.apache.spark.SparkContext - Created broadcast 5 from show at Dw05ReleaseRegisterUsers.scala:66
11:51:26.708 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:26.710 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
11:51:26.715 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.6 MB)
11:51:26.717 INFO  [main] org.apache.spark.SparkContext - Created broadcast 5 from show at Dw03ReleaseExposure.scala:65
11:51:26.717 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:26.727 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:51:26.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.733 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.734 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.734 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.734 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.738 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 20 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:26.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:51:26.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:26.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
11:51:26.739 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:26.739 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-26_739_1563212621322941207-1
11:51:26.740 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:26.742 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:26.745 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:51:26.749 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:7247 (size: 2.2 KB, free: 1988.6 MB)
11:51:26.751 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
11:51:26.756 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:26.757 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
11:51:26.759 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:26.760 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 8)
11:51:26.762 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:26.763 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 16 (show at Dw03ReleaseExposure.scala:65)
11:51:26.764 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.765 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:26.765 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
11:51:26.765 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (show at Dw03ReleaseExposure.scala:65)
11:51:26.765 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
11:51:26.765 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:26.766 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:26.766 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 8). 1182 bytes result sent to driver
11:51:26.767 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 8) in 8 ms on localhost (executor driver) (1/1)
11:51:26.767 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
11:51:26.768 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.004 s
11:51:26.769 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:26.771 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.043776 s
11:51:26.774 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:51:26.776 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.32.1:7123 (size: 2.2 KB, free: 1988.6 MB)
11:51:26.776 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:51:26.777 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1004
11:51:26.778 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at Dw05ReleaseRegisterUsers.scala:66) with 3 output partitions
11:51:26.778 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:26.778 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
11:51:26.778 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
11:51:26.778 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
11:51:26.778 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:26.779 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:26.780 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 8)
11:51:26.781 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:26.784 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:26.789 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:51:26.790 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.790 INFO  [Executor task launch worker for task 8] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:26.791 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:7247 (size: 2.2 KB, free: 1988.6 MB)
11:51:26.792 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
11:51:26.793 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:26.793 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 3 tasks
11:51:26.793 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 8). 1225 bytes result sent to driver
11:51:26.794 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:26.794 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:26.795 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:26.797 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 9)
11:51:26.798 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 8) in 19 ms on localhost (executor driver) (1/1)
11:51:26.798 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
11:51:26.798 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 11)
11:51:26.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (show at Dw03ReleaseExposure.scala:65) finished in 0.022 s
11:51:26.798 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 10)
11:51:26.803 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.804 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.799 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.802 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.804 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.804 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
11:51:26.804 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
11:51:26.806 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at Dw03ReleaseExposure.scala:65, took 0.042060 s
11:51:26.805 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:26.807 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 9). 1182 bytes result sent to driver
11:51:26.809 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 11). 1182 bytes result sent to driver
11:51:26.810 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 9) in 15 ms on localhost (executor driver) (1/3)
11:51:26.810 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 11) in 16 ms on localhost (executor driver) (2/3)
11:51:26.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.813 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.814 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 10). 1225 bytes result sent to driver
11:51:26.817 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 10) in 23 ms on localhost (executor driver) (3/3)
11:51:26.818 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
11:51:26.818 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.025 s
11:51:26.819 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:26.819 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.042502 s
11:51:26.821 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 4 (show at Dw03ReleaseExposure.scala:65) with 3 output partitions
11:51:26.821 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (show at Dw03ReleaseExposure.scala:65)
11:51:26.821 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
11:51:26.821 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:26.822 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:26.825 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:26.830 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:51:26.831 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:51:26.833 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.32.1:7123 (size: 2.2 KB, free: 1988.6 MB)
11:51:26.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:26.833 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:26.834 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1004
11:51:26.837 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 9 (MapPartitionsRDD[18] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:26.837 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 3 tasks
11:51:26.841 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:26.842 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:26.846 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:26.847 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 11)
11:51:26.847 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 9)
11:51:26.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.847 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 10)
11:51:26.852 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.856 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.856 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
11:51:26.857 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:26.857 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:26.858 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:26.860 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 11). 1225 bytes result sent to driver
11:51:26.861 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 9). 1182 bytes result sent to driver
11:51:26.862 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 11) in 17 ms on localhost (executor driver) (1/3)
11:51:26.862 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 10). 1182 bytes result sent to driver
11:51:26.864 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 9) in 23 ms on localhost (executor driver) (2/3)
11:51:26.864 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 10) in 22 ms on localhost (executor driver) (3/3)
11:51:26.864 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
11:51:26.866 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (show at Dw03ReleaseExposure.scala:65) finished in 0.025 s
11:51:26.869 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 4 finished: show at Dw03ReleaseExposure.scala:65, took 0.050042 s
11:51:26.870 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.871 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.879 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-26_879_7936158357195999915-1
11:51:26.883 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
11:51:26.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:26.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:26.887 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.888 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.888 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.888 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.888 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:26.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:26.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:26.892 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:26.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.912 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:26.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:26.920 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-26_920_2546449916375762631-1
11:51:26.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.930 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
11:51:26.930 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 04)
11:51:26.930 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:26.931 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:26.932 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:26.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.941 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:26.941 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:26.962 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 311.6 KB, free 1987.7 MB)
11:51:26.967 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.967 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:26.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:26.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:26.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:26.990 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.7 MB)
11:51:26.992 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.6 MB)
11:51:26.993 INFO  [main] org.apache.spark.SparkContext - Created broadcast 8 from insertInto at SparkHelper.scala:35
11:51:26.993 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:27.000 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.006 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.006 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.007 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.007 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.007 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.008 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:27.010 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:27.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:27.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:27.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:27.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:27.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:27.037 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.038 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:27.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:27.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:27.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:27.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:27.044 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#123),(bdp_day#123 = 20190921)
11:51:27.045 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#116),(release_status#116 = 06)
11:51:27.046 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:27.046 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:27.047 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:27.059 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.059 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.079 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#112),(bdp_day#112 = 20190921)
11:51:27.079 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#105),(release_status#105 = 03)
11:51:27.080 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:27.080 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:27.081 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:27.084 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 312.0 KB, free 1987.7 MB)
11:51:27.090 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.091 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.105 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:27.106 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (insertInto at SparkHelper.scala:35)
11:51:27.106 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:27.106 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (insertInto at SparkHelper.scala:35)
11:51:27.106 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
11:51:27.107 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:27.109 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[24] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:27.109 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.7 MB)
11:51:27.114 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:51:27.117 INFO  [main] org.apache.spark.SparkContext - Created broadcast 8 from insertInto at SparkHelper.scala:35
11:51:27.117 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:27.120 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 311.6 KB, free 1987.7 MB)
11:51:27.170 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 167.3 KB, free 1987.5 MB)
11:51:27.173 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 62.0 KB, free 1987.5 MB)
11:51:27.190 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:27.191 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 27 (insertInto at SparkHelper.scala:35)
11:51:27.191 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:27.192 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (insertInto at SparkHelper.scala:35)
11:51:27.192 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
11:51:27.192 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:27.192 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.7 MB)
11:51:27.192 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:27.193 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:7345 (size: 62.0 KB, free: 1988.6 MB)
11:51:27.196 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
11:51:27.196 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 11 (MapPartitionsRDD[24] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:27.197 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 4 tasks
11:51:27.198 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:27.198 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:27.199 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:27.199 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:27.200 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 13)
11:51:27.200 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 14)
11:51:27.202 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 12)
11:51:27.221 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 15)
11:51:27.224 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.224 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:27.233 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.233 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:27.235 INFO  [Executor task launch worker for task 13] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.236 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.240 INFO  [Executor task launch worker for task 14] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.241 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.247 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.248 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:27.252 INFO  [Executor task launch worker for task 15] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.253 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.255 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 172.9 KB, free 1987.5 MB)
11:51:27.257 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.258 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:27.260 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.6 MB)
11:51:27.262 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 64.0 KB, free 1987.5 MB)
11:51:27.264 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.265 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.265 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:7247 (size: 64.0 KB, free: 1988.6 MB)
11:51:27.267 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
11:51:27.268 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 11 (MapPartitionsRDD[30] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:27.268 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 4 tasks
11:51:27.271 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:27.272 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:27.273 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:27.274 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:27.274 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 12)
11:51:27.275 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 15)
11:51:27.276 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 13)
11:51:27.276 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 14)
11:51:27.310 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.310 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:27.323 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.323 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:27.323 INFO  [Executor task launch worker for task 13] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.327 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.327 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.330 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.344 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.344 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:27.351 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.351 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:27.351 INFO  [Executor task launch worker for task 15] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.352 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.354 INFO  [Executor task launch worker for task 14] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.354 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.367 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000000_0
11:51:27.368 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 12). 2545 bytes result sent to driver
11:51:27.370 INFO  [Executor task launch worker for task 13] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000001_0
11:51:27.370 INFO  [Executor task launch worker for task 15] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000003_0
11:51:27.371 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 12) in 100 ms on localhost (executor driver) (1/4)
11:51:27.373 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 15). 2459 bytes result sent to driver
11:51:27.375 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 15) in 102 ms on localhost (executor driver) (2/4)
11:51:27.380 INFO  [main] org.apache.spark.SparkContext - Created broadcast 8 from insertInto at SparkHelper.scala:35
11:51:27.380 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:27.382 INFO  [Executor task launch worker for task 13] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000001_0
11:51:27.384 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 13). 2545 bytes result sent to driver
11:51:27.386 INFO  [Executor task launch worker for task 14] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000002_0
11:51:27.386 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 13) in 114 ms on localhost (executor driver) (3/4)
11:51:27.388 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 14). 2502 bytes result sent to driver
11:51:27.399 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 14) in 127 ms on localhost (executor driver) (4/4)
11:51:27.399 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
11:51:27.401 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (insertInto at SparkHelper.scala:35) finished in 0.096 s
11:51:27.402 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: insertInto at SparkHelper.scala:35, took 0.211614 s
11:51:27.377 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000000_0
11:51:27.377 INFO  [Executor task launch worker for task 15] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000003_0
11:51:27.445 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:27.382 INFO  [Executor task launch worker for task 14] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000002_0
11:51:27.453 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (insertInto at SparkHelper.scala:35)
11:51:27.454 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 5 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:27.455 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (insertInto at SparkHelper.scala:35)
11:51:27.455 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
11:51:27.457 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:27.458 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (MapPartitionsRDD[24] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:27.437 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 13). 2455 bytes result sent to driver
11:51:27.439 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 15). 2455 bytes result sent to driver
11:51:27.446 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 12). 2412 bytes result sent to driver
11:51:27.452 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 14). 2412 bytes result sent to driver
11:51:27.466 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 13) in 268 ms on localhost (executor driver) (1/4)
11:51:27.472 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 15) in 271 ms on localhost (executor driver) (2/4)
11:51:27.474 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 12) in 277 ms on localhost (executor driver) (3/4)
11:51:27.478 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 14) in 280 ms on localhost (executor driver) (4/4)
11:51:27.479 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
11:51:27.484 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (insertInto at SparkHelper.scala:35) finished in 0.287 s
11:51:27.485 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: insertInto at SparkHelper.scala:35, took 0.378688 s
11:51:27.496 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:27.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:27.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:27.510 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:27.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:27.511 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:27.523 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 167.4 KB, free 1987.5 MB)
11:51:27.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:27.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:27.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.560 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.561 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:27.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:27.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:27.586 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.5 MB)
11:51:27.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:27.588 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:27.590 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.32.1:7123 (size: 62.1 KB, free: 1988.6 MB)
11:51:27.591 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:1004
11:51:27.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 11 (MapPartitionsRDD[24] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:27.592 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 4 tasks
11:51:27.593 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:27.594 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:27.594 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:27.594 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:27.595 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 14)
11:51:27.596 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 15)
11:51:27.598 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 12)
11:51:27.598 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 13)
11:51:27.617 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:27.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:27.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:27.618 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.619 INFO  [Executor task launch worker for task 14] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:27.626 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.627 INFO  [Executor task launch worker for task 13] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:27.649 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.650 INFO  [Executor task launch worker for task 15] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:27.653 INFO  [Executor task launch worker for task 15] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.654 INFO  [Executor task launch worker for task 15] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.663 INFO  [Executor task launch worker for task 13] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.663 INFO  [Executor task launch worker for task 13] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.675 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.676 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.677 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.677 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.677 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:27.678 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:27.679 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:27.684 INFO  [Executor task launch worker for task 15] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000003_0
11:51:27.686 INFO  [Executor task launch worker for task 15] org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 15). 2455 bytes result sent to driver
11:51:27.686 INFO  [Executor task launch worker for task 14] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.686 INFO  [Executor task launch worker for task 14] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.714 INFO  [Executor task launch worker for task 13] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000001_0
11:51:27.717 INFO  [Executor task launch worker for task 13] org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 13). 2412 bytes result sent to driver
11:51:27.735 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:27.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:27.735 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:27.742 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:27.742 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:27.747 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:27.747 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:27.751 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 15) in 157 ms on localhost (executor driver) (1/4)
11:51:27.752 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:51:27.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:27.755 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:27.760 INFO  [Executor task launch worker for task 14] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000002_0
11:51:27.762 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 13) in 169 ms on localhost (executor driver) (2/4)
11:51:27.764 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115127_0011_m_000000_0
11:51:27.765 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 12). 2455 bytes result sent to driver
11:51:27.766 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 12) in 173 ms on localhost (executor driver) (3/4)
11:51:27.767 INFO  [Executor task launch worker for task 14] org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 14). 2455 bytes result sent to driver
11:51:27.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:27.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:27.774 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 14) in 180 ms on localhost (executor driver) (4/4)
11:51:27.774 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
11:51:27.775 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (insertInto at SparkHelper.scala:35) finished in 0.183 s
11:51:27.776 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 5 finished: insertInto at SparkHelper.scala:35, took 0.331199 s
11:51:27.779 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
11:51:27.779 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:27.780 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:27.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:27.798 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:27.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:27.835 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:27.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:27.863 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:27.880 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:27.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:27.881 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:27.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:27.907 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:27.907 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:27.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:27.919 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:27.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.919 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:27.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.919 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.920 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.920 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.920 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.920 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.921 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:27.941 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:27.941 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:27.957 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:27.958 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:27.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:27.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:27.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:27.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:27.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:27.995 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.995 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.995 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.996 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.996 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.996 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.996 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:27.996 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:27.998 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:27.999 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:28.004 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.004 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.004 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:28.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.020 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:51:28.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:28.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:28.021 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:28.022 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:28.024 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:28.025 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:28.025 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:28.026 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:28.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:28.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:28.036 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:28.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:28.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:28.047 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.048 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.048 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.049 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.049 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.049 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.049 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.050 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.050 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.050 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:28.061 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:28.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:28.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:28.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:28.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:28.063 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:28.063 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:28.063 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:28.116 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
11:51:28.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:28.117 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:28.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:28.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:28.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:28.121 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:28.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:28.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.125 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:28.146 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.146 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.146 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.146 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:28.156 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:28.170 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.171 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:28.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.174 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.174 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.174 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.175 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:28.175 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:28.175 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:28.175 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:28.176 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.176 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.177 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:28.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:28.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:28.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:28.179 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:28.182 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.183 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:28.200 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:28.201 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:28.201 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:28.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.248 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.249 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:28.256 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:28.256 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:28.256 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:28.256 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:28.256 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:28.257 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:28.257 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:28.257 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:28.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:28.291 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:28.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.295 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:28.312 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:28.332 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.333 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:28.334 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:28.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:28.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:28.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:28.336 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:28.617 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
11:51:28.618 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#227),(bdp_day#227 = 20190922)
11:51:28.619 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 04)
11:51:28.619 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:28.619 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:28.620 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:28.621 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
11:51:28.622 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 03)
11:51:28.622 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:28.623 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:28.619 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#220),(release_status#220 = 06)
11:51:28.624 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:28.625 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:28.625 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:28.626 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:28.660 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 312.0 KB, free 1987.2 MB)
11:51:28.661 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 311.6 KB, free 1987.2 MB)
11:51:28.676 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 311.6 KB, free 1987.2 MB)
11:51:28.680 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
11:51:28.682 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.5 MB)
11:51:28.682 INFO  [main] org.apache.spark.SparkContext - Created broadcast 10 from show at Dw04ReleaseClick.scala:66
11:51:28.686 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.1 MB)
11:51:28.687 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:28.687 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.5 MB)
11:51:28.688 INFO  [main] org.apache.spark.SparkContext - Created broadcast 10 from show at Dw05ReleaseRegisterUsers.scala:66
11:51:28.689 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:28.692 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
11:51:28.694 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.5 MB)
11:51:28.694 INFO  [main] org.apache.spark.SparkContext - Created broadcast 10 from show at Dw03ReleaseExposure.scala:65
11:51:28.695 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:28.792 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:28.801 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:51:28.803 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 29 (show at Dw04ReleaseClick.scala:66)
11:51:28.804 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
11:51:28.804 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (show at Dw04ReleaseClick.scala:66)
11:51:28.804 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
11:51:28.804 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
11:51:28.805 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[29] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:28.808 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:28.810 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 29 (show at Dw03ReleaseExposure.scala:65)
11:51:28.810 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
11:51:28.810 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (show at Dw03ReleaseExposure.scala:65)
11:51:28.810 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
11:51:28.810 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
11:51:28.811 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[29] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:28.819 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 36 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:28.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 6 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:51:28.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 13 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:28.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 12)
11:51:28.820 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 12)
11:51:28.821 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 12 (MapPartitionsRDD[36] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:28.891 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 17.6 KB, free 1987.1 MB)
11:51:28.891 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 17.6 KB, free 1987.1 MB)
11:51:28.896 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.1 MB)
11:51:28.899 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 21.7 KB, free 1987.1 MB)
11:51:28.899 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:7345 (size: 7.5 KB, free: 1988.5 MB)
11:51:28.900 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
11:51:28.902 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.1 MB)
11:51:28.904 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[29] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:28.904 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:7123 (size: 7.5 KB, free: 1988.5 MB)
11:51:28.904 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.1 MB)
11:51:28.904 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 8 tasks
11:51:28.905 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
11:51:28.906 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on 192.168.32.1:7247 (size: 9.6 KB, free: 1988.5 MB)
11:51:28.907 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:1004
11:51:28.908 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[29] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:28.908 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 8 tasks
11:51:28.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[36] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:28.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 8 tasks
11:51:28.914 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:28.914 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 17, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:28.914 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 18, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:28.915 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 19, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:28.915 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 20, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:28.915 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 21, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:28.916 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 22, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:28.916 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:28.916 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 23, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:28.916 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 17, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:28.917 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 18, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:28.917 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 19, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:28.917 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 20, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:28.917 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 21, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:28.918 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 22, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:28.918 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 23, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:28.918 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 17)
11:51:28.919 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 18)
11:51:28.919 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)
11:51:28.919 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 19)
11:51:28.931 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 17)
11:51:28.932 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 18)
11:51:28.935 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)
11:51:28.935 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 21)
11:51:28.935 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 19)
11:51:28.940 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 16, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:28.940 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 17, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:28.943 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 18, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:28.935 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 20)
11:51:28.944 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 19, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:28.946 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 12.0 (TID 20, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:28.945 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 21)
11:51:28.947 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 22)
11:51:28.947 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 12.0 (TID 21, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:28.947 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 23)
11:51:28.947 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 12.0 (TID 22, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:28.948 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 12.0 (TID 23, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:28.950 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 20)
11:51:28.950 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 16)
11:51:28.951 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 18)
11:51:28.952 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 17)
11:51:28.957 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 23)
11:51:28.963 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 19)
11:51:28.966 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 22)
11:51:28.987 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Running task 5.0 in stage 12.0 (TID 21)
11:51:29.025 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Running task 7.0 in stage 12.0 (TID 23)
11:51:29.036 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Running task 6.0 in stage 12.0 (TID 22)
11:51:29.036 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Running task 4.0 in stage 12.0 (TID 20)
11:51:29.086 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:51:29.086 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:51:29.086 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:51:29.086 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:51:29.086 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:51:29.086 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:51:29.087 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:51:29.086 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:51:29.089 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:51:29.090 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:51:29.087 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:51:29.090 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:51:29.087 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:51:29.090 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:51:29.088 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:51:29.088 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:51:29.140 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 27.140368 ms
11:51:29.145 INFO  [Executor task launch worker for task 22] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:51:29.151 INFO  [Executor task launch worker for task 23] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:51:29.152 INFO  [Executor task launch worker for task 21] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:51:29.152 INFO  [Executor task launch worker for task 19] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:51:29.153 INFO  [Executor task launch worker for task 18] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:51:29.155 INFO  [Executor task launch worker for task 17] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:51:29.158 INFO  [Executor task launch worker for task 20] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:51:29.160 INFO  [Executor task launch worker for task 16] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:51:33.028 INFO  [Executor task launch worker for task 17] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.036 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.042 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.067 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:33.067 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:33.067 INFO  [Executor task launch worker for task 17] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:33.131 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.134 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.169 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.170 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.173 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:33.215 INFO  [Executor task launch worker for task 23] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:33.265 INFO  [Executor task launch worker for task 22] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.296 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.384 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 1414 bytes result sent to driver
11:51:33.385 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 23). 1414 bytes result sent to driver
11:51:33.385 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 22). 1414 bytes result sent to driver
11:51:33.389 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 23) in 4473 ms on localhost (executor driver) (1/8)
11:51:33.389 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 4480 ms on localhost (executor driver) (2/8)
11:51:33.391 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 22) in 4476 ms on localhost (executor driver) (3/8)
11:51:33.447 INFO  [Executor task launch worker for task 17] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.450 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 18). 1414 bytes result sent to driver
11:51:33.455 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 18) in 4539 ms on localhost (executor driver) (1/8)
11:51:33.455 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 20). 1414 bytes result sent to driver
11:51:33.457 INFO  [Executor task launch worker for task 16] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.457 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 20) in 4540 ms on localhost (executor driver) (2/8)
11:51:33.485 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 1610 bytes result sent to driver
11:51:33.490 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:33.491 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 4558 ms on localhost (executor driver) (1/8)
11:51:33.528 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.541 INFO  [Executor task launch worker for task 20] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.552 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:33.564 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 22). 1414 bytes result sent to driver
11:51:33.566 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 20). 1371 bytes result sent to driver
11:51:33.570 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 20) in 4655 ms on localhost (executor driver) (4/8)
11:51:33.576 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 22) in 4659 ms on localhost (executor driver) (3/8)
11:51:33.578 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 23). 1371 bytes result sent to driver
11:51:33.583 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 17). 1524 bytes result sent to driver
11:51:33.586 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 23) in 4668 ms on localhost (executor driver) (4/8)
11:51:33.588 INFO  [Executor task launch worker for task 22] org.apache.spark.executor.Executor - Finished task 6.0 in stage 12.0 (TID 22). 1524 bytes result sent to driver
11:51:33.590 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 17) in 4650 ms on localhost (executor driver) (2/8)
11:51:33.592 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 12.0 (TID 22) in 4645 ms on localhost (executor driver) (3/8)
11:51:33.595 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:33.633 INFO  [Executor task launch worker for task 23] org.apache.spark.executor.Executor - Finished task 7.0 in stage 12.0 (TID 23). 1610 bytes result sent to driver
11:51:33.638 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 12.0 (TID 23) in 4691 ms on localhost (executor driver) (4/8)
11:51:33.643 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 21). 1524 bytes result sent to driver
11:51:33.653 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 21) in 4707 ms on localhost (executor driver) (5/8)
11:51:33.722 INFO  [Executor task launch worker for task 21] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.784 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 18). 1524 bytes result sent to driver
11:51:33.785 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 21). 1371 bytes result sent to driver
11:51:33.788 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 18) in 4848 ms on localhost (executor driver) (6/8)
11:51:33.790 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 21) in 4873 ms on localhost (executor driver) (5/8)
11:51:33.795 INFO  [Executor task launch worker for task 16] org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 16). 1414 bytes result sent to driver
11:51:33.801 INFO  [Executor task launch worker for task 20] org.apache.spark.executor.Executor - Finished task 4.0 in stage 12.0 (TID 20). 1524 bytes result sent to driver
11:51:33.802 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 12.0 (TID 20) in 4856 ms on localhost (executor driver) (7/8)
11:51:33.801 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 16) in 4890 ms on localhost (executor driver) (6/8)
11:51:33.808 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 17). 1371 bytes result sent to driver
11:51:33.813 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 17) in 4897 ms on localhost (executor driver) (7/8)
11:51:33.840 INFO  [Executor task launch worker for task 17] org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 17). 1414 bytes result sent to driver
11:51:33.847 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 17) in 4932 ms on localhost (executor driver) (5/8)
11:51:33.867 INFO  [Executor task launch worker for task 21] org.apache.spark.executor.Executor - Finished task 5.0 in stage 12.0 (TID 21). 1457 bytes result sent to driver
11:51:33.869 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 12.0 (TID 21) in 4954 ms on localhost (executor driver) (6/8)
11:51:33.903 INFO  [Executor task launch worker for task 18] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:33.918 INFO  [Executor task launch worker for task 18] org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 18). 1371 bytes result sent to driver
11:51:33.921 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 18) in 5007 ms on localhost (executor driver) (7/8)
11:51:33.958 INFO  [Executor task launch worker for task 19] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:35.335 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 19). 1543 bytes result sent to driver
11:51:35.336 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 19) in 6422 ms on localhost (executor driver) (8/8)
11:51:35.337 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
11:51:35.337 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (show at Dw04ReleaseClick.scala:66) finished in 6.428 s
11:51:35.338 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:35.338 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:35.343 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
11:51:35.344 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:35.390 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 19). 1586 bytes result sent to driver
11:51:35.391 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 19) in 6474 ms on localhost (executor driver) (8/8)
11:51:35.391 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
11:51:35.392 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (show at Dw03ReleaseExposure.scala:65) finished in 6.481 s
11:51:35.392 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:35.393 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:35.393 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
11:51:35.393 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:35.433 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[31] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:35.434 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[31] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:35.441 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 1987.1 MB)
11:51:35.441 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 1987.1 MB)
11:51:35.442 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.1 MB)
11:51:35.444 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:7345 (size: 2.3 KB, free: 1988.5 MB)
11:51:35.444 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
11:51:35.444 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.1 MB)
11:51:35.445 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:35.445 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks
11:51:35.446 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 24, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:35.446 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 24)
11:51:35.447 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:7123 (size: 2.3 KB, free: 1988.5 MB)
11:51:35.447 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
11:51:35.448 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
11:51:35.448 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks
11:51:35.449 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 24, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:35.449 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 24)
11:51:35.455 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:35.455 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:35.455 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:35.455 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 2 ms
11:51:35.570 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 24). 4706 bytes result sent to driver
11:51:35.570 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 24) in 122 ms on localhost (executor driver) (1/1)
11:51:35.570 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
11:51:35.571 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (show at Dw03ReleaseExposure.scala:65) finished in 0.123 s
11:51:35.571 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: show at Dw03ReleaseExposure.scala:65, took 6.762632 s
11:51:35.616 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
11:51:35.617 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:35.618 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:35.621 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 24). 4694 bytes result sent to driver
11:51:35.622 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 24) in 176 ms on localhost (executor driver) (1/1)
11:51:35.622 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
11:51:35.622 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (show at Dw04ReleaseClick.scala:66) finished in 0.177 s
11:51:35.623 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: show at Dw04ReleaseClick.scala:66, took 6.831167 s
11:51:35.638 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
11:51:35.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:35.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.639 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:35.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.640 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:35.646 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1
11:51:35.709 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.710 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.711 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.711 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.712 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.713 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.714 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:35.727 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1
11:51:35.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:35.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:35.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:35.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:35.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:35.763 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:35.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:35.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:35.770 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:35.771 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:35.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:35.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.793 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:35.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.795 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:35.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:35.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:35.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:35.797 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:35.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:35.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:35.819 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:35.819 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:35.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:35.820 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:35.839 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
11:51:35.839 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 03)
11:51:35.840 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:35.840 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:35.840 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:35.848 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:35.848 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:35.861 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 311.6 KB, free 1986.8 MB)
11:51:35.879 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#205),(bdp_day#205 = 20190922)
11:51:35.880 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#198),(release_status#198 = 04)
11:51:35.880 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:35.880 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:35.881 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:35.886 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:35.887 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:35.888 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1986.8 MB)
11:51:35.890 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.5 MB)
11:51:35.892 INFO  [main] org.apache.spark.SparkContext - Created broadcast 13 from insertInto at SparkHelper.scala:35
11:51:35.893 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:35.900 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 311.6 KB, free 1986.8 MB)
11:51:35.919 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1986.8 MB)
11:51:35.920 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.5 MB)
11:51:35.921 INFO  [main] org.apache.spark.SparkContext - Created broadcast 13 from insertInto at SparkHelper.scala:35
11:51:35.922 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:35.945 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:35.946 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 35 (insertInto at SparkHelper.scala:35)
11:51:35.946 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 7 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:35.946 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (insertInto at SparkHelper.scala:35)
11:51:35.946 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
11:51:35.946 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
11:51:35.947 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (MapPartitionsRDD[35] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:35.949 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 17.6 KB, free 1986.8 MB)
11:51:35.953 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1986.8 MB)
11:51:35.954 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:7123 (size: 7.5 KB, free: 1988.5 MB)
11:51:35.955 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1004
11:51:35.955 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[35] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:35.955 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 8 tasks
11:51:35.956 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 25, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:35.956 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 26, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:35.957 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 27, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:35.957 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 28, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:35.957 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 29, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:35.957 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 30, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:35.958 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 31, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:35.958 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 32, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:35.958 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 28)
11:51:35.958 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 26)
11:51:35.958 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 27)
11:51:35.958 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 25)
11:51:35.958 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 29)
11:51:35.958 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 30)
11:51:35.958 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 31)
11:51:35.958 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 32)
11:51:35.961 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:51:35.963 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:51:35.963 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:51:35.963 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:51:35.964 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:51:35.964 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:51:35.965 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:51:35.967 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:51:35.979 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:35.979 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:35.979 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:35.982 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:35.983 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 35 (insertInto at SparkHelper.scala:35)
11:51:35.984 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 7 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:35.984 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (insertInto at SparkHelper.scala:35)
11:51:35.984 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
11:51:35.984 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
11:51:35.984 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (MapPartitionsRDD[35] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:35.986 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 17.6 KB, free 1986.8 MB)
11:51:35.990 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1986.8 MB)
11:51:35.991 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:7345 (size: 7.5 KB, free: 1988.5 MB)
11:51:35.991 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1004
11:51:35.991 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[35] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:35.992 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 8 tasks
11:51:35.992 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 25, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:35.993 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 26, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:35.993 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 27, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:35.993 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 28, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:35.993 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 29, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:35.993 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 30, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:35.994 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 31, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:35.994 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 32, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:35.994 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 26)
11:51:35.994 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 25)
11:51:35.994 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 27)
11:51:35.994 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 30)
11:51:35.994 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 29)
11:51:35.994 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 28)
11:51:35.994 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 31)
11:51:35.994 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 32)
11:51:35.997 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:51:35.998 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:51:35.999 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:51:36.000 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:51:36.002 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:51:36.002 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:51:36.003 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:51:36.006 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:51:36.014 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:36.030 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 25). 1328 bytes result sent to driver
11:51:36.033 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 25) in 77 ms on localhost (executor driver) (1/8)
11:51:36.037 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 32). 1371 bytes result sent to driver
11:51:36.038 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 32) in 80 ms on localhost (executor driver) (2/8)
11:51:36.040 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:36.041 INFO  [Executor task launch worker for task 26] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:36.045 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:36.051 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:36.074 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.091 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.112 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.115 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 30). 1414 bytes result sent to driver
11:51:36.126 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 27). 1371 bytes result sent to driver
11:51:36.136 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 30) in 179 ms on localhost (executor driver) (3/8)
11:51:36.136 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 27) in 180 ms on localhost (executor driver) (4/8)
11:51:36.141 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.142 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.148 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.171 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.187 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 29). 1371 bytes result sent to driver
11:51:36.189 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 29) in 232 ms on localhost (executor driver) (5/8)
11:51:36.193 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 26). 1371 bytes result sent to driver
11:51:36.198 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 31). 1371 bytes result sent to driver
11:51:36.200 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 31) in 243 ms on localhost (executor driver) (6/8)
11:51:36.201 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 26) in 245 ms on localhost (executor driver) (7/8)
11:51:36.231 INFO  [Executor task launch worker for task 26] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:36.233 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 27). 1371 bytes result sent to driver
11:51:36.234 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 27) in 241 ms on localhost (executor driver) (1/8)
11:51:36.244 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 29). 1371 bytes result sent to driver
11:51:36.250 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 31). 1371 bytes result sent to driver
11:51:36.250 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 29) in 257 ms on localhost (executor driver) (2/8)
11:51:36.251 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 31) in 257 ms on localhost (executor driver) (3/8)
11:51:36.254 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 26). 1371 bytes result sent to driver
11:51:36.254 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 26) in 262 ms on localhost (executor driver) (4/8)
11:51:36.258 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 25). 1371 bytes result sent to driver
11:51:36.258 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 25) in 266 ms on localhost (executor driver) (5/8)
11:51:36.270 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 32). 1371 bytes result sent to driver
11:51:36.274 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 32) in 280 ms on localhost (executor driver) (6/8)
11:51:36.277 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 30). 1371 bytes result sent to driver
11:51:36.277 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 30) in 284 ms on localhost (executor driver) (7/8)
11:51:37.253 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
11:51:37.258 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:7345 in memory (size: 62.0 KB, free: 1988.6 MB)
11:51:37.263 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:37.265 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 267
11:51:37.274 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:7345 in memory (size: 2.3 KB, free: 1988.6 MB)
11:51:37.278 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:7345 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:37.282 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:7345 in memory (size: 7.5 KB, free: 1988.6 MB)
11:51:37.283 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
11:51:37.284 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:7345 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:37.286 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:37.287 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:37.306 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
11:51:37.342 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
11:51:37.344 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:7123 in memory (size: 62.1 KB, free: 1988.6 MB)
11:51:37.346 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:7123 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:37.348 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
11:51:37.350 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:7123 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:37.371 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:7123 in memory (size: 2.3 KB, free: 1988.6 MB)
11:51:37.374 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:7123 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:37.369 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 92
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 179
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 176
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 122
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 123
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 183
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 119
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 178
11:51:37.376 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 121
11:51:37.377 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 3
11:51:37.377 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 177
11:51:37.377 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 212
11:51:37.377 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 184
11:51:37.377 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
11:51:37.380 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:7123 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:37.381 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.5 MB)
11:51:37.382 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:7123 in memory (size: 7.5 KB, free: 1988.6 MB)
11:51:37.382 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 182
11:51:37.383 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 187
11:51:37.383 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 125
11:51:37.383 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 185
11:51:37.383 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 180
11:51:37.383 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 175
11:51:37.384 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 2
11:51:37.387 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.32.1:7247 in memory (size: 2.2 KB, free: 1988.5 MB)
11:51:37.388 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:7123 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:37.390 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 124
11:51:37.392 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.32.1:7247 in memory (size: 2.2 KB, free: 1988.6 MB)
11:51:37.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 126
11:51:37.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 186
11:51:37.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 120
11:51:37.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 118
11:51:37.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 181
11:51:37.394 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.6 MB)
11:51:37.396 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on 192.168.32.1:7247 in memory (size: 64.0 KB, free: 1988.6 MB)
11:51:37.397 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 83
11:51:37.397 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 93
11:51:37.397 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 87
11:51:37.397 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 88
11:51:37.397 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 86
11:51:37.397 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 91
11:51:37.399 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 1
11:51:37.399 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 90
11:51:37.402 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.7 MB)
11:51:37.402 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 82
11:51:37.403 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 89
11:51:37.403 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 85
11:51:37.403 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 84
11:51:37.440 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 28). 1586 bytes result sent to driver
11:51:37.440 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 28) in 1447 ms on localhost (executor driver) (8/8)
11:51:37.441 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
11:51:37.441 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (insertInto at SparkHelper.scala:35) finished in 1.449 s
11:51:37.442 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:37.442 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:37.442 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
11:51:37.442 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:37.442 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[37] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:37.492 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 167.4 KB, free 1987.9 MB)
11:51:37.495 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.8 MB)
11:51:37.501 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:7345 (size: 62.1 KB, free: 1988.6 MB)
11:51:37.504 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1004
11:51:37.504 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:37.505 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 4 tasks
11:51:37.506 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 33, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:37.506 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 34, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:37.507 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 15.0 (TID 35, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:37.507 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 15.0 (TID 36, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:37.507 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 33)
11:51:37.508 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 34)
11:51:37.508 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 2.0 in stage 15.0 (TID 35)
11:51:37.543 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:37.547 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
11:51:37.555 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:37.555 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:37.564 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:37.565 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:37.585 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 3.0 in stage 15.0 (TID 36)
11:51:37.600 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:37.600 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:37.670 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 28). 1586 bytes result sent to driver
11:51:37.673 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 28) in 1716 ms on localhost (executor driver) (8/8)
11:51:37.673 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
11:51:37.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (insertInto at SparkHelper.scala:35) finished in 1.718 s
11:51:37.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:37.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:37.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
11:51:37.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:37.674 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[37] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:37.709 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 167.4 KB, free 1987.9 MB)
11:51:37.714 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.8 MB)
11:51:37.719 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:7123 (size: 62.1 KB, free: 1988.6 MB)
11:51:37.728 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1004
11:51:37.735 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 15 (MapPartitionsRDD[37] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:37.735 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 4 tasks
11:51:37.736 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 33, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:37.736 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 34, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:37.736 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 15.0 (TID 35, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:37.736 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 15.0 (TID 36, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:37.737 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 34)
11:51:37.737 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 2.0 in stage 15.0 (TID 35)
11:51:37.737 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 3.0 in stage 15.0 (TID 36)
11:51:37.737 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 33)
11:51:37.756 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:37.756 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:37.774 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:37.775 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:37.788 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:37.788 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:37.827 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:37.827 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:37.852 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:37.852 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:37.919 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:37.925 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:37.926 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:37.946 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:37.963 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:37.974 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:38.014 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:38.014 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:38.038 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:38.039 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:38.049 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:38.049 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:38.051 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:38.052 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:38.074 INFO  [Executor task launch worker for task 19] org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 19). 1782 bytes result sent to driver
11:51:38.076 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 19) in 9133 ms on localhost (executor driver) (8/8)
11:51:38.076 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
11:51:38.077 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 12 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 9.142 s
11:51:38.077 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:38.078 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:38.079 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 13)
11:51:38.079 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:38.082 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 13 (MapPartitionsRDD[38] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:38.086 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
11:51:38.087 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1988.3 MB)
11:51:38.089 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on 192.168.32.1:7247 (size: 2.3 KB, free: 1988.7 MB)
11:51:38.089 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:1004
11:51:38.090 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[38] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:38.090 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 1 tasks
11:51:38.091 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 24, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:38.091 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 24)
11:51:38.095 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:38.095 INFO  [Executor task launch worker for task 24] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:38.110 INFO  [Executor task launch worker for task 24] org.apache.spark.executor.Executor - Finished task 0.0 in stage 13.0 (TID 24). 5906 bytes result sent to driver
11:51:38.111 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 13.0 (TID 24) in 21 ms on localhost (executor driver) (1/1)
11:51:38.111 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 13.0, whose tasks have all completed, from pool 
11:51:38.111 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 13 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.021 s
11:51:38.111 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 6 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 9.309985 s
11:51:38.128 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:51:38.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:38.130 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:38.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.150 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.150 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.150 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.150 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.151 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.151 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.151 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.151 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:38.156 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1
11:51:38.185 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:38.186 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:38.190 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:38.191 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:38.208 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:38.208 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:38.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:38.228 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:38.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:38.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:38.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:38.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:38.270 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#227),(bdp_day#227 = 20190922)
11:51:38.270 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#220),(release_status#220 = 06)
11:51:38.270 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:38.270 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:38.270 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:38.279 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:38.279 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:38.292 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
11:51:38.306 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:51:38.307 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:51:38.309 INFO  [main] org.apache.spark.SparkContext - Created broadcast 13 from insertInto at SparkHelper.scala:35
11:51:38.309 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:38.338 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6db94c74
11:51:38.338 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52515176
11:51:38.339 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@58a82e9e
11:51:38.339 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10a4e0a6
11:51:38.340 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d9152f1
11:51:38.340 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4be53d25
11:51:38.340 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@297927fc
11:51:38.340 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2822271c
11:51:38.354 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:38.356 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 43 (insertInto at SparkHelper.scala:35)
11:51:38.356 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 7 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:38.357 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 15 (insertInto at SparkHelper.scala:35)
11:51:38.357 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 14)
11:51:38.357 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 14)
11:51:38.357 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 14 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:38.360 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 21.7 KB, free 1988.0 MB)
11:51:38.362 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.0 MB)
11:51:38.363 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on 192.168.32.1:7247 (size: 9.6 KB, free: 1988.6 MB)
11:51:38.364 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:1004
11:51:38.364 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[43] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:38.364 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 14.0 with 8 tasks
11:51:38.365 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 14.0 (TID 25, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:38.366 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 14.0 (TID 26, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:38.367 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 14.0 (TID 27, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:38.367 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 14.0 (TID 28, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:38.368 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 14.0 (TID 29, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:38.368 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 14.0 (TID 30, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:38.368 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 14.0 (TID 31, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:38.369 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 14.0 (TID 32, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:38.369 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Running task 0.0 in stage 14.0 (TID 25)
11:51:38.370 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Running task 1.0 in stage 14.0 (TID 26)
11:51:38.370 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Running task 4.0 in stage 14.0 (TID 29)
11:51:38.370 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Running task 2.0 in stage 14.0 (TID 27)
11:51:38.371 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Running task 3.0 in stage 14.0 (TID 28)
11:51:38.371 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Running task 6.0 in stage 14.0 (TID 31)
11:51:38.372 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Running task 7.0 in stage 14.0 (TID 32)
11:51:38.373 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Running task 5.0 in stage 14.0 (TID 30)
11:51:38.376 INFO  [Executor task launch worker for task 25] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 0-5984398, partition values: [20190922]
11:51:38.378 INFO  [Executor task launch worker for task 28] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 17953194-23937592, partition values: [20190922]
11:51:38.379 INFO  [Executor task launch worker for task 26] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 5984398-11968796, partition values: [20190922]
11:51:38.380 INFO  [Executor task launch worker for task 31] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 35906388-41890786, partition values: [20190922]
11:51:38.381 INFO  [Executor task launch worker for task 29] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 23937592-29921990, partition values: [20190922]
11:51:38.383 INFO  [Executor task launch worker for task 27] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 11968796-17953194, partition values: [20190922]
11:51:38.385 INFO  [Executor task launch worker for task 32] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 41890786-43680880, partition values: [20190922]
11:51:38.387 INFO  [Executor task launch worker for task 30] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190922/11111111111, range: 29921990-35906388, partition values: [20190922]
11:51:38.395 INFO  [Executor task launch worker for task 25] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.402 INFO  [Executor task launch worker for task 26] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.410 INFO  [Executor task launch worker for task 25] org.apache.spark.executor.Executor - Finished task 0.0 in stage 14.0 (TID 25). 1567 bytes result sent to driver
11:51:38.412 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 14.0 (TID 25) in 47 ms on localhost (executor driver) (1/8)
11:51:38.414 INFO  [Executor task launch worker for task 27] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.413 INFO  [Executor task launch worker for task 30] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.422 INFO  [Executor task launch worker for task 26] org.apache.spark.executor.Executor - Finished task 1.0 in stage 14.0 (TID 26). 1524 bytes result sent to driver
11:51:38.424 INFO  [Executor task launch worker for task 29] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.424 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 14.0 (TID 26) in 59 ms on localhost (executor driver) (2/8)
11:51:38.433 INFO  [Executor task launch worker for task 34] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
11:51:38.438 INFO  [Executor task launch worker for task 36] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
11:51:38.458 INFO  [Executor task launch worker for task 28] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.458 INFO  [Executor task launch worker for task 31] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.467 INFO  [Executor task launch worker for task 32] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:38.476 INFO  [Executor task launch worker for task 27] org.apache.spark.executor.Executor - Finished task 2.0 in stage 14.0 (TID 27). 1567 bytes result sent to driver
11:51:38.477 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 14.0 (TID 27) in 110 ms on localhost (executor driver) (3/8)
11:51:38.479 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.479 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.479 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115137_0015_m_000002_0/bdp_day=20190922/part-00002-f07cb429-2a69-4e06-b95e-2197b8126198.c000
11:51:38.479 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.479 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115137_0015_m_000001_0/bdp_day=20190922/part-00001-f07cb429-2a69-4e06-b95e-2197b8126198.c000
11:51:38.481 INFO  [Executor task launch worker for task 30] org.apache.spark.executor.Executor - Finished task 5.0 in stage 14.0 (TID 30). 1524 bytes result sent to driver
11:51:38.479 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115137_0015_m_000000_0/bdp_day=20190922/part-00000-f07cb429-2a69-4e06-b95e-2197b8126198.c000
11:51:38.482 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 14.0 (TID 30) in 114 ms on localhost (executor driver) (4/8)
11:51:38.479 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.482 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115137_0015_m_000003_0/bdp_day=20190922/part-00003-f07cb429-2a69-4e06-b95e-2197b8126198.c000
11:51:38.485 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.485 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.485 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.486 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115138_0015_m_000002_0/bdp_day=20190922/part-00002-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000
11:51:38.486 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115138_0015_m_000003_0/bdp_day=20190922/part-00003-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000
11:51:38.486 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115138_0015_m_000001_0/bdp_day=20190922/part-00001-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000
11:51:38.486 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:38.486 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115137_0015_m_000000_0/bdp_day=20190922/part-00000-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000
11:51:38.488 INFO  [Executor task launch worker for task 29] org.apache.spark.executor.Executor - Finished task 4.0 in stage 14.0 (TID 29). 1567 bytes result sent to driver
11:51:38.488 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 14.0 (TID 29) in 120 ms on localhost (executor driver) (5/8)
11:51:38.507 INFO  [Executor task launch worker for task 31] org.apache.spark.executor.Executor - Finished task 6.0 in stage 14.0 (TID 31). 1524 bytes result sent to driver
11:51:38.507 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 14.0 (TID 31) in 139 ms on localhost (executor driver) (6/8)
11:51:38.510 INFO  [Executor task launch worker for task 32] org.apache.spark.executor.Executor - Finished task 7.0 in stage 14.0 (TID 32). 1524 bytes result sent to driver
11:51:38.510 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 14.0 (TID 32) in 142 ms on localhost (executor driver) (7/8)
11:51:38.554 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:38.554 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.554 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.554 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.554 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.554 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.554 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.554 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.554 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:38.597 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.597 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.597 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.597 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.597 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:38.598 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:38.598 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:38.598 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:38.598 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:39.014 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2091cbe6
11:51:39.014 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1c2dd009
11:51:39.014 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@675c44b
11:51:39.015 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@68ac4a82
11:51:39.015 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@188a2701
11:51:39.015 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@37438516
11:51:39.015 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@4203b5f1
11:51:39.015 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@201947bc
11:51:39.439 INFO  [Executor task launch worker for task 36] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,839
11:51:39.445 INFO  [Executor task launch worker for task 35] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,238
11:51:39.449 INFO  [Executor task launch worker for task 34] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,711
11:51:39.454 INFO  [Executor task launch worker for task 36] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,356
11:51:39.458 INFO  [Executor task launch worker for task 35] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,271
11:51:39.471 INFO  [Executor task launch worker for task 33] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,283
11:51:39.490 INFO  [Executor task launch worker for task 33] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,296
11:51:39.491 INFO  [Executor task launch worker for task 34] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,556
11:51:39.672 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.673 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.673 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.673 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.673 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.673 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.673 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.673 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 290,389B for [release_session] BINARY: 10,752 values, 290,312B raw, 290,312B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.684 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.685 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.685 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.686 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.686 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:39.686 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:39.686 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.686 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.686 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.687 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.686 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:39.687 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 107,571B for [device_num] BINARY: 10,752 values, 107,528B raw, 107,528B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.687 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.688 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.688 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.687 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 2,750B for [device_type] BINARY: 10,753 values, 2,719B raw, 2,719B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:39.689 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,345 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:39.689 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 2,751B for [device_type] BINARY: 10,752 values, 2,720B raw, 2,720B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:39.689 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.689 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.690 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.689 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:39.690 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:39.691 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:39.691 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.691 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:39.692 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.692 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:39.692 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:39.690 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:39.692 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:39.692 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:39.690 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 4,105B for [sources] BINARY: 10,752 values, 4,063B raw, 4,063B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:39.692 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:39.692 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.692 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:39.693 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:39.692 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:39.693 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.693 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:39.693 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,407 entries, 19,256B raw, 2,407B comp}
11:51:39.693 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:39.694 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 16,206B for [ct] INT64: 10,752 values, 16,159B raw, 16,159B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,401 entries, 19,208B raw, 2,401B comp}
11:51:39.694 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.694 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:39.694 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:39.694 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,416 entries, 19,328B raw, 2,416B comp}
11:51:39.695 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,428 entries, 19,424B raw, 2,428B comp}
11:51:40.552 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115137_0015_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/task_20190926115137_0015_m_000002
11:51:40.552 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115138_0015_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/task_20190926115138_0015_m_000002
11:51:40.552 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115137_0015_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/task_20190926115137_0015_m_000000
11:51:40.553 INFO  [Executor task launch worker for task 35] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115137_0015_m_000002_0: Committed
11:51:40.553 INFO  [Executor task launch worker for task 33] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115137_0015_m_000000_0: Committed
11:51:40.553 INFO  [Executor task launch worker for task 35] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115138_0015_m_000002_0: Committed
11:51:40.554 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 33). 2482 bytes result sent to driver
11:51:40.554 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 2.0 in stage 15.0 (TID 35). 2525 bytes result sent to driver
11:51:40.554 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 2.0 in stage 15.0 (TID 35). 2525 bytes result sent to driver
11:51:40.555 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 15.0 (TID 35) in 2819 ms on localhost (executor driver) (1/4)
11:51:40.556 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 33) in 2820 ms on localhost (executor driver) (2/4)
11:51:40.556 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 15.0 (TID 35) in 3049 ms on localhost (executor driver) (1/4)
11:51:40.790 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115137_0015_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/task_20190926115137_0015_m_000001
11:51:40.790 INFO  [Executor task launch worker for task 34] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115137_0015_m_000001_0: Committed
11:51:40.790 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 34). 2482 bytes result sent to driver
11:51:40.790 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115137_0015_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/task_20190926115137_0015_m_000000
11:51:40.790 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115137_0015_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/_temporary/0/task_20190926115137_0015_m_000003
11:51:40.791 INFO  [Executor task launch worker for task 33] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115137_0015_m_000000_0: Committed
11:51:40.791 INFO  [Executor task launch worker for task 36] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115137_0015_m_000003_0: Committed
11:51:40.792 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 34) in 3285 ms on localhost (executor driver) (2/4)
11:51:40.792 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 33). 2482 bytes result sent to driver
11:51:40.792 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 3.0 in stage 15.0 (TID 36). 2482 bytes result sent to driver
11:51:40.793 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 33) in 3287 ms on localhost (executor driver) (3/4)
11:51:40.793 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 15.0 (TID 36) in 3286 ms on localhost (executor driver) (4/4)
11:51:40.793 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115138_0015_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/task_20190926115138_0015_m_000003
11:51:40.793 INFO  [Executor task launch worker for task 36] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115138_0015_m_000003_0: Committed
11:51:40.793 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
11:51:40.793 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (insertInto at SparkHelper.scala:35) finished in 3.288 s
11:51:40.793 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 3.0 in stage 15.0 (TID 36). 2482 bytes result sent to driver
11:51:40.793 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 7 finished: insertInto at SparkHelper.scala:35, took 4.810535 s
11:51:40.794 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 15.0 (TID 36) in 3058 ms on localhost (executor driver) (3/4)
11:51:40.825 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:40.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:40.825 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:40.837 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115138_0015_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/_temporary/0/task_20190926115138_0015_m_000001
11:51:40.837 INFO  [Executor task launch worker for task 34] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115138_0015_m_000001_0: Committed
11:51:40.838 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 34). 2482 bytes result sent to driver
11:51:40.838 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 34) in 3102 ms on localhost (executor driver) (4/4)
11:51:40.839 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
11:51:40.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (insertInto at SparkHelper.scala:35) finished in 3.102 s
11:51:40.840 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 7 finished: insertInto at SparkHelper.scala:35, took 4.894712 s
11:51:40.852 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:40.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:40.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:40.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:40.901 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:40.908 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:40.908 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:40.909 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:40.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:40.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:40.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:40.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:40.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:40.929 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:40.930 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:40.930 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:40.930 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:40.930 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:40.964 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:40.964 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:40.981 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]
11:51:40.981 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]	
11:51:40.982 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.982 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.982 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:40.983 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:40.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:40.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:41.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:41.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:41.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:41.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:41.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:41.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:41.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:41.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:41.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:41.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:41.014 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00000-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
11:51:41.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]
11:51:41.028 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]	
11:51:41.068 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.081 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00001-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
11:51:41.083 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.086 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00002-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
11:51:41.087 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.090 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00003-876f2164-ec43-46ba-8bb7-a707ed771d98.c000
11:51:41.091 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.111 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00000-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
11:51:41.118 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.121 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00001-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
11:51:41.122 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.124 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00002-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
11:51:41.125 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.127 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00003-1563b5f0-1478-431c-9ca4-3ee44c53081d.c000
11:51:41.129 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:41.165 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
11:51:41.193 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
11:51:41.249 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/bdp_day=20190922/part-00000-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00000-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, Status:true
11:51:41.250 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/bdp_day=20190922/part-00000-f07cb429-2a69-4e06-b95e-2197b8126198.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00000-f07cb429-2a69-4e06-b95e-2197b8126198.c000, Status:true
11:51:41.320 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/bdp_day=20190922/part-00001-f07cb429-2a69-4e06-b95e-2197b8126198.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00001-f07cb429-2a69-4e06-b95e-2197b8126198.c000, Status:true
11:51:41.321 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/bdp_day=20190922/part-00001-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00001-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, Status:true
11:51:41.349 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/bdp_day=20190922/part-00002-f07cb429-2a69-4e06-b95e-2197b8126198.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00002-f07cb429-2a69-4e06-b95e-2197b8126198.c000, Status:true
11:51:41.351 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/bdp_day=20190922/part-00002-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00002-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, Status:true
11:51:41.363 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/bdp_day=20190922/part-00003-f07cb429-2a69-4e06-b95e-2197b8126198.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190922/part-00003-f07cb429-2a69-4e06-b95e-2197b8126198.c000, Status:true
11:51:41.368 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]
11:51:41.369 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190922]	
11:51:41.369 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/bdp_day=20190922/part-00003-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190922/part-00003-cf616204-0775-4460-9a1c-20a5ae17ab0c.c000, Status:true
11:51:41.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]
11:51:41.377 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190922]	
11:51:41.420 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_click
11:51:41.420 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_click	
11:51:41.420 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190922]
11:51:41.430 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_exposure
11:51:41.430 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_exposure	
11:51:41.430 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190922]
11:51:41.468 WARN  [main] hive.log - Updating partition stats fast for: dw_release_click
11:51:41.473 WARN  [main] hive.log - Updated size to 1765301
11:51:41.482 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
11:51:41.487 WARN  [main] hive.log - Updated size to 2286731
11:51:41.794 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on 192.168.32.1:7247 in memory (size: 2.3 KB, free: 1988.6 MB)
11:51:41.795 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 269
11:51:41.938 INFO  [Executor task launch worker for task 28] org.apache.spark.executor.Executor - Finished task 3.0 in stage 14.0 (TID 28). 1739 bytes result sent to driver
11:51:41.939 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 14.0 (TID 28) in 3572 ms on localhost (executor driver) (8/8)
11:51:41.939 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 14.0, whose tasks have all completed, from pool 
11:51:41.939 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 14 (insertInto at SparkHelper.scala:35) finished in 3.574 s
11:51:41.940 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:41.940 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:41.940 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 15)
11:51:41.940 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:41.940 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 15 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:41.963 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
11:51:41.964 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
11:51:41.965 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on 192.168.32.1:7247 (size: 64.1 KB, free: 1988.6 MB)
11:51:41.965 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:1004
11:51:41.966 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 15 (MapPartitionsRDD[46] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:41.966 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 15.0 with 4 tasks
11:51:41.966 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 15.0 (TID 33, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:41.966 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 15.0 (TID 34, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:41.966 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 15.0 (TID 35, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:41.966 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 15.0 (TID 36, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:41.967 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Running task 1.0 in stage 15.0 (TID 34)
11:51:41.967 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Running task 0.0 in stage 15.0 (TID 33)
11:51:41.967 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Running task 2.0 in stage 15.0 (TID 35)
11:51:41.967 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Running task 3.0 in stage 15.0 (TID 36)
11:51:41.984 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:41.984 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:41.984 INFO  [Executor task launch worker for task 33] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:41.984 INFO  [Executor task launch worker for task 34] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:41.986 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:41.986 INFO  [Executor task launch worker for task 35] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:41.988 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:41.989 INFO  [Executor task launch worker for task 36] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:42.016 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:42.017 INFO  [Executor task launch worker for task 34] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:42.018 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:42.018 INFO  [Executor task launch worker for task 35] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:42.019 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:42.019 INFO  [Executor task launch worker for task 33] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:42.022 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:42.022 INFO  [Executor task launch worker for task 36] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:42.044 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-35_646_5378599238798843018-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
11:51:42.044 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-35_727_2206226797398175962-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
11:51:42.062 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
11:51:42.063 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
11:51:42.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:42.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:42.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:42.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:42.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:42.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:42.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:42.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:42.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:42.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:42.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:42.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:42.109 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.110 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.111 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:42.111 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.111 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.111 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.111 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.112 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.112 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:42.126 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:42.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:42.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:42.127 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:42.128 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:42.128 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:42.132 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.132 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.133 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fc1fe53
11:51:42.134 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.134 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.135 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6baa7029
11:51:42.135 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@88d7a05
11:51:42.135 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@257b6783
11:51:42.159 INFO  [Executor task launch worker for task 33] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
11:51:42.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.160 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.170 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:42.170 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:42.170 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:42.170 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:42.170 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115142_0015_m_000003_0/bdp_day=20190922/part-00003-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000
11:51:42.170 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115142_0015_m_000002_0/bdp_day=20190922/part-00002-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000
11:51:42.171 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115142_0015_m_000001_0/bdp_day=20190922/part-00001-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000
11:51:42.171 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115142_0015_m_000000_0/bdp_day=20190922/part-00000-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000
11:51:42.173 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:42.174 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:42.174 INFO  [Executor task launch worker for task 34] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:42.174 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:42.174 INFO  [Executor task launch worker for task 35] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:42.174 INFO  [Executor task launch worker for task 36] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:42.177 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:42.178 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:42.178 INFO  [Executor task launch worker for task 33] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:42.178 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:42.178 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:42.178 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:42.178 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:42.178 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:42.178 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:42.178 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:42.178 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:42.178 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:42.178 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:42.178 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:42.178 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:42.178 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:42.179 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:42.179 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:42.179 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:42.179 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:42.179 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:42.179 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:42.179 INFO  [Executor task launch worker for task 34] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:42.179 INFO  [Executor task launch worker for task 33] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:42.180 INFO  [Executor task launch worker for task 35] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:42.180 INFO  [Executor task launch worker for task 36] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:42.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.187 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.188 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.189 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:42.198 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:42.198 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:42.198 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:42.199 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:42.199 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:42.199 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:42.199 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:42.199 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:42.209 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:42.210 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:42.210 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:42.210 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:42.210 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:42.211 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:42.211 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:42.211 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:42.252 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:42.253 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:42.257 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.257 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.260 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:42.261 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:42.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.279 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.287 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:42.287 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:42.300 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.300 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.300 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.300 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.300 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.300 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.301 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.301 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.301 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.301 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:42.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:42.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:42.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:42.303 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:42.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.305 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:42.306 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:42.307 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:42.346 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
11:51:42.347 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 04)
11:51:42.347 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:42.347 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:42.347 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:42.349 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
11:51:42.349 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 03)
11:51:42.350 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:42.350 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:42.350 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:42.367 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 311.6 KB, free 1987.5 MB)
11:51:42.368 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 311.6 KB, free 1987.5 MB)
11:51:42.387 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.5 MB)
11:51:42.387 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.6 MB)
11:51:42.387 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.5 MB)
11:51:42.388 INFO  [main] org.apache.spark.SparkContext - Created broadcast 16 from show at Dw03ReleaseExposure.scala:65
11:51:42.388 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:42.388 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.6 MB)
11:51:42.389 INFO  [main] org.apache.spark.SparkContext - Created broadcast 16 from show at Dw04ReleaseClick.scala:66
11:51:42.390 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:42.402 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:42.402 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 42 (show at Dw04ReleaseClick.scala:66)
11:51:42.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 8 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
11:51:42.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (show at Dw04ReleaseClick.scala:66)
11:51:42.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
11:51:42.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
11:51:42.403 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[42] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:42.406 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:42.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 42 (show at Dw03ReleaseExposure.scala:65)
11:51:42.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 8 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
11:51:42.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (show at Dw03ReleaseExposure.scala:65)
11:51:42.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
11:51:42.406 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
11:51:42.406 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 17.6 KB, free 1987.4 MB)
11:51:42.407 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[42] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:42.409 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.4 MB)
11:51:42.410 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.32.1:7345 (size: 7.5 KB, free: 1988.5 MB)
11:51:42.411 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1004
11:51:42.412 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[42] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:42.412 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 8 tasks
11:51:42.412 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 37, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:42.412 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 38, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:42.412 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 39, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:42.412 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 17.6 KB, free 1987.4 MB)
11:51:42.413 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 40, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:42.413 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 41, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:42.413 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 42, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:42.414 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 43, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:42.414 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 16.0 (TID 44, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:42.414 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 38)
11:51:42.414 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 41)
11:51:42.415 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 43)
11:51:42.415 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 37)
11:51:42.415 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.4 MB)
11:51:42.416 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 39)
11:51:42.417 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 40)
11:51:42.417 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:51:42.419 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.32.1:7123 (size: 7.5 KB, free: 1988.5 MB)
11:51:42.418 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 42)
11:51:42.420 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1004
11:51:42.421 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[42] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:42.418 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 7.0 in stage 16.0 (TID 44)
11:51:42.421 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 8 tasks
11:51:42.419 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:51:42.419 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:51:42.423 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 37, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:42.419 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:51:42.422 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:51:42.424 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 38, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:42.424 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 39, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:42.424 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 40, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:42.425 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 41, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:42.425 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 42, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:42.425 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 43, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:42.426 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 16.0 (TID 44, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:42.426 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 40)
11:51:42.426 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 41)
11:51:42.422 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:51:42.427 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 37)
11:51:42.428 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 38)
11:51:42.428 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 39)
11:51:42.432 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:51:42.433 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:51:42.434 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 7.0 in stage 16.0 (TID 44)
11:51:42.435 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:51:42.436 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 43)
11:51:42.423 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:51:42.439 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 42)
11:51:42.423 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:51:42.431 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.447 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.450 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:51:42.452 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 38). 1371 bytes result sent to driver
11:51:42.453 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 38) in 41 ms on localhost (executor driver) (1/8)
11:51:42.454 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:51:42.466 INFO  [Executor task launch worker for task 37] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.479 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:51:42.481 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:51:42.485 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:51:42.496 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.509 INFO  [Executor task launch worker for task 39] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.511 INFO  [Executor task launch worker for task 39] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.522 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 37). 1371 bytes result sent to driver
11:51:42.522 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.524 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 37) in 112 ms on localhost (executor driver) (2/8)
11:51:42.529 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.532 INFO  [Executor task launch worker for task 37] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.536 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 39). 1371 bytes result sent to driver
11:51:42.539 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 39). 1328 bytes result sent to driver
11:51:42.539 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 39) in 127 ms on localhost (executor driver) (3/8)
11:51:42.540 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 39) in 116 ms on localhost (executor driver) (1/8)
11:51:42.557 INFO  [Executor task launch worker for task 36] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2d1323b3
11:51:42.557 INFO  [Executor task launch worker for task 35] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3f9992ee
11:51:42.558 INFO  [Executor task launch worker for task 33] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@7c7952f6
11:51:42.559 INFO  [Executor task launch worker for task 34] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@334ed0ba
11:51:42.593 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 43). 1371 bytes result sent to driver
11:51:42.593 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 43) in 180 ms on localhost (executor driver) (4/8)
11:51:42.596 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.601 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:42.601 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.608 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.609 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.644 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 41). 1371 bytes result sent to driver
11:51:42.647 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.649 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 37). 1371 bytes result sent to driver
11:51:42.655 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 41) in 242 ms on localhost (executor driver) (5/8)
11:51:42.664 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 37) in 241 ms on localhost (executor driver) (2/8)
11:51:42.703 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 42). 1371 bytes result sent to driver
11:51:42.703 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 42) in 290 ms on localhost (executor driver) (6/8)
11:51:42.726 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:42.743 INFO  [Executor task launch worker for task 34] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,854
11:51:42.748 INFO  [Executor task launch worker for task 36] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,634
11:51:42.758 INFO  [Executor task launch worker for task 33] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 323,058
11:51:42.765 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 7.0 in stage 16.0 (TID 44). 1371 bytes result sent to driver
11:51:42.768 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 16.0 (TID 44) in 354 ms on localhost (executor driver) (7/8)
11:51:42.770 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 43). 1371 bytes result sent to driver
11:51:42.771 INFO  [Executor task launch worker for task 35] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,810
11:51:42.772 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 43) in 347 ms on localhost (executor driver) (3/8)
11:51:42.775 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 38). 1371 bytes result sent to driver
11:51:42.776 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 38) in 353 ms on localhost (executor driver) (4/8)
11:51:42.802 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 42). 1371 bytes result sent to driver
11:51:42.803 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 42) in 378 ms on localhost (executor driver) (5/8)
11:51:42.849 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 41). 1371 bytes result sent to driver
11:51:42.850 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 41) in 425 ms on localhost (executor driver) (6/8)
11:51:42.852 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.852 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.853 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.853 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.853 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.855 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.856 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.857 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.857 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:42.858 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:42.858 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.858 INFO  [Executor task launch worker for task 36] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.858 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.859 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.860 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.860 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:42.861 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:42.861 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.861 INFO  [Executor task launch worker for task 34] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.864 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.864 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.865 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.865 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.865 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.865 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:42.865 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:42.866 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.866 INFO  [Executor task launch worker for task 33] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:42.868 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:42.868 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:42.869 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 7.0 in stage 16.0 (TID 44). 1414 bytes result sent to driver
11:51:42.869 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 16.0 (TID 44) in 444 ms on localhost (executor driver) (7/8)
11:51:42.871 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:42.871 INFO  [Executor task launch worker for task 35] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:43.108 INFO  [Executor task launch worker for task 35] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115142_0015_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/task_20190926115142_0015_m_000002
11:51:43.108 INFO  [Executor task launch worker for task 33] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115142_0015_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/task_20190926115142_0015_m_000000
11:51:43.108 INFO  [Executor task launch worker for task 36] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115142_0015_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/task_20190926115142_0015_m_000003
11:51:43.108 INFO  [Executor task launch worker for task 35] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115142_0015_m_000002_0: Committed
11:51:43.108 INFO  [Executor task launch worker for task 36] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115142_0015_m_000003_0: Committed
11:51:43.108 INFO  [Executor task launch worker for task 33] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115142_0015_m_000000_0: Committed
11:51:43.110 INFO  [Executor task launch worker for task 36] org.apache.spark.executor.Executor - Finished task 3.0 in stage 15.0 (TID 36). 2572 bytes result sent to driver
11:51:43.110 INFO  [Executor task launch worker for task 35] org.apache.spark.executor.Executor - Finished task 2.0 in stage 15.0 (TID 35). 2615 bytes result sent to driver
11:51:43.110 INFO  [Executor task launch worker for task 33] org.apache.spark.executor.Executor - Finished task 0.0 in stage 15.0 (TID 33). 2615 bytes result sent to driver
11:51:43.111 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 15.0 (TID 36) in 1145 ms on localhost (executor driver) (1/4)
11:51:43.111 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 15.0 (TID 35) in 1145 ms on localhost (executor driver) (2/4)
11:51:43.111 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 15.0 (TID 33) in 1145 ms on localhost (executor driver) (3/4)
11:51:43.544 INFO  [Executor task launch worker for task 34] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115142_0015_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/_temporary/0/task_20190926115142_0015_m_000001
11:51:43.544 INFO  [Executor task launch worker for task 34] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115142_0015_m_000001_0: Committed
11:51:43.545 INFO  [Executor task launch worker for task 34] org.apache.spark.executor.Executor - Finished task 1.0 in stage 15.0 (TID 34). 2572 bytes result sent to driver
11:51:43.546 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 15.0 (TID 34) in 1580 ms on localhost (executor driver) (4/4)
11:51:43.546 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 15.0, whose tasks have all completed, from pool 
11:51:43.546 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 15 (insertInto at SparkHelper.scala:35) finished in 1.580 s
11:51:43.546 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 7 finished: insertInto at SparkHelper.scala:35, took 5.191519 s
11:51:43.580 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:43.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:43.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:43.603 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:43.604 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:43.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.628 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:43.630 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:43.630 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:43.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:43.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:43.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:43.652 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:43.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:43.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:43.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:43.653 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:43.654 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:43.654 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:43.693 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 327
11:51:43.693 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 278
11:51:43.693 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 5
11:51:43.693 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 273
11:51:43.694 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 276
11:51:43.694 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 275
11:51:43.694 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 272
11:51:43.694 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 271
11:51:43.696 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.32.1:7345 in memory (size: 62.1 KB, free: 1988.6 MB)
11:51:43.696 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 269
11:51:43.696 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 274
11:51:43.697 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 268
11:51:43.698 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 192.168.32.1:7345 in memory (size: 7.5 KB, free: 1988.6 MB)
11:51:43.709 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:43.712 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 277
11:51:43.712 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 270
11:51:43.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]
11:51:43.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]	
11:51:43.761 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00000-03068620-8444-4ea9-853a-1de6907c559a.c000
11:51:43.767 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:43.771 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00001-03068620-8444-4ea9-853a-1de6907c559a.c000
11:51:43.772 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:43.774 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00002-03068620-8444-4ea9-853a-1de6907c559a.c000
11:51:43.776 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:43.777 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00003-03068620-8444-4ea9-853a-1de6907c559a.c000
11:51:43.780 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:43.805 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
11:51:43.811 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 40). 1543 bytes result sent to driver
11:51:43.812 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 40) in 1388 ms on localhost (executor driver) (8/8)
11:51:43.812 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
11:51:43.812 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (show at Dw03ReleaseExposure.scala:65) finished in 1.391 s
11:51:43.812 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:43.813 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:43.813 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
11:51:43.813 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:43.813 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[44] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:43.814 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 1987.4 MB)
11:51:43.815 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.4 MB)
11:51:43.816 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.32.1:7123 (size: 2.3 KB, free: 1988.5 MB)
11:51:43.817 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1004
11:51:43.817 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[44] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
11:51:43.817 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks
11:51:43.818 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 45, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:43.818 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 45)
11:51:43.821 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:43.821 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:43.825 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 45). 4551 bytes result sent to driver
11:51:43.826 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 45) in 9 ms on localhost (executor driver) (1/1)
11:51:43.826 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
11:51:43.826 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (show at Dw03ReleaseExposure.scala:65) finished in 0.009 s
11:51:43.827 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 8 finished: show at Dw03ReleaseExposure.scala:65, took 1.420864 s
11:51:43.831 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 40). 1586 bytes result sent to driver
11:51:43.832 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 40) in 1420 ms on localhost (executor driver) (8/8)
11:51:43.832 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
11:51:43.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (show at Dw04ReleaseClick.scala:66) finished in 1.421 s
11:51:43.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:43.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:43.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
11:51:43.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:43.833 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[44] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:43.833 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/bdp_day=20190922/part-00000-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00000-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, Status:true
11:51:43.834 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:43.836 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1988.0 MB)
11:51:43.837 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.32.1:7345 (size: 2.3 KB, free: 1988.6 MB)
11:51:43.838 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1004
11:51:43.838 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[44] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:43.839 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks
11:51:43.839 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
11:51:43.839 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 45, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:43.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:43.839 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 45)
11:51:43.839 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:43.841 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:43.841 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:43.846 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 45). 4712 bytes result sent to driver
11:51:43.847 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 45) in 8 ms on localhost (executor driver) (1/1)
11:51:43.847 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
11:51:43.848 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (show at Dw04ReleaseClick.scala:66) finished in 0.009 s
11:51:43.849 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 8 finished: show at Dw04ReleaseClick.scala:66, took 1.446946 s
11:51:43.861 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
11:51:43.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:43.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:43.888 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.889 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.890 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:43.890 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/bdp_day=20190922/part-00001-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00001-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, Status:true
11:51:43.898 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1
11:51:43.910 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/bdp_day=20190922/part-00002-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00002-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, Status:true
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:43.923 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:43.929 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1
11:51:43.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:43.932 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:43.942 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/bdp_day=20190922/part-00003-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190922/part-00003-8e28c7bc-8e65-44ab-b889-473fcb00cc1e.c000, Status:true
11:51:43.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]
11:51:43.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:43.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190922]	
11:51:43.949 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:43.976 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:43.977 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:43.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:43.983 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:43.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:43.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:43.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_register_users
11:51:43.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_register_users	
11:51:43.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190922]
11:51:44.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:44.011 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:44.014 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.014 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.014 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.014 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.014 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.014 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:44.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:44.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:44.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:44.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:44.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.039 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.040 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.040 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.040 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:44.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:44.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:44.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:44.042 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:44.051 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
11:51:44.055 WARN  [main] hive.log - Updated size to 872108
11:51:44.066 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
11:51:44.066 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 03)
11:51:44.067 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:44.067 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:44.067 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:44.074 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:44.074 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:44.145 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#298),(bdp_day#298 = 20190923)
11:51:44.146 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#291),(release_status#291 = 04)
11:51:44.146 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:44.146 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:44.147 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:44.152 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:44.153 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:44.168 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 311.6 KB, free 1987.7 MB)
11:51:44.176 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 311.6 KB, free 1987.1 MB)
11:51:44.198 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.7 MB)
11:51:44.198 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.6 MB)
11:51:44.200 INFO  [main] org.apache.spark.SparkContext - Created broadcast 19 from insertInto at SparkHelper.scala:35
11:51:44.200 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:44.213 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
11:51:44.215 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.5 MB)
11:51:44.217 INFO  [main] org.apache.spark.SparkContext - Created broadcast 19 from insertInto at SparkHelper.scala:35
11:51:44.217 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:44.236 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:44.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 48 (insertInto at SparkHelper.scala:35)
11:51:44.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 9 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:44.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (insertInto at SparkHelper.scala:35)
11:51:44.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 18)
11:51:44.236 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 18)
11:51:44.237 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 18 (MapPartitionsRDD[48] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:44.239 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 17.6 KB, free 1987.7 MB)
11:51:44.241 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.7 MB)
11:51:44.242 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.32.1:7345 (size: 7.5 KB, free: 1988.6 MB)
11:51:44.242 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1004
11:51:44.242 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[48] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:44.242 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 8 tasks
11:51:44.243 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 46, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:44.244 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 47, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:44.244 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 18.0 (TID 48, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:44.244 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 18.0 (TID 49, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:44.245 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 18.0 (TID 50, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:44.245 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 18.0 (TID 51, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:44.245 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 18.0 (TID 52, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:44.245 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 18.0 (TID 53, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:44.245 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 18.0 (TID 49)
11:51:44.245 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 18.0 (TID 48)
11:51:44.245 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 47)
11:51:44.245 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 5.0 in stage 18.0 (TID 51)
11:51:44.245 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 4.0 in stage 18.0 (TID 50)
11:51:44.245 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 7.0 in stage 18.0 (TID 53)
11:51:44.245 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 6.0 in stage 18.0 (TID 52)
11:51:44.245 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 46)
11:51:44.248 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:51:44.248 INFO  [Executor task launch worker for task 51] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:51:44.251 INFO  [Executor task launch worker for task 50] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:51:44.257 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:51:44.260 INFO  [Executor task launch worker for task 53] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:51:44.260 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:51:44.264 INFO  [Executor task launch worker for task 52] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:51:44.265 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:51:44.272 INFO  [Executor task launch worker for task 49] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.273 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:44.274 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 48 (insertInto at SparkHelper.scala:35)
11:51:44.275 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 9 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:44.275 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (insertInto at SparkHelper.scala:35)
11:51:44.275 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 18)
11:51:44.275 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 18)
11:51:44.276 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 18 (MapPartitionsRDD[48] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:44.279 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 17.6 KB, free 1987.1 MB)
11:51:44.282 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.1 MB)
11:51:44.282 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.32.1:7123 (size: 7.5 KB, free: 1988.5 MB)
11:51:44.283 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1004
11:51:44.283 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[48] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:44.284 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 8 tasks
11:51:44.284 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 46, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:44.285 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 47, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:44.285 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 18.0 (TID 48, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:44.285 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 18.0 (TID 49, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:44.285 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 18.0 (TID 50, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:44.285 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 18.0 (TID 51, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:44.286 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 18.0 (TID 52, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:44.286 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 18.0 (TID 53, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:44.286 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 46)
11:51:44.286 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 47)
11:51:44.286 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 4.0 in stage 18.0 (TID 50)
11:51:44.286 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 5.0 in stage 18.0 (TID 51)
11:51:44.286 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 6.0 in stage 18.0 (TID 52)
11:51:44.286 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 7.0 in stage 18.0 (TID 53)
11:51:44.286 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 18.0 (TID 48)
11:51:44.286 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 18.0 (TID 49)
11:51:44.289 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:51:44.290 INFO  [Executor task launch worker for task 52] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:51:44.292 INFO  [Executor task launch worker for task 53] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:51:44.293 INFO  [Executor task launch worker for task 50] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:51:44.293 INFO  [Executor task launch worker for task 51] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:51:44.294 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:51:44.298 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:51:44.301 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:51:44.321 INFO  [Executor task launch worker for task 46] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.322 INFO  [Executor task launch worker for task 51] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.351 INFO  [Executor task launch worker for task 49] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.351 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 46). 1328 bytes result sent to driver
11:51:44.352 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 46) in 68 ms on localhost (executor driver) (1/8)
11:51:44.352 INFO  [Executor task launch worker for task 51] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.356 INFO  [Executor task launch worker for task 48] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.360 INFO  [Executor task launch worker for task 52] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.366 INFO  [Executor task launch worker for task 48] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.382 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 18.0 (TID 48). 1328 bytes result sent to driver
11:51:44.388 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 5.0 in stage 18.0 (TID 51). 1414 bytes result sent to driver
11:51:44.390 INFO  [Executor task launch worker for task 52] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.400 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 6.0 in stage 18.0 (TID 52). 1328 bytes result sent to driver
11:51:44.404 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 5.0 in stage 18.0 (TID 51). 1328 bytes result sent to driver
11:51:44.408 INFO  [Executor task launch worker for task 47] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.410 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 6.0 in stage 18.0 (TID 52). 1371 bytes result sent to driver
11:51:44.413 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 18.0 (TID 48) in 169 ms on localhost (executor driver) (1/8)
11:51:44.414 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 18.0 (TID 52) in 128 ms on localhost (executor driver) (2/8)
11:51:44.414 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 18.0 (TID 52) in 169 ms on localhost (executor driver) (2/8)
11:51:44.414 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 18.0 (TID 51) in 129 ms on localhost (executor driver) (3/8)
11:51:44.415 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 18.0 (TID 51) in 169 ms on localhost (executor driver) (3/8)
11:51:44.420 INFO  [Executor task launch worker for task 53] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.430 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 18.0 (TID 48). 1371 bytes result sent to driver
11:51:44.431 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 18.0 (TID 48) in 146 ms on localhost (executor driver) (4/8)
11:51:44.432 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 7.0 in stage 18.0 (TID 53). 1371 bytes result sent to driver
11:51:44.433 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 18.0 (TID 53) in 188 ms on localhost (executor driver) (4/8)
11:51:44.436 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 47). 1371 bytes result sent to driver
11:51:44.436 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 47) in 193 ms on localhost (executor driver) (5/8)
11:51:44.505 INFO  [Executor task launch worker for task 53] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.508 INFO  [Executor task launch worker for task 50] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.532 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 7.0 in stage 18.0 (TID 53). 1371 bytes result sent to driver
11:51:44.533 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 18.0 (TID 53) in 247 ms on localhost (executor driver) (5/8)
11:51:44.533 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 4.0 in stage 18.0 (TID 50). 1371 bytes result sent to driver
11:51:44.535 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 18.0 (TID 50) in 291 ms on localhost (executor driver) (6/8)
11:51:44.539 INFO  [Executor task launch worker for task 46] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:44.548 INFO  [Executor task launch worker for task 47] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.553 INFO  [Executor task launch worker for task 50] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:44.556 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 46). 1414 bytes result sent to driver
11:51:44.557 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 46) in 314 ms on localhost (executor driver) (7/8)
11:51:44.559 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 47). 1371 bytes result sent to driver
11:51:44.560 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 47) in 276 ms on localhost (executor driver) (6/8)
11:51:44.576 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 4.0 in stage 18.0 (TID 50). 1371 bytes result sent to driver
11:51:44.577 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 18.0 (TID 50) in 292 ms on localhost (executor driver) (7/8)
11:51:44.641 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-38_156_4301309861461088076-1/-ext-10000/bdp_day=20190922 with partSpec {bdp_day=20190922}
11:51:44.657 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:51:44.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:44.658 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:44.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:44.671 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:44.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:44.717 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.737 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.738 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:44.753 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:44.753 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:44.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:44.794 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:44.794 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:44.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:44.832 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:44.897 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.898 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.898 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.898 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.898 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.898 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:44.899 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:44.913 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:51:44.914 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:44.915 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:44.915 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:44.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:44.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:44.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:44.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:44.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:44.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:44.917 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:44.974 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:44.975 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:45.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:45.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:45.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:45.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:45.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:45.082 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:45.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:45.083 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:45.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:45.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:45.139 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#331),(bdp_day#331 = 20190923)
11:51:45.140 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#324),(release_status#324 = 06)
11:51:45.140 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:45.140 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:45.141 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:45.155 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
11:51:45.258 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_16_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
11:51:45.259 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_16_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.5 MB)
11:51:45.261 INFO  [main] org.apache.spark.SparkContext - Created broadcast 16 from show at Dw05ReleaseRegisterUsers.scala:66
11:51:45.261 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:45.270 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:51:45.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 52 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:45.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 8 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:51:45.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 17 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:45.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 16)
11:51:45.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 16)
11:51:45.272 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 16 (MapPartitionsRDD[52] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:45.277 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17 stored as values in memory (estimated size 21.7 KB, free 1987.4 MB)
11:51:45.278 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_17_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.4 MB)
11:51:45.279 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_17_piece0 in memory on 192.168.32.1:7247 (size: 9.6 KB, free: 1988.5 MB)
11:51:45.280 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 17 from broadcast at DAGScheduler.scala:1004
11:51:45.281 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[52] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:45.281 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 16.0 with 8 tasks
11:51:45.282 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 16.0 (TID 37, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:45.282 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 16.0 (TID 38, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:45.282 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 16.0 (TID 39, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:45.282 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 16.0 (TID 40, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:45.283 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 16.0 (TID 41, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:45.283 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 16.0 (TID 42, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:45.283 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 16.0 (TID 43, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:45.283 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 16.0 (TID 44, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:45.283 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Running task 0.0 in stage 16.0 (TID 37)
11:51:45.283 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Running task 1.0 in stage 16.0 (TID 38)
11:51:45.283 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Running task 2.0 in stage 16.0 (TID 39)
11:51:45.284 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Running task 3.0 in stage 16.0 (TID 40)
11:51:45.284 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Running task 6.0 in stage 16.0 (TID 43)
11:51:45.285 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Running task 4.0 in stage 16.0 (TID 41)
11:51:45.285 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Running task 5.0 in stage 16.0 (TID 42)
11:51:45.285 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Running task 7.0 in stage 16.0 (TID 44)
11:51:45.289 INFO  [Executor task launch worker for task 39] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:51:45.289 INFO  [Executor task launch worker for task 37] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:51:45.289 INFO  [Executor task launch worker for task 43] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:51:45.289 INFO  [Executor task launch worker for task 40] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:51:45.290 INFO  [Executor task launch worker for task 38] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:51:45.293 INFO  [Executor task launch worker for task 42] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:51:45.293 INFO  [Executor task launch worker for task 41] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:51:45.298 INFO  [Executor task launch worker for task 44] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:51:45.333 INFO  [Executor task launch worker for task 39] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.345 INFO  [Executor task launch worker for task 39] org.apache.spark.executor.Executor - Finished task 2.0 in stage 16.0 (TID 39). 1524 bytes result sent to driver
11:51:45.346 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 16.0 (TID 39) in 64 ms on localhost (executor driver) (1/8)
11:51:45.353 INFO  [Executor task launch worker for task 41] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.356 INFO  [Executor task launch worker for task 38] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.368 INFO  [Executor task launch worker for task 41] org.apache.spark.executor.Executor - Finished task 4.0 in stage 16.0 (TID 41). 1481 bytes result sent to driver
11:51:45.369 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 16.0 (TID 41) in 87 ms on localhost (executor driver) (2/8)
11:51:45.372 INFO  [Executor task launch worker for task 38] org.apache.spark.executor.Executor - Finished task 1.0 in stage 16.0 (TID 38). 1524 bytes result sent to driver
11:51:45.373 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 16.0 (TID 38) in 91 ms on localhost (executor driver) (3/8)
11:51:45.386 INFO  [Executor task launch worker for task 40] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.405 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 277
11:51:45.405 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 281
11:51:45.405 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 273
11:51:45.405 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 330
11:51:45.406 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.6 MB)
11:51:45.407 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 278
11:51:45.407 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 275
11:51:45.408 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on 192.168.32.1:7247 in memory (size: 9.6 KB, free: 1988.6 MB)
11:51:45.408 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 280
11:51:45.409 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.32.1:7247 in memory (size: 64.1 KB, free: 1988.6 MB)
11:51:45.409 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 271
11:51:45.409 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 270
11:51:45.409 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 274
11:51:45.409 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 272
11:51:45.410 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 5
11:51:45.410 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 276
11:51:45.410 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 279
11:51:45.436 INFO  [Executor task launch worker for task 42] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.439 INFO  [Executor task launch worker for task 44] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.443 INFO  [Executor task launch worker for task 43] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.443 INFO  [Executor task launch worker for task 37] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:45.445 INFO  [Executor task launch worker for task 42] org.apache.spark.executor.Executor - Finished task 5.0 in stage 16.0 (TID 42). 1567 bytes result sent to driver
11:51:45.445 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 16.0 (TID 42) in 162 ms on localhost (executor driver) (4/8)
11:51:45.450 INFO  [Executor task launch worker for task 44] org.apache.spark.executor.Executor - Finished task 7.0 in stage 16.0 (TID 44). 1567 bytes result sent to driver
11:51:45.451 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 16.0 (TID 44) in 168 ms on localhost (executor driver) (5/8)
11:51:45.455 INFO  [Executor task launch worker for task 37] org.apache.spark.executor.Executor - Finished task 0.0 in stage 16.0 (TID 37). 1567 bytes result sent to driver
11:51:45.455 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 16.0 (TID 37) in 174 ms on localhost (executor driver) (6/8)
11:51:45.458 INFO  [Executor task launch worker for task 43] org.apache.spark.executor.Executor - Finished task 6.0 in stage 16.0 (TID 43). 1524 bytes result sent to driver
11:51:45.459 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 16.0 (TID 43) in 176 ms on localhost (executor driver) (7/8)
11:51:46.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 331
11:51:46.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 333
11:51:46.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 330
11:51:46.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 384
11:51:46.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 328
11:51:46.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 335
11:51:46.393 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 329
11:51:46.394 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.32.1:7123 in memory (size: 7.5 KB, free: 1988.5 MB)
11:51:46.397 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.32.1:7123 in memory (size: 2.3 KB, free: 1988.5 MB)
11:51:46.397 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 327
11:51:46.398 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.32.1:7123 in memory (size: 26.2 KB, free: 1988.5 MB)
11:51:46.399 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 6
11:51:46.400 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_15_piece0 on 192.168.32.1:7123 in memory (size: 62.1 KB, free: 1988.6 MB)
11:51:46.401 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 334
11:51:46.401 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 332
11:51:46.501 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 18.0 (TID 49). 1543 bytes result sent to driver
11:51:46.502 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 18.0 (TID 49) in 2258 ms on localhost (executor driver) (8/8)
11:51:46.502 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
11:51:46.502 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 18 (insertInto at SparkHelper.scala:35) finished in 2.259 s
11:51:46.502 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:46.502 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:46.502 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 19)
11:51:46.502 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:46.503 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[50] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:46.505 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 18.0 (TID 49). 1629 bytes result sent to driver
11:51:46.506 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 18.0 (TID 49) in 2221 ms on localhost (executor driver) (8/8)
11:51:46.506 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
11:51:46.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 18 (insertInto at SparkHelper.scala:35) finished in 2.222 s
11:51:46.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:46.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:46.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 19)
11:51:46.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:46.506 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[50] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:46.531 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 167.4 KB, free 1987.5 MB)
11:51:46.534 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.4 MB)
11:51:46.534 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.32.1:7345 (size: 62.1 KB, free: 1988.5 MB)
11:51:46.534 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 167.4 KB, free 1987.5 MB)
11:51:46.534 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1004
11:51:46.536 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 19 (MapPartitionsRDD[50] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:46.536 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 4 tasks
11:51:46.537 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1987.4 MB)
11:51:46.538 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 54, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:46.538 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 55, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:46.538 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 56, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:46.538 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.32.1:7123 (size: 62.1 KB, free: 1988.5 MB)
11:51:46.538 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 57, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:46.538 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 56)
11:51:46.538 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 55)
11:51:46.538 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 54)
11:51:46.538 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 57)
11:51:46.538 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1004
11:51:46.538 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 19 (MapPartitionsRDD[50] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:46.538 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 4 tasks
11:51:46.539 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 54, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:46.540 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 55, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:46.540 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 56, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:46.540 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 57, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:46.541 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 54)
11:51:46.541 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 55)
11:51:46.541 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 57)
11:51:46.543 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 56)
11:51:46.556 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.556 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:46.558 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.558 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:46.559 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.559 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:46.559 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.559 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:46.559 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.559 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:46.563 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.563 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:46.563 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.563 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:46.564 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:46.564 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:46.572 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.572 INFO  [Executor task launch worker for task 55] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.576 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.577 INFO  [Executor task launch worker for task 56] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.578 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.579 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.580 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.581 INFO  [Executor task launch worker for task 56] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.581 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8d4cb8e
11:51:46.581 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.581 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000001_0/bdp_day=20190923/part-00001-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.582 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.586 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.586 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.586 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61842999
11:51:46.587 INFO  [Executor task launch worker for task 54] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.587 INFO  [Executor task launch worker for task 55] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.588 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.588 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000002_0/bdp_day=20190923/part-00002-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.589 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.594 INFO  [Executor task launch worker for task 57] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.596 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:46.597 INFO  [Executor task launch worker for task 57] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.594 INFO  [Executor task launch worker for task 54] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:46.596 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b44d651
11:51:46.599 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.599 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000003_0/bdp_day=20190923/part-00003-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.599 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.601 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b834589
11:51:46.601 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.601 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@432f8dfa
11:51:46.601 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000002_0/bdp_day=20190923/part-00002-8fdd4807-aac7-4310-9393-c2ceab869248.c000
11:51:46.601 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.601 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.601 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.601 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.602 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000001_0/bdp_day=20190923/part-00001-8fdd4807-aac7-4310-9393-c2ceab869248.c000
11:51:46.602 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@314b7709
11:51:46.607 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.608 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.608 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6f28613a
11:51:46.609 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.609 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ce1cca
11:51:46.609 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.609 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.609 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000000_0/bdp_day=20190923/part-00000-8fdd4807-aac7-4310-9393-c2ceab869248.c000
11:51:46.609 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.609 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.609 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.609 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000003_0/bdp_day=20190923/part-00003-8fdd4807-aac7-4310-9393-c2ceab869248.c000
11:51:46.609 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:46.609 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115146_0019_m_000000_0/bdp_day=20190923/part-00000-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.609 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.609 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.609 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.609 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.609 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.609 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:46.609 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.610 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.609 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.610 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:46.610 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.610 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.610 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.610 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:46.610 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.610 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:46.610 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.610 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:46.610 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.610 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:46.610 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.610 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.610 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:46.656 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@878aa14
11:51:46.656 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@455f078e
11:51:46.656 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@7cf42b0
11:51:46.656 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@359bf068
11:51:46.662 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@762c2e54
11:51:46.790 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5b2d1190
11:51:46.840 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@30a7622c
11:51:46.866 INFO  [Executor task launch worker for task 55] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,911
11:51:46.878 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:46.879 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:46.879 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:46.879 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 2,751B for [device_type] BINARY: 10,753 values, 2,720B raw, 2,720B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:46.880 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:46.880 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:46.881 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,412 entries, 19,296B raw, 2,412B comp}
11:51:46.890 INFO  [Executor task launch worker for task 57] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,336
11:51:46.906 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.907 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.907 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.908 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:46.908 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:46.908 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.909 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:46.930 INFO  [Executor task launch worker for task 55] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,366
11:51:46.940 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.940 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.941 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.941 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:46.941 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:46.941 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.943 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:46.961 INFO  [Executor task launch worker for task 56] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,172
11:51:46.969 INFO  [Executor task launch worker for task 56] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,231
11:51:46.972 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 290,389B for [release_session] BINARY: 10,752 values, 290,312B raw, 290,312B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:46.973 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:46.973 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 107,571B for [device_num] BINARY: 10,752 values, 107,528B raw, 107,528B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:46.974 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 2,751B for [device_type] BINARY: 10,752 values, 2,720B raw, 2,720B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:46.974 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 4,105B for [sources] BINARY: 10,752 values, 4,063B raw, 4,063B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:46.974 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:46.974 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 16,206B for [ct] INT64: 10,752 values, 16,159B raw, 16,159B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,404 entries, 19,232B raw, 2,404B comp}
11:51:46.974 INFO  [Executor task launch worker for task 57] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,353
11:51:46.979 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.980 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.980 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.980 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,345 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:46.981 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:46.981 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.981 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:46.981 INFO  [Executor task launch worker for task 54] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,536
11:51:46.981 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:46.981 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:46.982 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:46.982 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:46.983 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:46.983 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:46.983 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,426 entries, 19,408B raw, 2,426B comp}
11:51:46.990 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.990 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.990 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:46.991 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:46.991 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:46.991 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:46.991 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:47.176 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@7d8da667
11:51:47.213 INFO  [Executor task launch worker for task 54] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,209
11:51:47.219 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:47.220 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:47.221 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:47.221 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:47.221 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:47.221 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:47.221 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,403 entries, 19,224B raw, 2,403B comp}
11:51:47.237 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000003
11:51:47.237 INFO  [Executor task launch worker for task 57] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000003_0: Committed
11:51:47.238 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 57). 2482 bytes result sent to driver
11:51:47.239 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 57) in 701 ms on localhost (executor driver) (1/4)
11:51:47.239 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000002
11:51:47.239 INFO  [Executor task launch worker for task 56] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000002_0: Committed
11:51:47.239 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 56). 2439 bytes result sent to driver
11:51:47.240 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 56) in 702 ms on localhost (executor driver) (2/4)
11:51:48.436 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000003
11:51:48.436 INFO  [Executor task launch worker for task 57] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000003_0: Committed
11:51:48.436 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 57). 2482 bytes result sent to driver
11:51:48.437 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 57) in 1897 ms on localhost (executor driver) (1/4)
11:51:48.438 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000001
11:51:48.438 INFO  [Executor task launch worker for task 55] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000001_0: Committed
11:51:48.439 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 55). 2439 bytes result sent to driver
11:51:48.439 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 55) in 1901 ms on localhost (executor driver) (3/4)
11:51:48.450 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000000
11:51:48.450 INFO  [Executor task launch worker for task 54] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000000_0: Committed
11:51:48.450 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 54). 2439 bytes result sent to driver
11:51:48.451 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 54) in 1912 ms on localhost (executor driver) (2/4)
11:51:48.464 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000001
11:51:48.465 INFO  [Executor task launch worker for task 55] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000001_0: Committed
11:51:48.465 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 55). 2482 bytes result sent to driver
11:51:48.466 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 55) in 1926 ms on localhost (executor driver) (3/4)
11:51:48.813 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000002
11:51:48.813 INFO  [Executor task launch worker for task 56] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000002_0: Committed
11:51:48.813 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 56). 2482 bytes result sent to driver
11:51:48.814 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 56) in 2274 ms on localhost (executor driver) (4/4)
11:51:48.814 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
11:51:48.814 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 19 (insertInto at SparkHelper.scala:35) finished in 2.275 s
11:51:48.814 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 9 finished: insertInto at SparkHelper.scala:35, took 4.541496 s
11:51:48.815 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115146_0019_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/_temporary/0/task_20190926115146_0019_m_000000
11:51:48.815 INFO  [Executor task launch worker for task 54] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115146_0019_m_000000_0: Committed
11:51:48.816 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 54). 2482 bytes result sent to driver
11:51:48.817 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 54) in 2279 ms on localhost (executor driver) (4/4)
11:51:48.817 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
11:51:48.817 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 19 (insertInto at SparkHelper.scala:35) finished in 2.280 s
11:51:48.817 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 9 finished: insertInto at SparkHelper.scala:35, took 4.581970 s
11:51:48.904 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:48.904 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:48.905 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:48.908 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:48.909 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:48.909 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:48.959 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:48.959 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:48.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:48.961 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:49.005 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.006 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.006 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.006 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.006 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.007 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.007 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.007 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:49.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:49.008 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:49.011 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.012 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.013 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.013 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:49.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:49.014 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:49.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.069 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:49.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:49.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.078 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:49.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:49.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]
11:51:49.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]	
11:51:49.108 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]
11:51:49.108 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]	
11:51:49.150 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00000-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
11:51:49.151 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.153 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00001-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
11:51:49.154 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.156 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00002-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
11:51:49.157 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.159 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00003-b731af61-07dd-42ee-9152-72aa64c3ccd5.c000
11:51:49.160 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.174 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/bdp_day=20190923/part-00000-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00000-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, Status:true
11:51:49.183 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00000-11269de4-04b4-4795-b146-2e84393028c1.c000
11:51:49.184 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.185 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00001-11269de4-04b4-4795-b146-2e84393028c1.c000
11:51:49.186 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/bdp_day=20190923/part-00001-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00001-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, Status:true
11:51:49.186 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.189 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00002-11269de4-04b4-4795-b146-2e84393028c1.c000
11:51:49.190 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.192 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00003-11269de4-04b4-4795-b146-2e84393028c1.c000
11:51:49.193 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:49.195 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/bdp_day=20190923/part-00002-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00002-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, Status:true
11:51:49.206 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/bdp_day=20190923/part-00003-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190923/part-00003-c11f577c-c021-40fb-b6d3-5f846f8b20bd.c000, Status:true
11:51:49.207 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/bdp_day=20190923/part-00000-8fdd4807-aac7-4310-9393-c2ceab869248.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00000-8fdd4807-aac7-4310-9393-c2ceab869248.c000, Status:true
11:51:49.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]
11:51:49.210 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190923]	
11:51:49.219 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/bdp_day=20190923/part-00001-8fdd4807-aac7-4310-9393-c2ceab869248.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00001-8fdd4807-aac7-4310-9393-c2ceab869248.c000, Status:true
11:51:49.229 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/bdp_day=20190923/part-00002-8fdd4807-aac7-4310-9393-c2ceab869248.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00002-8fdd4807-aac7-4310-9393-c2ceab869248.c000, Status:true
11:51:49.237 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/bdp_day=20190923/part-00003-8fdd4807-aac7-4310-9393-c2ceab869248.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190923/part-00003-8fdd4807-aac7-4310-9393-c2ceab869248.c000, Status:true
11:51:49.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_exposure
11:51:49.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_exposure	
11:51:49.238 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190923]
11:51:49.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]
11:51:49.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190923]	
11:51:49.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_click
11:51:49.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_click	
11:51:49.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190923]
11:51:49.281 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
11:51:49.283 WARN  [main] hive.log - Updated size to 2286732
11:51:49.306 WARN  [main] hive.log - Updating partition stats fast for: dw_release_click
11:51:49.309 WARN  [main] hive.log - Updated size to 1765246
11:51:49.502 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-43_898_6925006179150857869-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
11:51:49.507 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
11:51:49.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:49.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:49.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:49.555 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:49.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:49.571 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:49.586 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.586 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.587 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:49.603 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:49.604 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:49.604 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:49.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:49.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:49.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:49.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:49.724 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.724 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:49.731 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:49.732 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:49.732 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:49.732 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:49.732 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:49.732 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:49.732 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:49.732 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:49.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:49.759 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:49.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:49.765 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:49.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:49.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:49.814 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.814 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.814 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.814 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.814 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.814 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:49.816 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:49.817 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:49.817 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:49.817 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:49.826 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-43_929_8039980895895140752-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
11:51:49.848 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
11:51:49.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:49.849 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:49.856 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#391),(bdp_day#391 = 20190924)
11:51:49.857 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#384),(release_status#384 = 03)
11:51:49.857 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:49.857 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:49.857 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:49.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:49.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:49.879 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 311.6 KB, free 1987.1 MB)
11:51:49.890 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:49.890 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:49.893 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
11:51:49.894 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.5 MB)
11:51:49.895 INFO  [main] org.apache.spark.SparkContext - Created broadcast 22 from show at Dw03ReleaseExposure.scala:65
11:51:49.895 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:49.909 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:49.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 55 (show at Dw03ReleaseExposure.scala:65)
11:51:49.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 10 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
11:51:49.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (show at Dw03ReleaseExposure.scala:65)
11:51:49.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 20)
11:51:49.910 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 20)
11:51:49.911 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:49.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.913 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.914 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 17.6 KB, free 1987.1 MB)
11:51:49.915 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.1 MB)
11:51:49.917 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.917 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.917 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.918 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.918 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:49.918 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:49.919 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.32.1:7123 (size: 7.5 KB, free: 1988.5 MB)
11:51:49.921 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1004
11:51:49.921 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:49.922 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 8 tasks
11:51:49.925 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 58, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:51:49.926 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 59, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:51:49.926 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 20.0 (TID 60, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:51:49.926 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 20.0 (TID 61, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:51:49.927 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 20.0 (TID 62, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:51:49.927 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 20.0 (TID 63, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:51:49.927 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 20.0 (TID 64, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:51:49.927 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 20.0 (TID 65, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:51:49.928 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 59)
11:51:49.928 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 58)
11:51:49.929 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Running task 3.0 in stage 20.0 (TID 61)
11:51:49.929 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 2.0 in stage 20.0 (TID 60)
11:51:49.929 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Running task 4.0 in stage 20.0 (TID 62)
11:51:49.934 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:49.935 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:49.935 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:49.929 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Running task 7.0 in stage 20.0 (TID 65)
11:51:49.931 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Running task 5.0 in stage 20.0 (TID 63)
11:51:49.932 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Running task 6.0 in stage 20.0 (TID 64)
11:51:49.933 INFO  [Executor task launch worker for task 59] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:51:49.934 INFO  [Executor task launch worker for task 61] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:51:49.939 INFO  [Executor task launch worker for task 62] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:51:49.940 INFO  [Executor task launch worker for task 58] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:51:49.942 INFO  [Executor task launch worker for task 65] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:51:49.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:49.948 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:49.942 INFO  [Executor task launch worker for task 60] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:51:49.946 INFO  [Executor task launch worker for task 63] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:51:49.950 INFO  [Executor task launch worker for task 64] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:51:49.966 INFO  [Executor task launch worker for task 59] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:49.976 INFO  [Executor task launch worker for task 63] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:49.976 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 59). 1371 bytes result sent to driver
11:51:49.978 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 59) in 53 ms on localhost (executor driver) (1/8)
11:51:49.983 INFO  [Executor task launch worker for task 64] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:49.985 INFO  [Executor task launch worker for task 61] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:49.987 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Finished task 5.0 in stage 20.0 (TID 63). 1371 bytes result sent to driver
11:51:49.987 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 20.0 (TID 63) in 60 ms on localhost (executor driver) (2/8)
11:51:49.994 INFO  [Executor task launch worker for task 62] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:49.997 INFO  [Executor task launch worker for task 65] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:50.006 INFO  [Executor task launch worker for task 58] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:50.014 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Finished task 4.0 in stage 20.0 (TID 62). 1328 bytes result sent to driver
11:51:50.014 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 20.0 (TID 62) in 88 ms on localhost (executor driver) (3/8)
11:51:50.017 INFO  [Executor task launch worker for task 60] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:50.017 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Finished task 7.0 in stage 20.0 (TID 65). 1371 bytes result sent to driver
11:51:50.018 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 20.0 (TID 65) in 91 ms on localhost (executor driver) (4/8)
11:51:50.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:50.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:50.021 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Finished task 6.0 in stage 20.0 (TID 64). 1371 bytes result sent to driver
11:51:50.022 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 20.0 (TID 64) in 95 ms on localhost (executor driver) (5/8)
11:51:50.025 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 58). 1328 bytes result sent to driver
11:51:50.026 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 58) in 104 ms on localhost (executor driver) (6/8)
11:51:50.029 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 2.0 in stage 20.0 (TID 60). 1371 bytes result sent to driver
11:51:50.030 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 20.0 (TID 60) in 104 ms on localhost (executor driver) (7/8)
11:51:50.040 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.040 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:50.050 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:50.050 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:50.050 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:50.050 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:50.051 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:50.051 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:50.051 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:50.051 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:50.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:50.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:50.100 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:50.100 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:50.117 INFO  [Executor task launch worker for task 40] org.apache.spark.executor.Executor - Finished task 3.0 in stage 16.0 (TID 40). 1739 bytes result sent to driver
11:51:50.118 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 16.0 (TID 40) in 4836 ms on localhost (executor driver) (8/8)
11:51:50.119 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 16.0, whose tasks have all completed, from pool 
11:51:50.119 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 16 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 4.838 s
11:51:50.119 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:50.119 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:50.119 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 17)
11:51:50.119 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:50.120 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 17 (MapPartitionsRDD[54] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:50.121 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:51:50.123 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1988.0 MB)
11:51:50.124 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_18_piece0 in memory on 192.168.32.1:7247 (size: 2.3 KB, free: 1988.6 MB)
11:51:50.124 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 18 from broadcast at DAGScheduler.scala:1004
11:51:50.125 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[54] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:50.125 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 17.0 with 1 tasks
11:51:50.126 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:50.126 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:50.126 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 17.0 (TID 45, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:50.127 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Running task 0.0 in stage 17.0 (TID 45)
11:51:50.129 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:50.129 INFO  [Executor task launch worker for task 45] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:50.133 INFO  [Executor task launch worker for task 45] org.apache.spark.executor.Executor - Finished task 0.0 in stage 17.0 (TID 45). 5958 bytes result sent to driver
11:51:50.133 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 17.0 (TID 45) in 7 ms on localhost (executor driver) (1/1)
11:51:50.134 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 17.0, whose tasks have all completed, from pool 
11:51:50.134 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 17 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.008 s
11:51:50.134 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 8 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 4.863186 s
11:51:50.144 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:51:50.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:50.145 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.165 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:50.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:50.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:50.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:50.172 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.173 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.174 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.174 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.174 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:50.179 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1
11:51:50.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:50.217 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:50.219 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#391),(bdp_day#391 = 20190924)
11:51:50.219 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#384),(release_status#384 = 04)
11:51:50.220 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:50.220 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:50.220 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:50.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:50.230 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:50.236 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 311.6 KB, free 1987.1 MB)
11:51:50.253 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
11:51:50.254 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.5 MB)
11:51:50.255 INFO  [main] org.apache.spark.SparkContext - Created broadcast 22 from show at Dw04ReleaseClick.scala:66
11:51:50.255 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:50.257 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:50.257 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:50.269 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:50.270 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 55 (show at Dw04ReleaseClick.scala:66)
11:51:50.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 10 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
11:51:50.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (show at Dw04ReleaseClick.scala:66)
11:51:50.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 20)
11:51:50.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 20)
11:51:50.271 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:50.273 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 17.6 KB, free 1987.1 MB)
11:51:50.275 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1987.1 MB)
11:51:50.276 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.32.1:7345 (size: 7.5 KB, free: 1988.5 MB)
11:51:50.276 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1004
11:51:50.277 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:50.277 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 8 tasks
11:51:50.278 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 58, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:51:50.278 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 59, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:51:50.278 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 20.0 (TID 60, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:51:50.279 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 20.0 (TID 61, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:51:50.279 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 20.0 (TID 62, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:51:50.279 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 20.0 (TID 63, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:51:50.280 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 20.0 (TID 64, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:51:50.280 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 20.0 (TID 65, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:51:50.280 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 58)
11:51:50.280 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 2.0 in stage 20.0 (TID 60)
11:51:50.280 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Running task 3.0 in stage 20.0 (TID 61)
11:51:50.280 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 59)
11:51:50.280 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Running task 4.0 in stage 20.0 (TID 62)
11:51:50.280 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Running task 5.0 in stage 20.0 (TID 63)
11:51:50.280 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Running task 6.0 in stage 20.0 (TID 64)
11:51:50.280 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Running task 7.0 in stage 20.0 (TID 65)
11:51:50.284 INFO  [Executor task launch worker for task 59] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:51:50.285 INFO  [Executor task launch worker for task 63] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:51:50.285 INFO  [Executor task launch worker for task 64] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:51:50.286 INFO  [Executor task launch worker for task 61] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:51:50.287 INFO  [Executor task launch worker for task 62] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:51:50.287 INFO  [Executor task launch worker for task 60] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:51:50.288 INFO  [Executor task launch worker for task 65] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:51:50.288 INFO  [Executor task launch worker for task 58] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:51:50.303 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:50.304 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:50.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:50.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:50.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:50.306 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:50.309 INFO  [Executor task launch worker for task 63] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.322 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Finished task 5.0 in stage 20.0 (TID 63). 1371 bytes result sent to driver
11:51:50.322 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 20.0 (TID 63) in 43 ms on localhost (executor driver) (1/8)
11:51:50.323 INFO  [Executor task launch worker for task 62] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.331 INFO  [Executor task launch worker for task 59] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.341 INFO  [Executor task launch worker for task 58] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.341 INFO  [Executor task launch worker for task 60] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.342 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Finished task 4.0 in stage 20.0 (TID 62). 1371 bytes result sent to driver
11:51:50.344 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 20.0 (TID 62) in 65 ms on localhost (executor driver) (2/8)
11:51:50.346 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 59). 1371 bytes result sent to driver
11:51:50.346 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 59) in 68 ms on localhost (executor driver) (3/8)
11:51:50.352 INFO  [Executor task launch worker for task 65] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.352 INFO  [Executor task launch worker for task 64] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.359 INFO  [Executor task launch worker for task 61] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:50.369 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Finished task 7.0 in stage 20.0 (TID 65). 1328 bytes result sent to driver
11:51:50.370 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 20.0 (TID 65) in 90 ms on localhost (executor driver) (4/8)
11:51:50.372 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 2.0 in stage 20.0 (TID 60). 1328 bytes result sent to driver
11:51:50.373 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 20.0 (TID 60) in 95 ms on localhost (executor driver) (5/8)
11:51:50.377 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Finished task 6.0 in stage 20.0 (TID 64). 1371 bytes result sent to driver
11:51:50.378 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 20.0 (TID 64) in 99 ms on localhost (executor driver) (6/8)
11:51:50.381 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 58). 1371 bytes result sent to driver
11:51:50.381 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 58) in 103 ms on localhost (executor driver) (7/8)
11:51:50.401 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#331),(bdp_day#331 = 20190923)
11:51:50.401 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#324),(release_status#324 = 06)
11:51:50.401 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:50.401 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:50.402 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:50.414 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:50.415 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:50.425 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19 stored as values in memory (estimated size 312.0 KB, free 1987.7 MB)
11:51:50.444 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_19_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.6 MB)
11:51:50.449 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_19_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:51:50.451 INFO  [main] org.apache.spark.SparkContext - Created broadcast 19 from insertInto at SparkHelper.scala:35
11:51:50.451 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:50.498 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:50.498 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 59 (insertInto at SparkHelper.scala:35)
11:51:50.498 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 9 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:50.498 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 19 (insertInto at SparkHelper.scala:35)
11:51:50.499 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 18)
11:51:50.499 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 18)
11:51:50.499 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 18 (MapPartitionsRDD[59] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:50.503 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20 stored as values in memory (estimated size 21.7 KB, free 1987.6 MB)
11:51:50.506 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_20_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.6 MB)
11:51:50.506 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_20_piece0 in memory on 192.168.32.1:7247 (size: 9.6 KB, free: 1988.6 MB)
11:51:50.507 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 20 from broadcast at DAGScheduler.scala:1004
11:51:50.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[59] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:50.507 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 18.0 with 8 tasks
11:51:50.508 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 18.0 (TID 46, localhost, executor driver, partition 0, ANY, 5381 bytes)
11:51:50.508 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 18.0 (TID 47, localhost, executor driver, partition 1, ANY, 5381 bytes)
11:51:50.508 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 18.0 (TID 48, localhost, executor driver, partition 2, ANY, 5381 bytes)
11:51:50.508 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 18.0 (TID 49, localhost, executor driver, partition 3, ANY, 5381 bytes)
11:51:50.508 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 18.0 (TID 50, localhost, executor driver, partition 4, ANY, 5381 bytes)
11:51:50.508 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 18.0 (TID 51, localhost, executor driver, partition 5, ANY, 5381 bytes)
11:51:50.509 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 18.0 (TID 52, localhost, executor driver, partition 6, ANY, 5381 bytes)
11:51:50.509 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 18.0 (TID 53, localhost, executor driver, partition 7, ANY, 5381 bytes)
11:51:50.509 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Running task 0.0 in stage 18.0 (TID 46)
11:51:50.509 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Running task 3.0 in stage 18.0 (TID 49)
11:51:50.509 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Running task 4.0 in stage 18.0 (TID 50)
11:51:50.509 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Running task 1.0 in stage 18.0 (TID 47)
11:51:50.510 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Running task 5.0 in stage 18.0 (TID 51)
11:51:50.510 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Running task 6.0 in stage 18.0 (TID 52)
11:51:50.510 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Running task 2.0 in stage 18.0 (TID 48)
11:51:50.510 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Running task 7.0 in stage 18.0 (TID 53)
11:51:50.514 INFO  [Executor task launch worker for task 46] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 0-5984398, partition values: [20190923]
11:51:50.515 INFO  [Executor task launch worker for task 47] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 5984398-11968796, partition values: [20190923]
11:51:50.516 INFO  [Executor task launch worker for task 49] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 17953194-23937592, partition values: [20190923]
11:51:50.517 INFO  [Executor task launch worker for task 48] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 11968796-17953194, partition values: [20190923]
11:51:50.517 INFO  [Executor task launch worker for task 51] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 29921990-35906388, partition values: [20190923]
11:51:50.517 INFO  [Executor task launch worker for task 50] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 23937592-29921990, partition values: [20190923]
11:51:50.517 INFO  [Executor task launch worker for task 52] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 35906388-41890786, partition values: [20190923]
11:51:50.517 INFO  [Executor task launch worker for task 53] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190923/11111111111, range: 41890786-43680880, partition values: [20190923]
11:51:50.561 INFO  [Executor task launch worker for task 51] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.564 INFO  [Executor task launch worker for task 49] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.594 INFO  [Executor task launch worker for task 51] org.apache.spark.executor.Executor - Finished task 5.0 in stage 18.0 (TID 51). 1567 bytes result sent to driver
11:51:50.595 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 18.0 (TID 51) in 87 ms on localhost (executor driver) (1/8)
11:51:50.608 INFO  [Executor task launch worker for task 46] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.619 INFO  [Executor task launch worker for task 46] org.apache.spark.executor.Executor - Finished task 0.0 in stage 18.0 (TID 46). 1524 bytes result sent to driver
11:51:50.620 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 18.0 (TID 46) in 112 ms on localhost (executor driver) (2/8)
11:51:50.692 INFO  [Executor task launch worker for task 47] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.707 INFO  [Executor task launch worker for task 50] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.709 INFO  [Executor task launch worker for task 52] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.721 INFO  [Executor task launch worker for task 53] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.724 INFO  [Executor task launch worker for task 47] org.apache.spark.executor.Executor - Finished task 1.0 in stage 18.0 (TID 47). 1524 bytes result sent to driver
11:51:50.725 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 18.0 (TID 47) in 217 ms on localhost (executor driver) (3/8)
11:51:50.729 INFO  [Executor task launch worker for task 50] org.apache.spark.executor.Executor - Finished task 4.0 in stage 18.0 (TID 50). 1481 bytes result sent to driver
11:51:50.729 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 18.0 (TID 50) in 221 ms on localhost (executor driver) (4/8)
11:51:50.731 INFO  [Executor task launch worker for task 48] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:51:50.731 INFO  [Executor task launch worker for task 52] org.apache.spark.executor.Executor - Finished task 6.0 in stage 18.0 (TID 52). 1481 bytes result sent to driver
11:51:50.732 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 18.0 (TID 52) in 223 ms on localhost (executor driver) (5/8)
11:51:50.735 INFO  [Executor task launch worker for task 53] org.apache.spark.executor.Executor - Finished task 7.0 in stage 18.0 (TID 53). 1524 bytes result sent to driver
11:51:50.736 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 18.0 (TID 53) in 227 ms on localhost (executor driver) (6/8)
11:51:50.741 INFO  [Executor task launch worker for task 48] org.apache.spark.executor.Executor - Finished task 2.0 in stage 18.0 (TID 48). 1524 bytes result sent to driver
11:51:50.741 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 18.0 (TID 48) in 233 ms on localhost (executor driver) (7/8)
11:51:51.536 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Finished task 3.0 in stage 20.0 (TID 61). 1543 bytes result sent to driver
11:51:51.537 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 20.0 (TID 61) in 1611 ms on localhost (executor driver) (8/8)
11:51:51.537 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
11:51:51.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 20 (show at Dw03ReleaseExposure.scala:65) finished in 1.615 s
11:51:51.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:51.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:51.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 21)
11:51:51.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:51.537 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[57] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:51.538 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1987.1 MB)
11:51:51.540 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.1 MB)
11:51:51.540 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.32.1:7123 (size: 2.3 KB, free: 1988.5 MB)
11:51:51.541 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1004
11:51:51.542 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[57] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
11:51:51.543 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks
11:51:51.543 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 66, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:51.544 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 66)
11:51:51.546 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:51.546 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:51.551 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 66). 4553 bytes result sent to driver
11:51:51.551 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 66) in 8 ms on localhost (executor driver) (1/1)
11:51:51.551 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
11:51:51.551 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (show at Dw03ReleaseExposure.scala:65) finished in 0.008 s
11:51:51.552 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 10 finished: show at Dw03ReleaseExposure.scala:65, took 1.642143 s
11:51:51.558 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
11:51:51.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:51.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:51.581 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.582 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:51.586 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1
11:51:51.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:51.629 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:51.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:51.635 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:51.656 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:51.656 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:51.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.678 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.679 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.679 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.679 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.679 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.679 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:51.680 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:51.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:51.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:51.681 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:51.717 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#391),(bdp_day#391 = 20190924)
11:51:51.718 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#384),(release_status#384 = 03)
11:51:51.718 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:51.718 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:51.719 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:51.722 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:51.722 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:51.731 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 311.6 KB, free 1986.8 MB)
11:51:51.744 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1986.7 MB)
11:51:51.745 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.5 MB)
11:51:51.745 INFO  [main] org.apache.spark.SparkContext - Created broadcast 25 from insertInto at SparkHelper.scala:35
11:51:51.746 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:51.782 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:51.783 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 61 (insertInto at SparkHelper.scala:35)
11:51:51.783 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 11 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:51.783 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (insertInto at SparkHelper.scala:35)
11:51:51.783 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 22)
11:51:51.783 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 22)
11:51:51.783 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 22 (MapPartitionsRDD[61] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:51.785 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 17.6 KB, free 1986.7 MB)
11:51:51.787 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1986.7 MB)
11:51:51.788 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 192.168.32.1:7123 (size: 7.5 KB, free: 1988.5 MB)
11:51:51.788 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1004
11:51:51.789 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[61] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:51.789 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 8 tasks
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 67, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 68, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 69, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 70, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 71, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 72, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 73, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:51:51.790 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 74, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:51:51.791 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 69)
11:51:51.791 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 68)
11:51:51.791 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 70)
11:51:51.791 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 67)
11:51:51.791 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 71)
11:51:51.791 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 73)
11:51:51.791 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 72)
11:51:51.791 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 74)
11:51:51.793 INFO  [Executor task launch worker for task 70] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:51:51.793 INFO  [Executor task launch worker for task 67] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:51:51.794 INFO  [Executor task launch worker for task 69] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:51:51.794 INFO  [Executor task launch worker for task 68] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:51:51.795 INFO  [Executor task launch worker for task 71] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:51:51.796 INFO  [Executor task launch worker for task 72] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:51:51.796 INFO  [Executor task launch worker for task 73] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:51:51.798 INFO  [Executor task launch worker for task 74] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:51:51.799 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Finished task 3.0 in stage 20.0 (TID 61). 1543 bytes result sent to driver
11:51:51.800 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 20.0 (TID 61) in 1522 ms on localhost (executor driver) (8/8)
11:51:51.800 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
11:51:51.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 20 (show at Dw04ReleaseClick.scala:66) finished in 1.522 s
11:51:51.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:51.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:51.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 21)
11:51:51.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:51.800 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[57] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:51.801 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1987.1 MB)
11:51:51.805 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1987.1 MB)
11:51:51.806 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.32.1:7345 (size: 2.3 KB, free: 1988.5 MB)
11:51:51.807 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1004
11:51:51.808 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[57] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:51.808 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks
11:51:51.808 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 66, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:51.809 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 66)
11:51:51.809 INFO  [Executor task launch worker for task 68] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.810 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:51.810 INFO  [Executor task launch worker for task 67] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.810 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:51.816 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 66). 4694 bytes result sent to driver
11:51:51.816 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 66) in 8 ms on localhost (executor driver) (1/1)
11:51:51.817 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
11:51:51.817 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (show at Dw04ReleaseClick.scala:66) finished in 0.009 s
11:51:51.817 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 10 finished: show at Dw04ReleaseClick.scala:66, took 1.547400 s
11:51:51.819 INFO  [Executor task launch worker for task 72] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.821 INFO  [Executor task launch worker for task 71] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.823 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 68). 1328 bytes result sent to driver
11:51:51.824 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 68) in 34 ms on localhost (executor driver) (1/8)
11:51:51.827 INFO  [Executor task launch worker for task 70] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.829 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 67). 1371 bytes result sent to driver
11:51:51.830 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 67) in 40 ms on localhost (executor driver) (2/8)
11:51:51.830 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
11:51:51.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:51.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:51.832 INFO  [Executor task launch worker for task 74] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.832 INFO  [Executor task launch worker for task 73] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.844 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 73). 1414 bytes result sent to driver
11:51:51.844 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 73) in 54 ms on localhost (executor driver) (3/8)
11:51:51.847 INFO  [Executor task launch worker for task 69] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"03"}))
11:51:51.848 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 71). 1414 bytes result sent to driver
11:51:51.848 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 71) in 58 ms on localhost (executor driver) (4/8)
11:51:51.852 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 72). 1371 bytes result sent to driver
11:51:51.853 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 72) in 63 ms on localhost (executor driver) (5/8)
11:51:51.856 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 74). 1328 bytes result sent to driver
11:51:51.857 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 74) in 67 ms on localhost (executor driver) (6/8)
11:51:51.859 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 69). 1371 bytes result sent to driver
11:51:51.860 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 69) in 70 ms on localhost (executor driver) (7/8)
11:51:51.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.868 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.869 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.869 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:51.872 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1
11:51:51.894 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:51.894 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:51.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:51.900 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:51.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:51.924 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:51.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.947 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:51.948 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:51.949 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:51.949 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:51.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:51.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:52.003 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#391),(bdp_day#391 = 20190924)
11:51:52.003 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#384),(release_status#384 = 04)
11:51:52.003 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:52.003 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:52.004 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:52.004 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 192.168.32.1:7345 in memory (size: 7.5 KB, free: 1988.5 MB)
11:51:52.004 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 334
11:51:52.004 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 384
11:51:52.004 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 392
11:51:52.004 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 446
11:51:52.004 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 386
11:51:52.004 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 444
11:51:52.005 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.5 MB)
11:51:52.006 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 329
11:51:52.006 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 394
11:51:52.006 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 445
11:51:52.007 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.32.1:7345 in memory (size: 62.1 KB, free: 1988.6 MB)
11:51:52.008 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:52.008 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 390
11:51:52.008 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:52.008 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 387
11:51:52.008 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 328
11:51:52.008 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 391
11:51:52.009 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 7
11:51:52.009 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 332
11:51:52.009 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 8
11:51:52.011 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 192.168.32.1:7345 in memory (size: 2.3 KB, free: 1988.6 MB)
11:51:52.013 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 388
11:51:52.013 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 452
11:51:52.013 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 333
11:51:52.013 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 389
11:51:52.013 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.32.1:7345 in memory (size: 2.3 KB, free: 1988.6 MB)
11:51:52.014 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 385
11:51:52.015 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 448
11:51:52.015 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 330
11:51:52.018 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.6 MB)
11:51:52.023 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 311.6 KB, free 1987.7 MB)
11:51:52.026 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 335
11:51:52.026 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 451
11:51:52.027 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 450
11:51:52.027 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 395
11:51:52.029 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 192.168.32.1:7345 in memory (size: 7.5 KB, free: 1988.6 MB)
11:51:52.029 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 449
11:51:52.029 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 447
11:51:52.030 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 393
11:51:52.031 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 192.168.32.1:7345 in memory (size: 26.2 KB, free: 1988.7 MB)
11:51:52.032 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 331
11:51:52.032 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 6
11:51:52.033 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.32.1:7345 in memory (size: 7.5 KB, free: 1988.7 MB)
11:51:52.040 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1988.0 MB)
11:51:52.040 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.6 MB)
11:51:52.041 INFO  [main] org.apache.spark.SparkContext - Created broadcast 25 from insertInto at SparkHelper.scala:35
11:51:52.041 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:52.080 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:52.081 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 61 (insertInto at SparkHelper.scala:35)
11:51:52.081 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 11 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:52.081 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (insertInto at SparkHelper.scala:35)
11:51:52.081 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 22)
11:51:52.081 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 22)
11:51:52.081 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 22 (MapPartitionsRDD[61] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:52.083 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 17.6 KB, free 1988.0 MB)
11:51:52.085 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1988.0 MB)
11:51:52.086 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 192.168.32.1:7345 (size: 7.5 KB, free: 1988.6 MB)
11:51:52.086 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1004
11:51:52.087 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[61] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:51:52.087 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 8 tasks
11:51:52.088 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 67, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:51:52.088 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 68, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:51:52.088 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 69, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:51:52.088 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 70, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:51:52.088 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 71, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:51:52.088 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 72, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:51:52.089 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 73, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:51:52.089 INFO  [dispatcher-event-loop-6] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 74, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:51:52.089 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 69)
11:51:52.089 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 71)
11:51:52.089 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 72)
11:51:52.089 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 74)
11:51:52.089 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 68)
11:51:52.089 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 67)
11:51:52.089 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 70)
11:51:52.090 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 73)
11:51:52.091 INFO  [Executor task launch worker for task 71] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:51:52.091 INFO  [Executor task launch worker for task 72] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:51:52.091 INFO  [Executor task launch worker for task 74] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:51:52.091 INFO  [Executor task launch worker for task 69] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:51:52.091 INFO  [Executor task launch worker for task 68] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:51:52.093 INFO  [Executor task launch worker for task 67] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:51:52.094 INFO  [Executor task launch worker for task 73] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:51:52.095 INFO  [Executor task launch worker for task 70] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:51:52.104 INFO  [Executor task launch worker for task 73] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.105 INFO  [Executor task launch worker for task 72] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.111 INFO  [Executor task launch worker for task 69] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.111 INFO  [Executor task launch worker for task 71] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.112 INFO  [Executor task launch worker for task 68] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.114 INFO  [Executor task launch worker for task 74] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.115 INFO  [Executor task launch worker for task 67] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.116 INFO  [Executor task launch worker for task 70] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"04"}))
11:51:52.122 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 72). 1328 bytes result sent to driver
11:51:52.123 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 72) in 35 ms on localhost (executor driver) (1/8)
11:51:52.126 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 71). 1371 bytes result sent to driver
11:51:52.127 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 71) in 39 ms on localhost (executor driver) (2/8)
11:51:52.130 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 69). 1371 bytes result sent to driver
11:51:52.130 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 69) in 42 ms on localhost (executor driver) (3/8)
11:51:52.134 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 73). 1328 bytes result sent to driver
11:51:52.134 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 73) in 45 ms on localhost (executor driver) (4/8)
11:51:52.138 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 67). 1371 bytes result sent to driver
11:51:52.138 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 67) in 51 ms on localhost (executor driver) (5/8)
11:51:52.141 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 68). 1414 bytes result sent to driver
11:51:52.141 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 68) in 53 ms on localhost (executor driver) (6/8)
11:51:52.145 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 74). 1328 bytes result sent to driver
11:51:52.145 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 74) in 56 ms on localhost (executor driver) (7/8)
11:51:52.881 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 70). 1543 bytes result sent to driver
11:51:52.881 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 70) in 1091 ms on localhost (executor driver) (8/8)
11:51:52.881 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
11:51:52.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 22 (insertInto at SparkHelper.scala:35) finished in 1.093 s
11:51:52.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:52.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:52.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 23)
11:51:52.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:52.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[63] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:52.905 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 167.4 KB, free 1986.6 MB)
11:51:52.907 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1986.5 MB)
11:51:52.908 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 192.168.32.1:7123 (size: 62.1 KB, free: 1988.4 MB)
11:51:52.908 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1004
11:51:52.908 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[63] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:52.908 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 4 tasks
11:51:52.908 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 75, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:52.909 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 76, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:52.909 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 77, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:52.909 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 78, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:52.909 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 77)
11:51:52.909 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 75)
11:51:52.909 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 76)
11:51:52.909 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 78)
11:51:52.923 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:52.923 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:52.923 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:52.923 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:52.923 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:52.923 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:52.926 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:52.926 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:52.932 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:52.933 INFO  [Executor task launch worker for task 78] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:52.933 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:52.933 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:52.933 INFO  [Executor task launch worker for task 75] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:52.933 INFO  [Executor task launch worker for task 77] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:52.934 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c8ceed0
11:51:52.935 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:52.935 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19b90e31
11:51:52.935 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115152_0023_m_000003_0/bdp_day=20190924/part-00003-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000
11:51:52.935 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:52.935 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:52.935 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:52.935 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115152_0023_m_000000_0/bdp_day=20190924/part-00000-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000
11:51:52.935 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:52.935 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:52.935 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:52.936 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:52.936 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:52.936 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7eb887bf
11:51:52.936 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:52.936 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:52.936 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:52.936 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:52.936 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:52.936 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:52.936 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:52.936 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:52.936 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115152_0023_m_000002_0/bdp_day=20190924/part-00002-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000
11:51:52.936 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:52.936 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:52.936 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:52.936 INFO  [Executor task launch worker for task 76] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:52.936 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:52.939 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14e6fb04
11:51:52.939 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:52.939 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115152_0023_m_000001_0/bdp_day=20190924/part-00001-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:52.939 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:52.966 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@31578e87
11:51:52.966 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@66c8f390
11:51:52.967 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@495cc716
11:51:52.967 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2a559302
11:51:53.013 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 447
11:51:53.015 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 192.168.32.1:7123 in memory (size: 2.3 KB, free: 1988.4 MB)
11:51:53.018 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 448
11:51:53.018 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 451
11:51:53.019 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 192.168.32.1:7123 in memory (size: 7.5 KB, free: 1988.4 MB)
11:51:53.020 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 501
11:51:53.020 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 450
11:51:53.021 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 445
11:51:53.023 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_23_piece0 on 192.168.32.1:7123 in memory (size: 7.5 KB, free: 1988.4 MB)
11:51:53.023 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 8
11:51:53.023 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 452
11:51:53.025 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.32.1:7123 in memory (size: 62.1 KB, free: 1988.5 MB)
11:51:53.028 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_22_piece0 on 192.168.32.1:7123 in memory (size: 26.2 KB, free: 1988.5 MB)
11:51:53.029 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 449
11:51:53.029 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 446
11:51:53.029 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 444
11:51:53.058 INFO  [Executor task launch worker for task 77] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 937,961
11:51:53.058 INFO  [Executor task launch worker for task 78] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,106
11:51:53.060 INFO  [Executor task launch worker for task 76] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,186
11:51:53.060 INFO  [Executor task launch worker for task 75] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 938,496
11:51:53.068 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 387,400B for [release_session] BINARY: 14,345 values, 387,323B raw, 387,323B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.068 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.068 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.068 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.069 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 143,501B for [device_num] BINARY: 14,345 values, 143,458B raw, 143,458B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.069 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,345 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:53.069 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.070 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:53.070 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,345 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:53.070 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:53.070 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,345 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.070 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.070 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.070 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,345 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:53.070 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:53.070 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.070 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 387,427B for [release_session] BINARY: 14,346 values, 387,350B raw, 387,350B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.074 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.074 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.074 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 3,658B for [device_type] BINARY: 14,346 values, 3,627B raw, 3,627B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:53.074 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:53.074 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 143,511B for [device_num] BINARY: 14,346 values, 143,468B raw, 143,468B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
11:51:53.075 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 3,657B for [device_type] BINARY: 14,346 values, 3,626B raw, 3,626B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 3 entries, 15B raw, 3B comp}
11:51:53.075 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.075 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 5,462B for [sources] BINARY: 14,346 values, 5,420B raw, 5,420B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 7 entries, 73B raw, 7B comp}
11:51:53.075 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:53.075 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 14,346 values, 12B raw, 12B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 6B raw, 1B comp}
11:51:53.075 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 19,819B for [ct] INT64: 14,346 values, 19,772B raw, 19,772B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1,357 entries, 10,856B raw, 1,357B comp}
11:51:53.109 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 70). 1543 bytes result sent to driver
11:51:53.110 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 70) in 1022 ms on localhost (executor driver) (8/8)
11:51:53.110 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
11:51:53.111 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 22 (insertInto at SparkHelper.scala:35) finished in 1.024 s
11:51:53.111 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:53.111 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:53.111 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 23)
11:51:53.111 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:53.111 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[63] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:53.136 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115152_0023_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/task_20190926115152_0023_m_000003
11:51:53.136 INFO  [Executor task launch worker for task 78] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115152_0023_m_000003_0: Committed
11:51:53.136 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115152_0023_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/task_20190926115152_0023_m_000000
11:51:53.136 INFO  [Executor task launch worker for task 75] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115152_0023_m_000000_0: Committed
11:51:53.137 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 75). 2525 bytes result sent to driver
11:51:53.138 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 75) in 230 ms on localhost (executor driver) (1/4)
11:51:53.152 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 78). 2525 bytes result sent to driver
11:51:53.153 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 78) in 244 ms on localhost (executor driver) (2/4)
11:51:53.157 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 167.4 KB, free 1987.9 MB)
11:51:53.160 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 62.0 KB, free 1987.8 MB)
11:51:53.161 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 192.168.32.1:7345 (size: 62.0 KB, free: 1988.6 MB)
11:51:53.161 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1004
11:51:53.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[63] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:53.161 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 4 tasks
11:51:53.161 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 75, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:53.162 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 76, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:53.162 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 77, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:53.162 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 78, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:53.162 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 75)
11:51:53.162 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 76)
11:51:53.162 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 78)
11:51:53.162 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 77)
11:51:53.182 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:53.182 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:53.182 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:53.182 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:53.182 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:53.182 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:53.183 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:53.183 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:53.192 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:53.192 INFO  [Executor task launch worker for task 75] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:53.192 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:53.192 INFO  [Executor task launch worker for task 77] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:53.195 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:53.195 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52e692b5
11:51:53.195 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@46f0a334
11:51:53.195 INFO  [Executor task launch worker for task 78] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:53.195 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:53.195 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115153_0023_m_000000_0/bdp_day=20190924/part-00000-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000
11:51:53.195 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:53.195 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115153_0023_m_000002_0/bdp_day=20190924/part-00002-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000
11:51:53.195 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:53.195 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:53.195 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:53.195 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:53.196 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:53.196 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:53.196 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:53.196 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:53.196 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:53.196 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:53.196 INFO  [Executor task launch worker for task 76] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:53.197 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4286b86d
11:51:53.197 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:53.197 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115153_0023_m_000003_0/bdp_day=20190924/part-00003-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:53.197 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:53.198 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1b31460
11:51:53.198 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:53.198 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115153_0023_m_000001_0/bdp_day=20190924/part-00001-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:53.198 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:53.208 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1bfdcdf1
11:51:53.210 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2e6afdb0
11:51:53.210 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5b49e34
11:51:53.211 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@70701ebd
11:51:53.254 INFO  [Executor task launch worker for task 77] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,120
11:51:53.255 INFO  [Executor task launch worker for task 75] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,377
11:51:53.256 INFO  [Executor task launch worker for task 76] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,391
11:51:53.258 INFO  [Executor task launch worker for task 78] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 714,893
11:51:53.264 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 290,389B for [release_session] BINARY: 10,752 values, 290,312B raw, 290,312B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.264 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.265 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.265 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.265 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 107,571B for [device_num] BINARY: 10,752 values, 107,528B raw, 107,528B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.265 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.265 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.265 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:53.265 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 2,750B for [device_type] BINARY: 10,752 values, 2,719B raw, 2,719B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:53.265 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.265 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:53.265 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 4,105B for [sources] BINARY: 10,752 values, 4,063B raw, 4,063B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:53.265 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.265 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.265 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,752 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.265 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,404 entries, 19,232B raw, 2,404B comp}
11:51:53.266 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:53.266 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:53.266 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.266 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 16,206B for [ct] INT64: 10,752 values, 16,159B raw, 16,159B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,420 entries, 19,360B raw, 2,420B comp}
11:51:53.266 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,412 entries, 19,296B raw, 2,412B comp}
11:51:53.268 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 290,416B for [release_session] BINARY: 10,753 values, 290,339B raw, 290,339B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.268 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.268 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 107,581B for [device_num] BINARY: 10,753 values, 107,538B raw, 107,538B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
11:51:53.269 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 2,752B for [device_type] BINARY: 10,753 values, 2,721B raw, 2,721B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:53.269 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 4,108B for [sources] BINARY: 10,753 values, 4,066B raw, 4,066B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:53.269 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 10,753 values, 12B raw, 12B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:53.269 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 16,218B for [ct] INT64: 10,753 values, 16,171B raw, 16,171B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2,431 entries, 19,448B raw, 2,431B comp}
11:51:53.364 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115153_0023_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/task_20190926115153_0023_m_000000
11:51:53.364 INFO  [Executor task launch worker for task 75] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115153_0023_m_000000_0: Committed
11:51:53.364 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 75). 2482 bytes result sent to driver
11:51:53.365 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 75) in 204 ms on localhost (executor driver) (1/4)
11:51:53.365 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115153_0023_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/task_20190926115153_0023_m_000002
11:51:53.366 INFO  [Executor task launch worker for task 77] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115153_0023_m_000002_0: Committed
11:51:53.366 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 77). 2482 bytes result sent to driver
11:51:53.366 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 77) in 204 ms on localhost (executor driver) (2/4)
11:51:53.553 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115152_0023_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/task_20190926115152_0023_m_000002
11:51:53.553 INFO  [Executor task launch worker for task 77] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115152_0023_m_000002_0: Committed
11:51:53.553 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 77). 2525 bytes result sent to driver
11:51:53.554 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 77) in 645 ms on localhost (executor driver) (3/4)
11:51:53.554 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115152_0023_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/_temporary/0/task_20190926115152_0023_m_000001
11:51:53.554 INFO  [Executor task launch worker for task 76] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115152_0023_m_000001_0: Committed
11:51:53.555 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 76). 2525 bytes result sent to driver
11:51:53.555 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 76) in 647 ms on localhost (executor driver) (4/4)
11:51:53.555 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
11:51:53.555 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (insertInto at SparkHelper.scala:35) finished in 0.647 s
11:51:53.555 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 11 finished: insertInto at SparkHelper.scala:35, took 1.773187 s
11:51:53.593 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:53.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:53.594 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:53.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:53.612 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:53.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.629 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.630 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:53.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:53.631 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:53.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:53.650 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:53.667 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
11:51:53.668 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
11:51:53.693 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00000-56422161-4456-42ce-91a2-2da4efa8049b.c000
11:51:53.694 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:53.696 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00001-56422161-4456-42ce-91a2-2da4efa8049b.c000
11:51:53.697 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:53.699 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00002-56422161-4456-42ce-91a2-2da4efa8049b.c000
11:51:53.700 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:53.702 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00003-56422161-4456-42ce-91a2-2da4efa8049b.c000
11:51:53.703 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:53.748 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/bdp_day=20190924/part-00000-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00000-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, Status:true
11:51:53.761 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/bdp_day=20190924/part-00001-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00001-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, Status:true
11:51:53.777 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115153_0023_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/task_20190926115153_0023_m_000003
11:51:53.777 INFO  [Executor task launch worker for task 78] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115153_0023_m_000003_0: Committed
11:51:53.778 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 78). 2482 bytes result sent to driver
11:51:53.778 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115153_0023_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/_temporary/0/task_20190926115153_0023_m_000001
11:51:53.778 INFO  [Executor task launch worker for task 76] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115153_0023_m_000001_0: Committed
11:51:53.779 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 76). 2482 bytes result sent to driver
11:51:53.779 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 78) in 617 ms on localhost (executor driver) (3/4)
11:51:53.780 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 76) in 618 ms on localhost (executor driver) (4/4)
11:51:53.780 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
11:51:53.780 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (insertInto at SparkHelper.scala:35) finished in 0.619 s
11:51:53.781 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 11 finished: insertInto at SparkHelper.scala:35, took 1.700673 s
11:51:53.790 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/bdp_day=20190924/part-00002-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00002-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, Status:true
11:51:53.805 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/bdp_day=20190924/part-00003-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_exposure/bdp_day=20190924/part-00003-b16d4ac9-5549-4e6d-bde9-6410c58c0137.c000, Status:true
11:51:53.811 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]
11:51:53.811 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_exposure[20190924]	
11:51:53.853 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:53.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:53.853 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:53.865 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 334
11:51:53.865 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 335
11:51:53.866 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 6
11:51:53.866 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 337
11:51:53.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_exposure
11:51:53.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_exposure	
11:51:53.866 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
11:51:53.882 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_17_piece0 on 192.168.32.1:7247 in memory (size: 9.6 KB, free: 1988.6 MB)
11:51:53.885 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 332
11:51:53.885 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 336
11:51:53.886 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:53.886 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:53.887 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_18_piece0 on 192.168.32.1:7247 in memory (size: 2.3 KB, free: 1988.6 MB)
11:51:53.888 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 387
11:51:53.888 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 331
11:51:53.892 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_16_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.6 MB)
11:51:53.895 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 333
11:51:53.895 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 338
11:51:53.914 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.914 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.915 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.916 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.916 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:53.916 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:53.919 WARN  [main] hive.log - Updating partition stats fast for: dw_release_exposure
11:51:53.919 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:53.919 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:53.921 WARN  [main] hive.log - Updated size to 2286733
11:51:53.943 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.943 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:53.944 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:53.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]
11:51:53.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]	
11:51:53.985 INFO  [Executor task launch worker for task 49] org.apache.spark.executor.Executor - Finished task 3.0 in stage 18.0 (TID 49). 1782 bytes result sent to driver
11:51:53.985 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 18.0 (TID 49) in 3477 ms on localhost (executor driver) (8/8)
11:51:53.986 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 18.0, whose tasks have all completed, from pool 
11:51:53.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 18 (insertInto at SparkHelper.scala:35) finished in 3.478 s
11:51:53.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:51:53.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:51:53.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 19)
11:51:53.986 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:51:53.987 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 19 (MapPartitionsRDD[62] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:54.015 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
11:51:54.017 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_21_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
11:51:54.018 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_21_piece0 in memory on 192.168.32.1:7247 (size: 64.1 KB, free: 1988.6 MB)
11:51:54.018 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 21 from broadcast at DAGScheduler.scala:1004
11:51:54.019 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 19 (MapPartitionsRDD[62] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:54.019 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 19.0 with 4 tasks
11:51:54.019 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 19.0 (TID 54, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:51:54.019 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 19.0 (TID 55, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:51:54.019 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 19.0 (TID 56, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:51:54.020 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 19.0 (TID 57, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:51:54.020 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Running task 1.0 in stage 19.0 (TID 55)
11:51:54.020 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Running task 0.0 in stage 19.0 (TID 54)
11:51:54.020 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Running task 3.0 in stage 19.0 (TID 57)
11:51:54.020 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Running task 2.0 in stage 19.0 (TID 56)
11:51:54.036 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:54.037 INFO  [Executor task launch worker for task 57] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:54.037 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:54.037 INFO  [Executor task launch worker for task 55] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.037 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:54.038 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:51:54.038 INFO  [Executor task launch worker for task 54] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.038 INFO  [Executor task launch worker for task 56] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:54.051 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.051 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.051 INFO  [Executor task launch worker for task 54] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.051 INFO  [Executor task launch worker for task 55] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.053 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.053 INFO  [Executor task launch worker for task 57] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.057 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.057 INFO  [Executor task launch worker for task 56] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.057 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@68366c9f
11:51:54.057 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32a043f2
11:51:54.057 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5de5007a
11:51:54.057 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:54.057 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:54.057 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:54.057 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115154_0019_m_000003_0/bdp_day=20190923/part-00003-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000
11:51:54.057 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115154_0019_m_000001_0/bdp_day=20190923/part-00001-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000
11:51:54.057 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115154_0019_m_000000_0/bdp_day=20190923/part-00000-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000
11:51:54.058 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:54.058 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:54.058 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:54.058 INFO  [Executor task launch worker for task 55] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:54.058 INFO  [Executor task launch worker for task 57] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:54.059 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/bdp_day=20190924/part-00000-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00000-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, Status:true
11:51:54.058 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:54.058 INFO  [Executor task launch worker for task 54] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:54.058 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:54.060 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3eefe68
11:51:54.061 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:54.061 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:54.061 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:54.061 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:54.061 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:54.061 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:54.061 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:54.061 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:54.061 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:51:54.061 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:54.061 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:54.062 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:54.062 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115154_0019_m_000002_0/bdp_day=20190923/part-00002-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000
11:51:54.062 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:54.063 INFO  [Executor task launch worker for task 55] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:54.063 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:54.063 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression set to false
11:51:54.063 INFO  [Executor task launch worker for task 54] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:54.063 INFO  [Executor task launch worker for task 57] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:54.065 INFO  [Executor task launch worker for task 56] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:51:54.065 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:51:54.065 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:51:54.065 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:51:54.065 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:51:54.065 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Validation is off
11:51:54.065 INFO  [Executor task launch worker for task 56] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:51:54.098 INFO  [Executor task launch worker for task 55] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3717d6bd
11:51:54.103 INFO  [Executor task launch worker for task 54] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2cbfd097
11:51:54.107 INFO  [Executor task launch worker for task 56] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@484f3e8
11:51:54.111 INFO  [Executor task launch worker for task 57] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@5ee9f69e
11:51:54.156 INFO  [Executor task launch worker for task 55] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,756
11:51:54.163 INFO  [Executor task launch worker for task 56] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,684
11:51:54.166 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.166 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.168 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.170 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.170 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:54.171 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:54.172 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.172 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.172 INFO  [Executor task launch worker for task 55] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.172 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.174 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.174 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.175 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:54.175 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:54.175 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.175 INFO  [Executor task launch worker for task 56] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.183 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/bdp_day=20190924/part-00001-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00001-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, Status:true
11:51:54.185 INFO  [Executor task launch worker for task 57] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,692
11:51:54.188 INFO  [Executor task launch worker for task 54] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,830
11:51:54.205 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.206 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.206 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.206 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.208 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.208 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-51_586_5521445569351759270-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
11:51:54.208 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.213 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.213 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:54.214 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.216 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:54.217 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:51:54.218 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:51:54.218 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.219 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
11:51:54.219 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:54.219 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:51:54.219 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:54.219 INFO  [Executor task launch worker for task 57] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.220 INFO  [Executor task launch worker for task 54] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:51:54.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:54.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:54.245 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/bdp_day=20190924/part-00002-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00002-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, Status:true
11:51:54.246 INFO  [Executor task launch worker for task 55] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115154_0019_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/task_20190926115154_0019_m_000001
11:51:54.248 INFO  [Executor task launch worker for task 55] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115154_0019_m_000001_0: Committed
11:51:54.248 INFO  [Executor task launch worker for task 55] org.apache.spark.executor.Executor - Finished task 1.0 in stage 19.0 (TID 55). 2572 bytes result sent to driver
11:51:54.249 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 19.0 (TID 55) in 230 ms on localhost (executor driver) (1/4)
11:51:54.250 INFO  [Executor task launch worker for task 56] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115154_0019_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/task_20190926115154_0019_m_000002
11:51:54.250 INFO  [Executor task launch worker for task 56] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115154_0019_m_000002_0: Committed
11:51:54.250 INFO  [Executor task launch worker for task 56] org.apache.spark.executor.Executor - Finished task 2.0 in stage 19.0 (TID 56). 2572 bytes result sent to driver
11:51:54.252 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 19.0 (TID 56) in 233 ms on localhost (executor driver) (2/4)
11:51:54.261 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/bdp_day=20190924/part-00003-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_click/bdp_day=20190924/part-00003-05ebb50d-6b89-4ff5-8ffc-afee29f4371c.c000, Status:true
11:51:54.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]
11:51:54.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_click[20190924]	
11:51:54.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: append_partition : db=dw_release tbl=dw_release_click[20190924]
11:51:54.384 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=append_partition : db=dw_release tbl=dw_release_click[20190924]	
11:51:54.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:54.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:54.404 WARN  [main] hive.log - Updating partition stats fast for: dw_release_click
11:51:54.404 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.405 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.405 WARN  [main] hive.log - Updated size to 1765422
11:51:54.418 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:54.418 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:54.419 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:54.423 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.424 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.471 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.483 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.483 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.483 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.483 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.483 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.484 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.484 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.484 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.484 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.484 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:54.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:54.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:54.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:54.488 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:54.489 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:54.489 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:54.489 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:54.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:54.510 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:54.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.514 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.546 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.546 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.546 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.546 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.546 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.546 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.547 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.547 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.547 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.547 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:54.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:54.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:54.548 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:54.572 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#484),(bdp_day#484 = 20190925)
11:51:54.573 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#477),(release_status#477 = 03)
11:51:54.573 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:54.573 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:54.574 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:54.587 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 311.6 KB, free 1986.8 MB)
11:51:54.599 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1986.8 MB)
11:51:54.599 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.5 MB)
11:51:54.600 INFO  [main] org.apache.spark.SparkContext - Created broadcast 28 from show at Dw03ReleaseExposure.scala:65
11:51:54.600 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:54.608 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-51_872_68779626095696674-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
11:51:54.612 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:54.613 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
11:51:54.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 68 (show at Dw03ReleaseExposure.scala:65)
11:51:54.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 12 (show at Dw03ReleaseExposure.scala:65) with 1 output partitions
11:51:54.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (show at Dw03ReleaseExposure.scala:65)
11:51:54.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
11:51:54.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:54.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:54.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:54.613 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[70] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:54.615 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 1986.8 MB)
11:51:54.616 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1986.8 MB)
11:51:54.617 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 192.168.32.1:7123 (size: 2.2 KB, free: 1988.5 MB)
11:51:54.617 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1004
11:51:54.617 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[70] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(0))
11:51:54.618 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks
11:51:54.619 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 79, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:54.619 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:54.619 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:54.619 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 79)
11:51:54.621 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.622 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:54.622 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 79). 1182 bytes result sent to driver
11:51:54.623 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 79) in 5 ms on localhost (executor driver) (1/1)
11:51:54.623 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
11:51:54.624 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (show at Dw03ReleaseExposure.scala:65) finished in 0.006 s
11:51:54.624 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 12 finished: show at Dw03ReleaseExposure.scala:65, took 0.012381 s
11:51:54.629 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw03ReleaseExposure.scala:65
11:51:54.630 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 13 (show at Dw03ReleaseExposure.scala:65) with 3 output partitions
11:51:54.630 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (show at Dw03ReleaseExposure.scala:65)
11:51:54.630 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)
11:51:54.630 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:54.631 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[70] at show at Dw03ReleaseExposure.scala:65), which has no missing parents
11:51:54.633 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 1986.8 MB)
11:51:54.636 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1986.8 MB)
11:51:54.638 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 192.168.32.1:7123 (size: 2.2 KB, free: 1988.5 MB)
11:51:54.639 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1004
11:51:54.640 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[70] at show at Dw03ReleaseExposure.scala:65) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:54.640 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 3 tasks
11:51:54.643 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 80, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:54.644 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 81, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:54.645 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 82, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:54.645 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:54.645 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:54.646 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 81)
11:51:54.648 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 80)
11:51:54.648 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.649 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:54.650 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 82)
11:51:54.652 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.652 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.652 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.652 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.653 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 80). 1182 bytes result sent to driver
11:51:54.653 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 81). 1182 bytes result sent to driver
11:51:54.653 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 82). 1182 bytes result sent to driver
11:51:54.654 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 81) in 10 ms on localhost (executor driver) (1/3)
11:51:54.654 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 82) in 9 ms on localhost (executor driver) (2/3)
11:51:54.655 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 80) in 12 ms on localhost (executor driver) (3/3)
11:51:54.655 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool 
11:51:54.656 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (show at Dw03ReleaseExposure.scala:65) finished in 0.016 s
11:51:54.657 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 13 finished: show at Dw03ReleaseExposure.scala:65, took 0.027457 s
11:51:54.662 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_exposure
11:51:54.662 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:54.662 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:54.670 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.670 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.670 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.670 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.671 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.671 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.671 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.671 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.680 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.680 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.680 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.681 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.681 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.681 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.681 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.681 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.687 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_exposure/.hive-staging_hive_2019-09-26_11-51-54_687_3887009288805780759-1
11:51:54.689 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:54.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:54.690 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:54.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.698 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:54.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:54.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.715 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.725 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.741 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.742 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.743 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.748 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:54.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.748 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.749 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.749 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:54.749 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.749 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.749 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:54.749 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.749 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:54.749 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:54.749 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:54.750 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:54.750 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:54.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:54.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:54.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:54.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:54.775 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#484),(bdp_day#484 = 20190925)
11:51:54.775 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#477),(release_status#477 = 03)
11:51:54.775 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:54.775 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,03)
11:51:54.776 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:54.781 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.781 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:54.781 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:54.787 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.788 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.790 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 311.6 KB, free 1986.5 MB)
11:51:54.801 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:54.802 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:54.806 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1986.4 MB)
11:51:54.807 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 192.168.32.1:7123 (size: 26.2 KB, free: 1988.5 MB)
11:51:54.808 INFO  [main] org.apache.spark.SparkContext - Created broadcast 31 from insertInto at SparkHelper.scala:35
11:51:54.808 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:54.819 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.819 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.819 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.820 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:54.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:54.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:54.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:54.842 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#484),(bdp_day#484 = 20190925)
11:51:54.843 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#477),(release_status#477 = 04)
11:51:54.844 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:54.844 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:54.844 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:54.848 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:54.849 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 74 (insertInto at SparkHelper.scala:35)
11:51:54.849 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 14 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:54.849 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (insertInto at SparkHelper.scala:35)
11:51:54.849 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 28)
11:51:54.849 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:54.850 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[76] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:54.856 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 311.6 KB, free 1987.5 MB)
11:51:54.871 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.5 MB)
11:51:54.871 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.6 MB)
11:51:54.872 INFO  [main] org.apache.spark.SparkContext - Created broadcast 28 from show at Dw04ReleaseClick.scala:66
11:51:54.872 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:54.874 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 167.4 KB, free 1986.3 MB)
11:51:54.876 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 62.1 KB, free 1986.2 MB)
11:51:54.876 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 192.168.32.1:7123 (size: 62.1 KB, free: 1988.4 MB)
11:51:54.879 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1004
11:51:54.880 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[76] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:54.880 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 4 tasks
11:51:54.880 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 83, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:54.881 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 29.0 (TID 84, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:54.881 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 29.0 (TID 85, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:54.881 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 29.0 (TID 86, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:54.882 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 83)
11:51:54.882 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Running task 2.0 in stage 29.0 (TID 85)
11:51:54.884 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:54.884 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Running task 1.0 in stage 29.0 (TID 84)
11:51:54.886 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Running task 3.0 in stage 29.0 (TID 86)
11:51:54.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 68 (show at Dw04ReleaseClick.scala:66)
11:51:54.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 12 (show at Dw04ReleaseClick.scala:66) with 1 output partitions
11:51:54.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (show at Dw04ReleaseClick.scala:66)
11:51:54.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
11:51:54.897 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:54.898 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[70] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:54.899 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 1987.5 MB)
11:51:54.901 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.5 MB)
11:51:54.918 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.919 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:54.919 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 192.168.32.1:7345 (size: 2.2 KB, free: 1988.6 MB)
11:51:54.920 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.920 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.920 INFO  [Executor task launch worker for task 85] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.920 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.920 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.921 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1004
11:51:54.922 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[70] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(0))
11:51:54.922 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks
11:51:54.921 INFO  [Executor task launch worker for task 85] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.922 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 79, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:54.922 INFO  [Executor task launch worker for task 83] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.922 INFO  [Executor task launch worker for task 86] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.922 INFO  [Executor task launch worker for task 86] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.922 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 79)
11:51:54.922 INFO  [Executor task launch worker for task 83] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.926 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.926 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.925 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.926 INFO  [Executor task launch worker for task 85] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115154_0029_m_000002_0
11:51:54.926 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:54.928 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 79). 1182 bytes result sent to driver
11:51:54.928 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Finished task 2.0 in stage 29.0 (TID 85). 2412 bytes result sent to driver
11:51:54.929 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 79) in 7 ms on localhost (executor driver) (1/1)
11:51:54.929 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
11:51:54.929 INFO  [Executor task launch worker for task 86] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115154_0029_m_000003_0
11:51:54.930 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 29.0 (TID 85) in 49 ms on localhost (executor driver) (1/4)
11:51:54.930 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Finished task 3.0 in stage 29.0 (TID 86). 2455 bytes result sent to driver
11:51:54.930 INFO  [Executor task launch worker for task 84] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:54.930 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (show at Dw04ReleaseClick.scala:66) finished in 0.008 s
11:51:54.931 INFO  [Executor task launch worker for task 84] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:54.931 INFO  [Executor task launch worker for task 83] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115154_0029_m_000000_0
11:51:54.932 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 83). 2412 bytes result sent to driver
11:51:54.932 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 29.0 (TID 86) in 51 ms on localhost (executor driver) (2/4)
11:51:54.932 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 12 finished: show at Dw04ReleaseClick.scala:66, took 0.048635 s
11:51:54.933 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 83) in 53 ms on localhost (executor driver) (3/4)
11:51:54.936 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw04ReleaseClick.scala:66
11:51:54.937 INFO  [Executor task launch worker for task 84] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115154_0029_m_000001_0
11:51:54.937 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 13 (show at Dw04ReleaseClick.scala:66) with 3 output partitions
11:51:54.937 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (show at Dw04ReleaseClick.scala:66)
11:51:54.937 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)
11:51:54.937 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:54.937 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Finished task 1.0 in stage 29.0 (TID 84). 2412 bytes result sent to driver
11:51:54.937 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[70] at show at Dw04ReleaseClick.scala:66), which has no missing parents
11:51:54.938 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 29.0 (TID 84) in 57 ms on localhost (executor driver) (4/4)
11:51:54.938 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool 
11:51:54.938 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (insertInto at SparkHelper.scala:35) finished in 0.058 s
11:51:54.939 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 1987.5 MB)
11:51:54.939 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 14 finished: insertInto at SparkHelper.scala:35, took 0.090124 s
11:51:54.941 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1987.5 MB)
11:51:54.943 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 192.168.32.1:7345 (size: 2.2 KB, free: 1988.6 MB)
11:51:54.944 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1004
11:51:54.944 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[70] at show at Dw04ReleaseClick.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:51:54.945 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 3 tasks
11:51:54.945 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 80, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:54.945 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 81, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:54.945 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 82, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:54.946 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 80)
11:51:54.947 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.947 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.949 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 81)
11:51:54.949 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 80). 1139 bytes result sent to driver
11:51:54.949 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 82)
11:51:54.951 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.955 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 80) in 10 ms on localhost (executor driver) (1/3)
11:51:54.956 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
11:51:54.957 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:54.957 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:54.958 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 81). 1225 bytes result sent to driver
11:51:54.958 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 82). 1182 bytes result sent to driver
11:51:54.959 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 81) in 14 ms on localhost (executor driver) (2/3)
11:51:54.959 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 82) in 14 ms on localhost (executor driver) (3/3)
11:51:54.959 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool 
11:51:54.960 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (show at Dw04ReleaseClick.scala:66) finished in 0.015 s
11:51:54.960 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 13 finished: show at Dw04ReleaseClick.scala:66, took 0.024129 s
11:51:54.963 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:54.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:54.964 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:54.965 INFO  [Executor task launch worker for task 57] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115154_0019_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/task_20190926115154_0019_m_000003
11:51:54.965 INFO  [Executor task launch worker for task 57] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115154_0019_m_000003_0: Committed
11:51:54.966 INFO  [Executor task launch worker for task 57] org.apache.spark.executor.Executor - Finished task 3.0 in stage 19.0 (TID 57). 2572 bytes result sent to driver
11:51:54.966 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_click
11:51:54.966 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 19.0 (TID 57) in 946 ms on localhost (executor driver) (3/4)
11:51:54.967 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:54.968 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:54.986 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:54.987 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:54.987 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.988 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:54.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:54.995 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_click/.hive-staging_hive_2019-09-26_11-51-54_994_9028612523288244615-1
11:51:55.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:55.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:55.017 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:55.022 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:55.023 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:55.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:55.029 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:55.052 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:55.052 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:55.053 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:55.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:55.053 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:55.079 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.081 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:55.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:55.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:55.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:55.084 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:55.086 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_exposure`
11:51:55.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:55.087 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:55.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:55.094 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:55.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_exposure
11:51:55.120 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_exposure	
11:51:55.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.150 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:55.152 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#484),(bdp_day#484 = 20190925)
11:51:55.152 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#477),(release_status#477 = 04)
11:51:55.153 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 5 more fields>
11:51:55.153 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,04)
11:51:55.153 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:51:55.162 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:55.163 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:55.176 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 311.6 KB, free 1987.1 MB)
11:51:55.197 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 26.2 KB, free 1987.1 MB)
11:51:55.198 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 192.168.32.1:7345 (size: 26.2 KB, free: 1988.5 MB)
11:51:55.200 INFO  [main] org.apache.spark.SparkContext - Created broadcast 31 from insertInto at SparkHelper.scala:35
11:51:55.200 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:51:55.204 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:51:55.215 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@5df8a180{HTTP/1.1,[http/1.1]}{0.0.0.0:4042}
11:51:55.218 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4042
11:51:55.235 INFO  [dispatcher-event-loop-7] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:51:55.248 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:51:55.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 74 (insertInto at SparkHelper.scala:35)
11:51:55.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 14 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:51:55.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (insertInto at SparkHelper.scala:35)
11:51:55.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 28)
11:51:55.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:51:55.249 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[76] at insertInto at SparkHelper.scala:35), which has no missing parents
11:51:55.283 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 167.3 KB, free 1987.0 MB)
11:51:55.285 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 62.0 KB, free 1986.9 MB)
11:51:55.286 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 192.168.32.1:7345 (size: 62.0 KB, free: 1988.5 MB)
11:51:55.286 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1004
11:51:55.287 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[76] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:51:55.287 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 4 tasks
11:51:55.287 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 83, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:51:55.287 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 29.0 (TID 84, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:51:55.288 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 29.0 (TID 85, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:51:55.288 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 29.0 (TID 86, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:51:55.288 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Running task 2.0 in stage 29.0 (TID 85)
11:51:55.289 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 83)
11:51:55.289 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Running task 1.0 in stage 29.0 (TID 84)
11:51:55.291 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Running task 3.0 in stage 29.0 (TID 86)
11:51:55.306 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:55.306 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:55.308 INFO  [Executor task launch worker for task 85] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:55.308 INFO  [Executor task launch worker for task 85] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:55.319 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:55.319 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:55.321 INFO  [Executor task launch worker for task 83] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:55.321 INFO  [Executor task launch worker for task 83] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:55.324 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:55.325 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:51:55.327 INFO  [Executor task launch worker for task 86] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:55.327 INFO  [Executor task launch worker for task 86] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:55.329 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:51:55.329 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:51:55.333 INFO  [Executor task launch worker for task 84] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:51:55.333 INFO  [Executor task launch worker for task 84] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:51:55.342 INFO  [Executor task launch worker for task 85] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115155_0029_m_000002_0
11:51:55.343 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Finished task 2.0 in stage 29.0 (TID 85). 2498 bytes result sent to driver
11:51:55.344 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 29.0 (TID 85) in 57 ms on localhost (executor driver) (1/4)
11:51:55.346 INFO  [Executor task launch worker for task 84] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115155_0029_m_000001_0
11:51:55.346 INFO  [Executor task launch worker for task 86] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115155_0029_m_000003_0
11:51:55.346 INFO  [Executor task launch worker for task 83] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115155_0029_m_000000_0
11:51:55.347 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Finished task 1.0 in stage 29.0 (TID 84). 2412 bytes result sent to driver
11:51:55.348 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Finished task 3.0 in stage 29.0 (TID 86). 2369 bytes result sent to driver
11:51:55.348 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 83). 2412 bytes result sent to driver
11:51:55.349 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 29.0 (TID 84) in 62 ms on localhost (executor driver) (2/4)
11:51:55.350 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 29.0 (TID 86) in 62 ms on localhost (executor driver) (3/4)
11:51:55.352 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 83) in 65 ms on localhost (executor driver) (4/4)
11:51:55.353 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool 
11:51:55.353 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (insertInto at SparkHelper.scala:35) finished in 0.066 s
11:51:55.354 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 14 finished: insertInto at SparkHelper.scala:35, took 0.104985 s
11:51:55.379 INFO  [Executor task launch worker for task 54] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115154_0019_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/_temporary/0/task_20190926115154_0019_m_000000
11:51:55.379 INFO  [Executor task launch worker for task 54] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115154_0019_m_000000_0: Committed
11:51:55.379 INFO  [Executor task launch worker for task 54] org.apache.spark.executor.Executor - Finished task 0.0 in stage 19.0 (TID 54). 2572 bytes result sent to driver
11:51:55.380 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 19.0 (TID 54) in 1361 ms on localhost (executor driver) (4/4)
11:51:55.381 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 19.0, whose tasks have all completed, from pool 
11:51:55.381 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 19 (insertInto at SparkHelper.scala:35) finished in 1.362 s
11:51:55.382 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 9 finished: insertInto at SparkHelper.scala:35, took 4.882849 s
11:51:55.387 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:55.387 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:55.388 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:55.416 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:55.416 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:55.426 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:51:55.428 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:55.429 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:55.442 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.442 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.443 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.443 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.443 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.443 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.443 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.444 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:55.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:55.445 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:55.456 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:55.456 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:55.474 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:51:55.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:55.474 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:55.486 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.487 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:55.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:55.489 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:55.507 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_click`
11:51:55.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:55.508 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:55.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:55.520 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:55.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:55.536 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:55.536 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:55.536 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:55.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:55.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:55.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:51:55.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:51:55.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:55.537 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:55.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_click
11:51:55.547 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_click	
11:51:55.559 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]
11:51:55.560 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]	
11:51:55.568 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:55.569 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:55.578 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:51:55.579 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:51:55.579 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:51:55.581 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:51:55.582 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:51:55.589 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:51:55.589 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:51:55.589 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@7ee55e70{HTTP/1.1,[http/1.1]}{0.0.0.0:4044}
11:51:55.589 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00000-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:51:55.591 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:55.591 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-5b9822b7-8daf-4e0d-b168-e0cc89857bdd
11:51:55.592 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4044
11:51:55.593 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00001-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:51:55.594 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:55.596 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00002-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:51:55.597 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:55.600 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00003-23add2bd-3dfa-41ad-9cca-6390eb0c1f35.c000
11:51:55.602 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:51:55.606 INFO  [dispatcher-event-loop-5] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:51:55.696 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/bdp_day=20190923/part-00000-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00000-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, Status:true
11:51:55.705 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/bdp_day=20190923/part-00001-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00001-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, Status:true
11:51:55.715 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/bdp_day=20190923/part-00002-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00002-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, Status:true
11:51:55.724 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/bdp_day=20190923/part-00003-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190923/part-00003-971883f2-ed0e-417d-bd8e-0153fc6054cc.c000, Status:true
11:51:55.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]
11:51:55.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190923]	
11:51:55.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_register_users
11:51:55.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_register_users	
11:51:55.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190923]
11:51:55.783 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
11:51:55.785 WARN  [main] hive.log - Updated size to 872108
11:51:55.803 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:51:55.804 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:51:55.804 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:51:55.807 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:51:55.814 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:51:55.814 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:51:55.815 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-515d6dd7-c389-4cdb-91b4-67afb5193e7f
11:51:55.982 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-51-50_179_7415586668247868666-1/-ext-10000/bdp_day=20190923 with partSpec {bdp_day=20190923}
11:51:55.984 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:51:55.984 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:51:55.985 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:51:55.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:55.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:56.005 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:51:56.005 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:51:56.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.021 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.021 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.021 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:56.034 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:51:56.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:56.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:56.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:56.041 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:56.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:56.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.074 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:51:56.081 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:51:56.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:51:56.082 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:51:56.107 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:51:56.107 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:51:56.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:56.111 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:56.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:51:56.127 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:51:56.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.144 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.145 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:51:56.146 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:51:56.146 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:51:56.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:51:56.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:51:56.147 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:51:56.561 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#435),(bdp_day#435 = 20190924)
11:51:56.561 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#428),(release_status#428 = 06)
11:51:56.562 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:51:56.562 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:51:56.562 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:51:56.570 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22 stored as values in memory (estimated size 312.0 KB, free 1987.4 MB)
11:51:56.579 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_22_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.4 MB)
11:51:56.579 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_22_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.5 MB)
11:51:56.580 INFO  [main] org.apache.spark.SparkContext - Created broadcast 22 from show at Dw05ReleaseRegisterUsers.scala:66
11:51:56.580 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:51:56.590 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:51:56.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 68 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:56.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 10 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:51:56.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 21 (show at Dw05ReleaseRegisterUsers.scala:66)
11:51:56.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 20)
11:51:56.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 20)
11:51:56.591 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 20 (MapPartitionsRDD[68] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:51:56.597 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23 stored as values in memory (estimated size 21.7 KB, free 1987.4 MB)
11:51:56.598 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_23_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1987.4 MB)
11:51:56.599 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_23_piece0 in memory on 192.168.32.1:7247 (size: 9.6 KB, free: 1988.5 MB)
11:51:56.599 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 23 from broadcast at DAGScheduler.scala:1004
11:51:56.600 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[68] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:52:00.928 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 20.0 with 8 tasks
11:52:00.929 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 20.0 (TID 58, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:52:00.930 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 20.0 (TID 59, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:52:00.930 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 20.0 (TID 60, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:52:00.930 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 20.0 (TID 61, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:52:00.930 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 20.0 (TID 62, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:52:00.931 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 20.0 (TID 63, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:52:00.931 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 20.0 (TID 64, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:52:00.931 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 20.0 (TID 65, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:52:00.931 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Running task 0.0 in stage 20.0 (TID 58)
11:52:00.931 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Running task 1.0 in stage 20.0 (TID 59)
11:52:00.932 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Running task 2.0 in stage 20.0 (TID 60)
11:52:00.932 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Running task 3.0 in stage 20.0 (TID 61)
11:52:00.932 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Running task 5.0 in stage 20.0 (TID 63)
11:52:00.932 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Running task 6.0 in stage 20.0 (TID 64)
11:52:00.932 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Running task 4.0 in stage 20.0 (TID 62)
11:52:00.933 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Running task 7.0 in stage 20.0 (TID 65)
11:52:00.939 INFO  [Executor task launch worker for task 59] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:52:00.940 INFO  [Executor task launch worker for task 58] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:52:00.944 INFO  [Executor task launch worker for task 61] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:52:00.945 INFO  [Executor task launch worker for task 62] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:52:00.951 INFO  [Executor task launch worker for task 65] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:52:00.959 INFO  [Executor task launch worker for task 60] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:52:00.963 INFO  [Executor task launch worker for task 64] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:52:00.964 INFO  [Executor task launch worker for task 63] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:52:01.003 INFO  [Executor task launch worker for task 65] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.004 INFO  [Executor task launch worker for task 60] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.032 INFO  [Executor task launch worker for task 64] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.033 INFO  [Executor task launch worker for task 59] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.037 INFO  [Executor task launch worker for task 60] org.apache.spark.executor.Executor - Finished task 2.0 in stage 20.0 (TID 60). 1481 bytes result sent to driver
11:52:01.038 INFO  [Executor task launch worker for task 61] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.039 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 20.0 (TID 60) in 109 ms on localhost (executor driver) (1/8)
11:52:01.040 INFO  [Executor task launch worker for task 63] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.043 INFO  [Executor task launch worker for task 64] org.apache.spark.executor.Executor - Finished task 6.0 in stage 20.0 (TID 64). 1481 bytes result sent to driver
11:52:01.045 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 20.0 (TID 64) in 113 ms on localhost (executor driver) (2/8)
11:52:01.047 INFO  [Executor task launch worker for task 59] org.apache.spark.executor.Executor - Finished task 1.0 in stage 20.0 (TID 59). 1524 bytes result sent to driver
11:52:01.049 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 20.0 (TID 59) in 119 ms on localhost (executor driver) (3/8)
11:52:01.052 INFO  [Executor task launch worker for task 63] org.apache.spark.executor.Executor - Finished task 5.0 in stage 20.0 (TID 63). 1567 bytes result sent to driver
11:52:01.053 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 20.0 (TID 63) in 123 ms on localhost (executor driver) (4/8)
11:52:01.054 INFO  [Executor task launch worker for task 62] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.054 INFO  [Executor task launch worker for task 58] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:01.065 INFO  [Executor task launch worker for task 62] org.apache.spark.executor.Executor - Finished task 4.0 in stage 20.0 (TID 62). 1567 bytes result sent to driver
11:52:01.066 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 20.0 (TID 62) in 136 ms on localhost (executor driver) (5/8)
11:52:01.069 INFO  [Executor task launch worker for task 58] org.apache.spark.executor.Executor - Finished task 0.0 in stage 20.0 (TID 58). 1524 bytes result sent to driver
11:52:01.070 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 20.0 (TID 58) in 142 ms on localhost (executor driver) (6/8)
11:52:01.089 INFO  [Executor task launch worker for task 65] org.apache.spark.executor.Executor - Finished task 7.0 in stage 20.0 (TID 65). 1524 bytes result sent to driver
11:52:01.089 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 20.0 (TID 65) in 158 ms on localhost (executor driver) (7/8)
11:52:02.466 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 219
11:52:02.468 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on 192.168.32.1:7247 in memory (size: 9.6 KB, free: 1988.5 MB)
11:52:02.468 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 389
11:52:02.468 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 391
11:52:02.468 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 395
11:52:02.468 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 392
11:52:02.468 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 398
11:52:02.470 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_21_piece0 on 192.168.32.1:7247 in memory (size: 64.1 KB, free: 1988.6 MB)
11:52:02.470 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 7
11:52:02.471 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 393
11:52:02.471 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 394
11:52:02.471 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 397
11:52:02.471 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 448
11:52:02.471 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 390
11:52:02.471 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 396
11:52:02.471 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 388
11:52:02.471 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_19_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.6 MB)
11:52:02.473 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_20_piece0 on 192.168.32.1:7247 in memory (size: 9.6 KB, free: 1988.6 MB)
11:52:02.473 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 399
11:52:02.473 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 216
11:52:02.474 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 4
11:52:02.475 INFO  [dispatcher-event-loop-7] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.7 MB)
11:52:02.476 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 217
11:52:02.476 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 220
11:52:02.476 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 213
11:52:02.476 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 218
11:52:02.476 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 215
11:52:02.477 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 214
11:52:02.526 INFO  [Executor task launch worker for task 61] org.apache.spark.executor.Executor - Finished task 3.0 in stage 20.0 (TID 61). 1782 bytes result sent to driver
11:52:02.527 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 20.0 (TID 61) in 1597 ms on localhost (executor driver) (8/8)
11:52:02.527 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 20.0, whose tasks have all completed, from pool 
11:52:02.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 20 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 1.599 s
11:52:02.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:52:02.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:52:02.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 21)
11:52:02.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:52:02.527 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 21 (MapPartitionsRDD[70] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:52:02.528 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1988.3 MB)
11:52:02.529 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1988.3 MB)
11:52:02.529 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_24_piece0 in memory on 192.168.32.1:7247 (size: 2.3 KB, free: 1988.7 MB)
11:52:02.530 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 24 from broadcast at DAGScheduler.scala:1004
11:52:02.531 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[70] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:52:02.531 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 21.0 with 1 tasks
11:52:02.531 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 21.0 (TID 66, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:52:02.531 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Running task 0.0 in stage 21.0 (TID 66)
11:52:02.533 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:52:02.533 INFO  [Executor task launch worker for task 66] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:02.535 INFO  [Executor task launch worker for task 66] org.apache.spark.executor.Executor - Finished task 0.0 in stage 21.0 (TID 66). 5843 bytes result sent to driver
11:52:02.535 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 21.0 (TID 66) in 4 ms on localhost (executor driver) (1/1)
11:52:02.535 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 21.0, whose tasks have all completed, from pool 
11:52:02.536 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 21 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.005 s
11:52:02.536 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 10 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 5.945337 s
11:52:02.542 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:52:02.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:02.543 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:02.554 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.555 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:02.558 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1
11:52:02.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:52:02.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:52:02.578 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:02.578 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:02.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:02.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:02.606 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:02.607 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:02.608 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:52:02.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:52:02.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:52:02.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:52:02.633 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#435),(bdp_day#435 = 20190924)
11:52:02.634 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#428),(release_status#428 = 06)
11:52:02.634 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:52:02.634 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:52:02.634 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
11:52:02.640 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:02.641 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:02.647 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
11:52:02.658 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_25_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:52:02.659 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_25_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:52:02.659 INFO  [main] org.apache.spark.SparkContext - Created broadcast 25 from insertInto at SparkHelper.scala:35
11:52:02.659 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5984398 bytes, open cost is considered as scanning 4194304 bytes.
11:52:02.692 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:52:02.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 75 (insertInto at SparkHelper.scala:35)
11:52:02.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 11 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:52:02.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 23 (insertInto at SparkHelper.scala:35)
11:52:02.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 22)
11:52:02.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 22)
11:52:02.693 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 22 (MapPartitionsRDD[75] at insertInto at SparkHelper.scala:35), which has no missing parents
11:52:02.697 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26 stored as values in memory (estimated size 21.7 KB, free 1988.0 MB)
11:52:02.699 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_26_piece0 stored as bytes in memory (estimated size 9.6 KB, free 1988.0 MB)
11:52:02.699 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_26_piece0 in memory on 192.168.32.1:7247 (size: 9.6 KB, free: 1988.6 MB)
11:52:02.699 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 26 from broadcast at DAGScheduler.scala:1004
11:52:02.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 8 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[75] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
11:52:02.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 22.0 with 8 tasks
11:52:02.701 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 22.0 (TID 67, localhost, executor driver, partition 0, ANY, 5391 bytes)
11:52:02.701 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 22.0 (TID 68, localhost, executor driver, partition 1, ANY, 5391 bytes)
11:52:02.701 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 22.0 (TID 69, localhost, executor driver, partition 2, ANY, 5391 bytes)
11:52:02.701 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 22.0 (TID 70, localhost, executor driver, partition 3, ANY, 5391 bytes)
11:52:02.701 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 22.0 (TID 71, localhost, executor driver, partition 4, ANY, 5391 bytes)
11:52:02.701 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 22.0 (TID 72, localhost, executor driver, partition 5, ANY, 5391 bytes)
11:52:02.701 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 22.0 (TID 73, localhost, executor driver, partition 6, ANY, 5391 bytes)
11:52:02.702 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 22.0 (TID 74, localhost, executor driver, partition 7, ANY, 5391 bytes)
11:52:02.702 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Running task 3.0 in stage 22.0 (TID 70)
11:52:02.702 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Running task 1.0 in stage 22.0 (TID 68)
11:52:02.702 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Running task 5.0 in stage 22.0 (TID 72)
11:52:02.702 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Running task 0.0 in stage 22.0 (TID 67)
11:52:02.702 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Running task 2.0 in stage 22.0 (TID 69)
11:52:02.702 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Running task 4.0 in stage 22.0 (TID 71)
11:52:02.702 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Running task 6.0 in stage 22.0 (TID 73)
11:52:02.702 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Running task 7.0 in stage 22.0 (TID 74)
11:52:02.705 INFO  [Executor task launch worker for task 72] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 29921990-35906388, partition values: [20190924]
11:52:02.705 INFO  [Executor task launch worker for task 68] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 5984398-11968796, partition values: [20190924]
11:52:02.705 INFO  [Executor task launch worker for task 71] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 23937592-29921990, partition values: [20190924]
11:52:02.706 INFO  [Executor task launch worker for task 70] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 17953194-23937592, partition values: [20190924]
11:52:02.706 INFO  [Executor task launch worker for task 69] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 11968796-17953194, partition values: [20190924]
11:52:02.706 INFO  [Executor task launch worker for task 73] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 35906388-41890786, partition values: [20190924]
11:52:02.706 INFO  [Executor task launch worker for task 67] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 0-5984398, partition values: [20190924]
11:52:02.706 INFO  [Executor task launch worker for task 74] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://hadoop102:9000/data/release/ods/release_session/bdp_day=20190924/15603638400003h4gka4h, range: 41890786-43680880, partition values: [20190924]
11:52:02.719 INFO  [Executor task launch worker for task 70] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.720 INFO  [Executor task launch worker for task 71] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.721 INFO  [Executor task launch worker for task 69] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.723 INFO  [Executor task launch worker for task 74] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.725 INFO  [Executor task launch worker for task 73] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.729 INFO  [Executor task launch worker for task 67] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.729 INFO  [Executor task launch worker for task 68] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.730 INFO  [Executor task launch worker for task 72] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"06"}))
11:52:02.741 INFO  [Executor task launch worker for task 73] org.apache.spark.executor.Executor - Finished task 6.0 in stage 22.0 (TID 73). 1481 bytes result sent to driver
11:52:02.743 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 22.0 (TID 73) in 42 ms on localhost (executor driver) (1/8)
11:52:02.745 INFO  [Executor task launch worker for task 68] org.apache.spark.executor.Executor - Finished task 1.0 in stage 22.0 (TID 68). 1481 bytes result sent to driver
11:52:02.746 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 22.0 (TID 68) in 45 ms on localhost (executor driver) (2/8)
11:52:02.750 INFO  [Executor task launch worker for task 72] org.apache.spark.executor.Executor - Finished task 5.0 in stage 22.0 (TID 72). 1524 bytes result sent to driver
11:52:02.751 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 22.0 (TID 72) in 50 ms on localhost (executor driver) (3/8)
11:52:02.754 INFO  [Executor task launch worker for task 67] org.apache.spark.executor.Executor - Finished task 0.0 in stage 22.0 (TID 67). 1481 bytes result sent to driver
11:52:02.755 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 22.0 (TID 67) in 54 ms on localhost (executor driver) (4/8)
11:52:02.758 INFO  [Executor task launch worker for task 74] org.apache.spark.executor.Executor - Finished task 7.0 in stage 22.0 (TID 74). 1524 bytes result sent to driver
11:52:02.759 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 22.0 (TID 74) in 58 ms on localhost (executor driver) (5/8)
11:52:02.762 INFO  [Executor task launch worker for task 71] org.apache.spark.executor.Executor - Finished task 4.0 in stage 22.0 (TID 71). 1481 bytes result sent to driver
11:52:02.762 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 22.0 (TID 71) in 61 ms on localhost (executor driver) (6/8)
11:52:02.766 INFO  [Executor task launch worker for task 69] org.apache.spark.executor.Executor - Finished task 2.0 in stage 22.0 (TID 69). 1567 bytes result sent to driver
11:52:02.766 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 22.0 (TID 69) in 65 ms on localhost (executor driver) (7/8)
11:52:03.856 INFO  [Executor task launch worker for task 70] org.apache.spark.executor.Executor - Finished task 3.0 in stage 22.0 (TID 70). 1696 bytes result sent to driver
11:52:03.856 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 22.0 (TID 70) in 1155 ms on localhost (executor driver) (8/8)
11:52:03.856 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 22.0, whose tasks have all completed, from pool 
11:52:03.856 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 22 (insertInto at SparkHelper.scala:35) finished in 1.155 s
11:52:03.856 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
11:52:03.857 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
11:52:03.857 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 23)
11:52:03.857 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
11:52:03.857 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 23 (MapPartitionsRDD[78] at insertInto at SparkHelper.scala:35), which has no missing parents
11:52:03.880 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27 stored as values in memory (estimated size 173.0 KB, free 1987.8 MB)
11:52:03.881 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_27_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1987.7 MB)
11:52:03.882 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Added broadcast_27_piece0 in memory on 192.168.32.1:7247 (size: 64.1 KB, free: 1988.6 MB)
11:52:03.882 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 27 from broadcast at DAGScheduler.scala:1004
11:52:03.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[78] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:52:03.882 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 23.0 with 4 tasks
11:52:03.883 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 23.0 (TID 75, localhost, executor driver, partition 0, ANY, 4726 bytes)
11:52:03.883 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 23.0 (TID 76, localhost, executor driver, partition 1, ANY, 4726 bytes)
11:52:03.883 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 23.0 (TID 77, localhost, executor driver, partition 2, ANY, 4726 bytes)
11:52:03.883 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 23.0 (TID 78, localhost, executor driver, partition 3, ANY, 4726 bytes)
11:52:03.883 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Running task 0.0 in stage 23.0 (TID 75)
11:52:03.883 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Running task 3.0 in stage 23.0 (TID 78)
11:52:03.883 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Running task 1.0 in stage 23.0 (TID 76)
11:52:03.883 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Running task 2.0 in stage 23.0 (TID 77)
11:52:03.896 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:52:03.897 INFO  [Executor task launch worker for task 77] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:52:03.897 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:52:03.897 INFO  [Executor task launch worker for task 76] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:03.897 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:52:03.897 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 8 blocks
11:52:03.897 INFO  [Executor task launch worker for task 78] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:03.897 INFO  [Executor task launch worker for task 75] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:03.903 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:03.904 INFO  [Executor task launch worker for task 75] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:03.904 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:03.904 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:03.904 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:03.904 INFO  [Executor task launch worker for task 78] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:03.904 INFO  [Executor task launch worker for task 76] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:03.904 INFO  [Executor task launch worker for task 77] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:03.907 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2843cb1c
11:52:03.907 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@63faa0e8
11:52:03.907 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51e28861
11:52:03.908 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:52:03.908 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:52:03.908 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:52:03.908 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115203_0023_m_000003_0/bdp_day=20190924/part-00003-0bde9499-d60c-4bce-9c62-33234bc204fd.c000
11:52:03.908 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115203_0023_m_000002_0/bdp_day=20190924/part-00002-0bde9499-d60c-4bce-9c62-33234bc204fd.c000
11:52:03.908 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@74a0a951
11:52:03.908 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115203_0023_m_000000_0/bdp_day=20190924/part-00000-0bde9499-d60c-4bce-9c62-33234bc204fd.c000
11:52:03.908 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression set to false
11:52:03.908 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression set to false
11:52:03.908 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
11:52:03.908 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression set to false
11:52:03.908 INFO  [Executor task launch worker for task 78] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:52:03.908 INFO  [Executor task launch worker for task 77] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:52:03.908 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/_temporary/attempt_20190926115203_0023_m_000001_0/bdp_day=20190924/part-00001-0bde9499-d60c-4bce-9c62-33234bc204fd.c000
11:52:03.908 INFO  [Executor task launch worker for task 75] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:52:03.908 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:52:03.908 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:52:03.909 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression set to false
11:52:03.909 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:52:03.909 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:52:03.909 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:52:03.909 INFO  [Executor task launch worker for task 76] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
11:52:03.909 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:52:03.909 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:52:03.909 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:52:03.909 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
11:52:03.909 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:52:03.909 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:52:03.909 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:52:03.909 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
11:52:03.909 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:52:03.910 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Validation is off
11:52:03.910 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Validation is off
11:52:03.910 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
11:52:03.910 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Validation is off
11:52:03.910 INFO  [Executor task launch worker for task 78] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:52:03.910 INFO  [Executor task launch worker for task 77] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:52:03.910 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Dictionary is on
11:52:03.910 INFO  [Executor task launch worker for task 75] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:52:03.910 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Validation is off
11:52:03.910 INFO  [Executor task launch worker for task 76] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
11:52:03.973 INFO  [Executor task launch worker for task 77] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@1c88290
11:52:03.974 INFO  [Executor task launch worker for task 75] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@21a46fd0
11:52:03.974 INFO  [Executor task launch worker for task 78] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@3783c7e8
11:52:03.974 INFO  [Executor task launch worker for task 76] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@77ea19d3
11:52:03.997 INFO  [Executor task launch worker for task 77] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,880
11:52:03.998 INFO  [Executor task launch worker for task 76] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,616
11:52:04.001 INFO  [Executor task launch worker for task 78] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 323,318
11:52:04.002 INFO  [Executor task launch worker for task 75] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 322,890
11:52:04.008 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 53,654B for [user_id] BINARY: 3,573 values, 53,602B raw, 53,602B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.008 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.009 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.009 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 96,554B for [release_session] BINARY: 3,573 values, 96,478B raw, 96,478B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.009 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.009 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 35,779B for [device_num] BINARY: 3,573 values, 35,737B raw, 35,737B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.009 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.010 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.010 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:52:04.010 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:52:04.011 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.011 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,573 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:52:04.011 INFO  [Executor task launch worker for task 76] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.011 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,573 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:52:04.012 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,573 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.012 INFO  [Executor task launch worker for task 77] parquet.hadoop.ColumnChunkPageWriteStore - written 28,637B for [ctime] INT64: 3,573 values, 28,591B raw, 28,591B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.013 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.013 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.013 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.013 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 53,669B for [user_id] BINARY: 3,574 values, 53,617B raw, 53,617B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.013 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.014 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:52:04.015 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:52:04.015 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 96,581B for [release_session] BINARY: 3,574 values, 96,505B raw, 96,505B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.015 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.018 INFO  [Executor task launch worker for task 78] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.019 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [release_status] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.020 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 35,789B for [device_num] BINARY: 3,574 values, 35,747B raw, 35,747B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.020 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 940B for [device_type] BINARY: 3,574 values, 910B raw, 910B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
11:52:04.020 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 1,398B for [sources] BINARY: 3,574 values, 1,357B raw, 1,357B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
11:52:04.020 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 40B for [channels] BINARY: 3,574 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
11:52:04.021 INFO  [Executor task launch worker for task 75] parquet.hadoop.ColumnChunkPageWriteStore - written 28,645B for [ctime] INT64: 3,574 values, 28,599B raw, 28,599B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
11:52:04.048 INFO  [Executor task launch worker for task 78] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115203_0023_m_000003_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/task_20190926115203_0023_m_000003
11:52:04.048 INFO  [Executor task launch worker for task 78] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115203_0023_m_000003_0: Committed
11:52:04.049 INFO  [Executor task launch worker for task 78] org.apache.spark.executor.Executor - Finished task 3.0 in stage 23.0 (TID 78). 2572 bytes result sent to driver
11:52:04.049 INFO  [Executor task launch worker for task 77] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115203_0023_m_000002_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/task_20190926115203_0023_m_000002
11:52:04.049 INFO  [Executor task launch worker for task 77] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115203_0023_m_000002_0: Committed
11:52:04.049 INFO  [Executor task launch worker for task 76] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115203_0023_m_000001_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/task_20190926115203_0023_m_000001
11:52:04.049 INFO  [Executor task launch worker for task 76] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115203_0023_m_000001_0: Committed
11:52:04.049 INFO  [Executor task launch worker for task 75] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190926115203_0023_m_000000_0' to hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/_temporary/0/task_20190926115203_0023_m_000000
11:52:04.049 INFO  [Executor task launch worker for task 75] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190926115203_0023_m_000000_0: Committed
11:52:04.049 INFO  [Executor task launch worker for task 77] org.apache.spark.executor.Executor - Finished task 2.0 in stage 23.0 (TID 77). 2572 bytes result sent to driver
11:52:04.050 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 23.0 (TID 78) in 166 ms on localhost (executor driver) (1/4)
11:52:04.050 INFO  [Executor task launch worker for task 76] org.apache.spark.executor.Executor - Finished task 1.0 in stage 23.0 (TID 76). 2572 bytes result sent to driver
11:52:04.050 INFO  [Executor task launch worker for task 75] org.apache.spark.executor.Executor - Finished task 0.0 in stage 23.0 (TID 75). 2572 bytes result sent to driver
11:52:04.050 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 23.0 (TID 77) in 167 ms on localhost (executor driver) (2/4)
11:52:04.050 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 23.0 (TID 76) in 167 ms on localhost (executor driver) (3/4)
11:52:04.050 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 23.0 (TID 75) in 167 ms on localhost (executor driver) (4/4)
11:52:04.051 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 23.0, whose tasks have all completed, from pool 
11:52:04.051 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 23 (insertInto at SparkHelper.scala:35) finished in 0.168 s
11:52:04.051 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 11 finished: insertInto at SparkHelper.scala:35, took 1.359710 s
11:52:04.196 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:52:04.196 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:04.196 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:04.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:04.211 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:04.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.226 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.227 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:04.228 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:04.228 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:04.246 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:52:04.246 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:52:04.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:52:04.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:52:04.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:52:04.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:52:04.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
11:52:04.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
11:52:04.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:04.248 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:04.262 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
11:52:04.263 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
11:52:04.284 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00000-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:52:04.284 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:52:04.286 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00001-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:52:04.289 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:52:04.291 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00002-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:52:04.292 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:52:04.294 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00003-cd0d57a1-8bc5-4dc8-8047-92f715be2e99.c000
11:52:04.294 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
11:52:04.309 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/bdp_day=20190924/part-00000-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00000-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, Status:true
11:52:04.318 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/bdp_day=20190924/part-00001-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00001-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, Status:true
11:52:04.325 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/bdp_day=20190924/part-00002-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00002-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, Status:true
11:52:04.335 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/bdp_day=20190924/part-00003-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, dest: hdfs://hadoop102:9000/data/release/dw/release_register_user/bdp_day=20190924/part-00003-0bde9499-d60c-4bce-9c62-33234bc204fd.c000, Status:true
11:52:04.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]
11:52:04.340 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dw_release tbl=dw_release_register_users[20190924]	
11:52:04.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dw_release tbl=dw_release_register_users
11:52:04.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=alter_partition : db=dw_release tbl=dw_release_register_users	
11:52:04.357 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[20190924]
11:52:04.386 WARN  [main] hive.log - Updating partition stats fast for: dw_release_register_users
11:52:04.388 WARN  [main] hive.log - Updated size to 872108
11:52:04.486 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-02_558_7552953835682093554-1/-ext-10000/bdp_day=20190924 with partSpec {bdp_day=20190924}
11:52:04.493 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:52:04.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:52:04.494 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:52:04.496 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_27_piece0 on 192.168.32.1:7247 in memory (size: 64.1 KB, free: 1988.6 MB)
11:52:04.497 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 507
11:52:04.498 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_25_piece0 on 192.168.32.1:7247 in memory (size: 26.3 KB, free: 1988.7 MB)
11:52:04.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:04.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:04.498 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 509
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 515
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 505
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 510
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 516
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 508
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 517
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 9
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 511
11:52:04.499 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 512
11:52:04.499 INFO  [dispatcher-event-loop-6] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_26_piece0 on 192.168.32.1:7247 in memory (size: 9.6 KB, free: 1988.7 MB)
11:52:04.500 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 514
11:52:04.502 INFO  [dispatcher-event-loop-5] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_24_piece0 on 192.168.32.1:7247 in memory (size: 2.3 KB, free: 1988.7 MB)
11:52:04.502 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 506
11:52:04.502 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 513
11:52:04.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:04.515 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:04.527 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.527 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.527 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.527 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.528 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:04.540 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
11:52:04.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:52:04.540 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:52:04.544 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:04.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:04.560 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:04.561 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.574 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:04.579 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$.user_register') as user_register
11:52:04.579 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
11:52:04.579 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
11:52:04.579 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
11:52:04.580 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
11:52:04.580 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
11:52:04.580 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
11:52:04.580 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
11:52:04.580 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
11:52:04.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
11:52:04.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: default	
11:52:04.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:52:04.605 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:52:04.609 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:04.610 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:04.625 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:04.625 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:04.638 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.639 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:04.640 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:52:04.641 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:52:04.641 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:52:04.641 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:52:04.660 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#539),(bdp_day#539 = 20190925)
11:52:04.661 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#532),(release_status#532 = 06)
11:52:04.661 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:52:04.661 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:52:04.662 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:52:04.673 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28 stored as values in memory (estimated size 312.0 KB, free 1988.0 MB)
11:52:04.689 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_28_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1988.0 MB)
11:52:04.690 INFO  [dispatcher-event-loop-4] org.apache.spark.storage.BlockManagerInfo - Added broadcast_28_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:52:04.690 INFO  [main] org.apache.spark.SparkContext - Created broadcast 28 from show at Dw05ReleaseRegisterUsers.scala:66
11:52:04.690 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:52:04.700 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:52:04.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 84 (show at Dw05ReleaseRegisterUsers.scala:66)
11:52:04.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 12 (show at Dw05ReleaseRegisterUsers.scala:66) with 1 output partitions
11:52:04.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 25 (show at Dw05ReleaseRegisterUsers.scala:66)
11:52:04.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 24)
11:52:04.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:52:04.700 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 25 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:52:04.701 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:52:04.703 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:52:04.703 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_29_piece0 in memory on 192.168.32.1:7247 (size: 2.2 KB, free: 1988.6 MB)
11:52:04.704 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 29 from broadcast at DAGScheduler.scala:1004
11:52:04.704 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(0))
11:52:04.704 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 25.0 with 1 tasks
11:52:04.705 INFO  [dispatcher-event-loop-7] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 25.0 (TID 79, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:52:04.705 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Running task 0.0 in stage 25.0 (TID 79)
11:52:04.723 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:04.723 INFO  [Executor task launch worker for task 79] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:04.724 INFO  [Executor task launch worker for task 79] org.apache.spark.executor.Executor - Finished task 0.0 in stage 25.0 (TID 79). 1225 bytes result sent to driver
11:52:04.724 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 25.0 (TID 79) in 19 ms on localhost (executor driver) (1/1)
11:52:04.725 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 25.0, whose tasks have all completed, from pool 
11:52:04.725 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 25 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.020 s
11:52:04.725 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 12 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.026046 s
11:52:04.727 INFO  [main] org.apache.spark.SparkContext - Starting job: show at Dw05ReleaseRegisterUsers.scala:66
11:52:04.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 13 (show at Dw05ReleaseRegisterUsers.scala:66) with 3 output partitions
11:52:04.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 27 (show at Dw05ReleaseRegisterUsers.scala:66)
11:52:04.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 26)
11:52:04.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:52:04.728 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 27 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66), which has no missing parents
11:52:04.729 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 1988.0 MB)
11:52:04.731 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1988.0 MB)
11:52:04.732 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_30_piece0 in memory on 192.168.32.1:7247 (size: 2.2 KB, free: 1988.6 MB)
11:52:04.732 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 30 from broadcast at DAGScheduler.scala:1004
11:52:04.733 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 27 (MapPartitionsRDD[86] at show at Dw05ReleaseRegisterUsers.scala:66) (first 15 tasks are for partitions Vector(1, 2, 3))
11:52:04.733 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 27.0 with 3 tasks
11:52:04.733 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 27.0 (TID 80, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:52:04.734 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 27.0 (TID 81, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:52:04.734 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 27.0 (TID 82, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:52:04.734 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Running task 0.0 in stage 27.0 (TID 80)
11:52:04.734 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Running task 1.0 in stage 27.0 (TID 81)
11:52:04.734 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Running task 2.0 in stage 27.0 (TID 82)
11:52:04.735 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:04.735 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:04.735 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:04.735 INFO  [Executor task launch worker for task 80] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:04.735 INFO  [Executor task launch worker for task 82] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:04.735 INFO  [Executor task launch worker for task 81] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:04.736 INFO  [Executor task launch worker for task 81] org.apache.spark.executor.Executor - Finished task 1.0 in stage 27.0 (TID 81). 1139 bytes result sent to driver
11:52:04.736 INFO  [Executor task launch worker for task 80] org.apache.spark.executor.Executor - Finished task 0.0 in stage 27.0 (TID 80). 1139 bytes result sent to driver
11:52:04.736 INFO  [Executor task launch worker for task 82] org.apache.spark.executor.Executor - Finished task 2.0 in stage 27.0 (TID 82). 1139 bytes result sent to driver
11:52:04.736 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 27.0 (TID 81) in 2 ms on localhost (executor driver) (1/3)
11:52:04.737 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 27.0 (TID 82) in 3 ms on localhost (executor driver) (2/3)
11:52:04.737 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 27.0 (TID 80) in 4 ms on localhost (executor driver) (3/3)
11:52:04.737 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 27.0, whose tasks have all completed, from pool 
11:52:04.737 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 27 (show at Dw05ReleaseRegisterUsers.scala:66) finished in 0.004 s
11:52:04.738 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 13 finished: show at Dw05ReleaseRegisterUsers.scala:66, took 0.009653 s
11:52:04.741 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dw_release.dw_release_register_users
11:52:04.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:04.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:04.759 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.759 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.759 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.759 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.760 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.760 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.760 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.760 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.760 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:04.764 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://hadoop102:9000/data/release/dw/release_register_user/.hive-staging_hive_2019-09-26_11-52-04_764_4669178591558998180-1
11:52:04.823 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
11:52:04.823 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: ods_release	
11:52:04.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:04.827 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:04.841 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
11:52:04.842 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
11:52:04.856 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.856 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.856 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.856 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.856 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.857 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.857 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.857 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.857 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:04.857 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:04.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
11:52:04.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
11:52:04.857 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
11:52:04.858 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
11:52:04.877 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#539),(bdp_day#539 = 20190925)
11:52:04.877 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#532),(release_status#532 = 06)
11:52:04.877 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
11:52:04.877 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,06)
11:52:04.877 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
11:52:04.882 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:04.882 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:04.892 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31 stored as values in memory (estimated size 312.0 KB, free 1987.7 MB)
11:52:04.908 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_31_piece0 stored as bytes in memory (estimated size 26.3 KB, free 1987.7 MB)
11:52:04.908 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_31_piece0 in memory on 192.168.32.1:7247 (size: 26.3 KB, free: 1988.6 MB)
11:52:04.909 INFO  [main] org.apache.spark.SparkContext - Created broadcast 31 from insertInto at SparkHelper.scala:35
11:52:04.909 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
11:52:04.956 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:35
11:52:04.957 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 91 (insertInto at SparkHelper.scala:35)
11:52:04.957 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 14 (insertInto at SparkHelper.scala:35) with 4 output partitions
11:52:04.957 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 29 (insertInto at SparkHelper.scala:35)
11:52:04.957 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 28)
11:52:04.957 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
11:52:04.958 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 29 (MapPartitionsRDD[94] at insertInto at SparkHelper.scala:35), which has no missing parents
11:52:04.994 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32 stored as values in memory (estimated size 172.9 KB, free 1987.5 MB)
11:52:04.995 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_32_piece0 stored as bytes in memory (estimated size 64.0 KB, free 1987.4 MB)
11:52:04.996 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_32_piece0 in memory on 192.168.32.1:7247 (size: 64.0 KB, free: 1988.5 MB)
11:52:04.996 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 32 from broadcast at DAGScheduler.scala:1004
11:52:04.997 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 29 (MapPartitionsRDD[94] at insertInto at SparkHelper.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
11:52:04.997 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 29.0 with 4 tasks
11:52:04.997 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 29.0 (TID 83, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
11:52:04.997 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 29.0 (TID 84, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
11:52:04.997 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 29.0 (TID 85, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
11:52:04.997 INFO  [dispatcher-event-loop-5] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 29.0 (TID 86, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
11:52:04.997 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Running task 0.0 in stage 29.0 (TID 83)
11:52:04.997 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Running task 3.0 in stage 29.0 (TID 86)
11:52:04.997 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Running task 2.0 in stage 29.0 (TID 85)
11:52:04.997 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Running task 1.0 in stage 29.0 (TID 84)
11:52:05.010 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:05.011 INFO  [Executor task launch worker for task 85] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:52:05.011 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:05.011 INFO  [Executor task launch worker for task 86] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:05.011 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:05.011 INFO  [Executor task launch worker for task 84] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
11:52:05.011 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
11:52:05.012 INFO  [Executor task launch worker for task 83] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
11:52:05.012 INFO  [Executor task launch worker for task 84] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:05.012 INFO  [Executor task launch worker for task 86] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:05.012 INFO  [Executor task launch worker for task 85] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:05.012 INFO  [Executor task launch worker for task 84] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:05.012 INFO  [Executor task launch worker for task 86] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:05.012 INFO  [Executor task launch worker for task 85] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:05.013 INFO  [Executor task launch worker for task 83] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
11:52:05.013 INFO  [Executor task launch worker for task 83] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
11:52:05.014 INFO  [Executor task launch worker for task 85] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115205_0029_m_000002_0
11:52:05.014 INFO  [Executor task launch worker for task 84] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115205_0029_m_000001_0
11:52:05.014 INFO  [Executor task launch worker for task 85] org.apache.spark.executor.Executor - Finished task 2.0 in stage 29.0 (TID 85). 2545 bytes result sent to driver
11:52:05.014 INFO  [Executor task launch worker for task 86] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115205_0029_m_000003_0
11:52:05.015 INFO  [Executor task launch worker for task 84] org.apache.spark.executor.Executor - Finished task 1.0 in stage 29.0 (TID 84). 2545 bytes result sent to driver
11:52:05.015 INFO  [Executor task launch worker for task 83] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190926115205_0029_m_000000_0
11:52:05.015 INFO  [Executor task launch worker for task 86] org.apache.spark.executor.Executor - Finished task 3.0 in stage 29.0 (TID 86). 2545 bytes result sent to driver
11:52:05.015 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 29.0 (TID 85) in 18 ms on localhost (executor driver) (1/4)
11:52:05.015 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 29.0 (TID 84) in 18 ms on localhost (executor driver) (2/4)
11:52:05.015 INFO  [Executor task launch worker for task 83] org.apache.spark.executor.Executor - Finished task 0.0 in stage 29.0 (TID 83). 2545 bytes result sent to driver
11:52:05.015 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 29.0 (TID 86) in 18 ms on localhost (executor driver) (3/4)
11:52:05.016 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 29.0 (TID 83) in 19 ms on localhost (executor driver) (4/4)
11:52:05.016 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 29.0, whose tasks have all completed, from pool 
11:52:05.016 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 29 (insertInto at SparkHelper.scala:35) finished in 0.019 s
11:52:05.016 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 14 finished: insertInto at SparkHelper.scala:35, took 0.059656 s
11:52:05.059 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
11:52:05.059 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:05.059 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:05.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:05.071 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:05.084 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.085 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:05.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:05.086 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:05.102 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
11:52:05.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:05.103 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:05.119 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dw_release`.`dw_release_register_users`
11:52:05.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dw_release
11:52:05.119 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_database: dw_release	
11:52:05.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:05.124 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:05.137 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dw_release tbl=dw_release_register_users
11:52:05.137 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=16507	ip=unknown-ip-addr	cmd=get_table : db=dw_release tbl=dw_release_register_users	
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
11:52:05.149 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
11:52:05.157 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
11:52:05.162 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@20282cb{HTTP/1.1,[http/1.1]}{0.0.0.0:4043}
11:52:05.164 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.32.1:4043
11:52:05.175 INFO  [dispatcher-event-loop-6] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
11:52:05.324 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
11:52:05.324 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
11:52:05.325 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
11:52:05.326 INFO  [dispatcher-event-loop-4] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
11:52:05.345 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
11:52:05.346 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
11:52:05.347 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\16507\AppData\Local\Temp\spark-197520e8-3161-4536-b162-77b16e0e76ad
